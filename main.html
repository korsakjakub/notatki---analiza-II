<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jakub Korsak" />
  <title>Notatki z Analizy II L2019, FUW</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
      img{width:50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Notatki z Analizy II L2019, FUW</h1>
<p class="author">Jakub Korsak</p>
</header>
<h1 id="wykład-26.02.2019">Wykład (26.02.2019)</h1>
<h2 id="funkcje-wielu-zmiennych">funkcje wielu zmiennych</h2>
<p><span class="math display">\[\begin{aligned}
        &amp;\mathbb{R}^{3}\rightarrow\mathbb{R}^{1} \text{ - Energia potencjalna } \mathcal{V}(x,y,z)\\
        &amp;\mathbb{R}^{4}\rightarrow\mathbb{R}^{1} \text{ - Potencjał pola niestacjonarnego} \mathcal{V}(x,y,z,t)\\
        &amp;\mathbb{R}^{3}\rightarrow\mathbb{R}^{3} \text{ - Natężenie pola } \mathcal{E}(x,y,z) \\
        &amp;\mathbb{R}^{4}\rightarrow\mathbb{R}^{3}\\
        &amp;\mathbb{R}^{1}\rightarrow\mathbb{R}^{3}\\
        &amp;\mathbb{R}^{1}\rightarrow\mathbb{R}^{4}\\
        &amp;\mathbb{R}^{1}\rightarrow\mathbb{R}^{6}\\
        &amp;\mathbb{R}^{6}\rightarrow\mathbb{R}^{1}\\
        &amp;\mathbb{R}^{8}\rightarrow\mathbb{R}^{1}\\
    .\end{aligned}\]</span></p>
<p>(Ciągłość Heine)<br />
Niech <span class="math inline">\(X\subset\mathbb{R}^n, x_{0} \in X, Y\subset\mathbb{R}^m\)</span>. Mówimy, że odwzorowanie <span class="math inline">\(T: X\rightarrow Y\)</span> jest ciągłe w punkcie <span class="math inline">\(x_0\)</span>, jeżeli <span class="math display">\[\underset{x_n \to x_0}{\forall}, T(x_{n})\rightarrow T(x_{0})\]</span><br />
<strong>Uwaga:</strong> <span class="math inline">\(x_{0} = (x_{1}, x_{2}, ..., x_{n})\)</span>.</p>
<p>Czy ciągłość w <span class="math inline">\(\mathbb{R}^{n} \iff\)</span> ciągłośc w <span class="math inline">\(\mathbb{R}^{1}\)</span>?</p>
<p>Niech funkcja</p>
<p><span class="math display">\[f(x,y) =
\begin{cases}
    0                           &amp; \quad x=y\\
    \frac{xy^2}{x^2+y^4}  &amp; \quad x\neq y
\end{cases}\]</span></p>
<p>czy <span class="math inline">\(f\)</span> - ciągła w <span class="math inline">\((0,0)\)</span>? dla trajektorii I:</p>
<p><span class="math display">\[\lim\limits_{y_{n}\to 0}(\lim\limits_{x_{n}\rightarrow 0} f(x_{n},y_{n})) = \lim\limits_{y_{n}\rightarrow 0}(0) = 0\]</span></p>
<p>dla trajektorii II:</p>
<p><span class="math display">\[\lim\limits_{x_{n}\to 0}(\lim\limits_{y_{n}\rightarrow 0} f(x_{n},y_{n})) = \lim\limits_{x_{n}\rightarrow 0}(0) = 0\]</span></p>
<p>weźmy <span class="math inline">\((x_{n},y_{n}) = (\frac{1}{n^{2}},\frac{1}{n})\)</span></p>
<p><span class="math display">\[f(x_{n},y_{n}) = \frac{\frac{1}{n^{2}} \frac{1}{n^{2}}}{\frac{1}{n^{4}}+\frac{1}{n^{4}}} = \frac{1}{2} \neq \lim\limits_{x_{n}\to 0, y_{n}\to 0} f(0,0)\]</span></p>
<figure>
<img src="img/fig_1.png" alt="trajektoria I i II" style="width:48.0%" /><figcaption>trajektoria I i II</figcaption>
</figure>
<p>(Ciągłość Cauchy)<br />
<span class="math inline">\((X,d_{X})\)</span> - przestrzeń wektorowa z metryką <span class="math inline">\(d_{X},\\ (Y,d_{Y})\)</span> - p.w. z metryką <span class="math inline">\(d_{Y}\)</span>.<br />
Niech <span class="math inline">\(x_{0}\in X\)</span>. Mówimy, że <span class="math inline">\(T: X\to Y\)</span> - ciągłe, jeżeli <span class="math display">\[\underset{\varepsilon &gt; 0}{\forall} \quad\underset{\delta}{\exists} \quad\underset{x\in X}{\forall} d_{X} (x,x_{0}) &lt; \delta \implies d_{Y} (T(x_{0}), T(x)) &lt; \varepsilon\]</span></p>
<p>Heine <span class="math inline">\(\iff\)</span> Cauchy</p>
<p><span class="math display">\[\implies_{(\text{przez sprzeczność})}\]</span></p>
<p>Zakładamy, że <span class="math display">\[\underset{x_n \to x_0}{\forall} T(x_{n}) \to T(x_{0})\]</span> oraz <span class="math display">\[\label{eq:p_1.1}
        \underset{\varepsilon &gt; 0}{\exists}, \underset{\delta &gt; 0}{\forall}, \underset{x\in X}{\exists} : d_{X} (x,x_{0}) &lt; \delta \quad\land\quad d_{Y} (T(x),T(x_{0})) \geq \varepsilon\]</span></p>
<p>Skoro <span class="math inline">\(T(x_{n})\to T(x_{0}) \underset{x_n \to x_0}{\forall}\)</span>, to w szczególności warunek spełniony dla ciągu, który jest taki:</p>
<p>Skoro (<a href="#eq:p_1.1" data-reference-type="ref" data-reference="eq:p_1.1">[eq:p_1.1]</a>), to dla <span class="math inline">\(\varepsilon&gt;0\)</span> weźmy <span class="math inline">\(\delta = 1\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
    &amp;\delta=1:\\
    &amp;&amp;\underset{x_1}{\exists} \quad d_{X} (x_{1},x_{0})&lt;1 \land d_{Y} (T(x_{1}), T(x_{0})) \geq \varepsilon \\
    &amp;\delta=\frac{1}{2}:\\
    &amp;&amp;\underset{x_2}{\exists} \quad d_{X} (x_{2},x_{0})&lt;\frac{1}{2} \land d_{Y} (T(x_{2}), T(x_{0})) \geq \varepsilon \\
    &amp;\delta=\frac{1}{3}:\\
    &amp;&amp;\underset{x_3}{\exists} \quad d_{X} (x_{3},x_{0})&lt;\frac{1}{3} \land d_{Y} (T(x_{3}), T(x_{0})) \geq \varepsilon\\
    &amp;\vdots &amp;\vdots\\
    &amp;\delta=\frac{1}{n}:\\
    &amp;&amp;\underset{x_n}{\exists} \quad d_{X}(x_{n},x_{0}) &lt; \frac{1}{n} \land d_{Y} (T(x_{n}),T(x_{0})) \geq \varepsilon
.\end{aligned}\]</span></p>
<p>Zauważmy, że taki ciąg <span class="math inline">\(x_{n} \to x_{0}\)</span>, ale <span class="math inline">\(T(x_{n}) \not\rightarrow T(x_{0})\)</span>, więc mamy sprzeczność. <span class="math inline">\(\Box\)</span></p>
<p><span class="math inline">\(\impliedby\)</span> Wiemy, że <span class="math display">\[\label{eq:p_1.2}
        \underset{x_n\to x_0}{\forall}\quad \underset{\varepsilon &gt; 0}{\forall}\quad \underset{x}{\exists}\quad d_{X} (x,x_{0}) &lt; \delta \implies d_{Y} (T(x),T(x_{0})) &lt; \varepsilon
    ,\]</span> czyli:</p>
<p><span class="math display">\[\label{eq:p_1.3}
    \underset{\delta_1}{\forall} \quad\underset{N}{\exists} \quad\underset{n&gt;N}{\forall} \quad d_{X} (x_{n}, x_{0}) &lt; \delta_{1}\]</span></p>
<p>Chcemy pokazać, że <span class="math inline">\(T(x_n) \to T(x_0)\)</span>, czyli, że <span class="math display">\[\underset{\varepsilon_1 &gt; 0}{\forall} \underset{N_1}{\exists} \underset{n&gt;N_1}{\forall} \quad d_Y (T(x_n),T(x_0)) &lt; \varepsilon_1 (\text{dla } x_n \to x_0)\]</span></p>
<p>Przyjmijmy <span class="math inline">\(\varepsilon=\varepsilon_1\)</span>. Oznacza to, że <span class="math inline">\(\underset{\delta}{\exists}\)</span> spełniająca warunek (<a href="#eq:p_1.2" data-reference-type="ref" data-reference="eq:p_1.2">[eq:p_1.2]</a>) dla <span class="math inline">\(\varepsilon_1\)</span>. Połóżmy <span class="math inline">\(\delta_1=\delta\)</span> we wzorze (<a href="#eq:p_1.3" data-reference-type="ref" data-reference="eq:p_1.3">[eq:p_1.3]</a>), czyli wiemy, że <span class="math display">\[\underset{N}{\exists} \underset{n&gt;N}{\forall} \quad d_X(x_n, x_0) &lt; \delta_1,\]</span> ale na mocy (<a href="#eq:p_1.2" data-reference-type="ref" data-reference="eq:p_1.2">[eq:p_1.2]</a>), wiemy, że <span class="math display">\[d_Y (T(x_n),T(x_0)) &lt; \varepsilon_1\]</span></p>
<h2 id="różniczkowalność"> Różniczkowalność: </h2>
<p>Pochodna cząstkowa:<br />
Niech <span class="math inline">\(\mathcal{O}\subset\mathbb{R}^{n}, \mathcal{O}\)</span> - otwarty, <span class="math inline">\(f: \mathcal{O}\to\mathbb{R}^{1}, x\in\mathbb{Q}, x_0 = (x_1^0,x_2^0,\dots,x_n^0)\)</span>.<br />
Mówimy, że <span class="math inline">\(f\)</span> ma w punkcie <span class="math inline">\(x\)</span> pochodną cząstkową w kierunku <span class="math inline">\(x^k\)</span>, jeżeli istnieje granica <span class="math display">\[g \overset{\text{def}}{=} \lim\limits_{h \to 0}\frac{f(x_1^0, x_2^0, \dots, x_k^0 + h, \dots, x_n^0) - f(x_1^0,\dots,x_n^0)}{h} \equiv \left. \frac{\partial}{\partial x} f \right |_{x=x_0}\]</span></p>
<figure>
<img src="img/fig_2.png" alt="Problemy: Umiemy tak jak po lewej, ale nic nie potrafimy zrobić z tym po prawej" style="width:80.0%" /><figcaption>Problemy: Umiemy tak jak po lewej, ale nic nie potrafimy zrobić z tym po prawej</figcaption>
</figure>
<p><span id="fig:fig_2" label="fig:fig_2">[fig:fig_2]</span></p>
<p>Pochodna cząstkowa<br />
Niech <span class="math inline">\(\mathbb{R}^{2}\to\mathbb{R}^{1}\)</span>. <span class="math display">\[\begin{aligned}
        &amp;\frac{\partial}{\partial x} f = \lim\limits_{h \to 0}\frac{f(x+h,y) - f(x,y)}{h},\\
        &amp;\frac{\partial}{\partial y} f = \lim\limits_{h \to 0}\frac{f(x,y+h) - f(x,y)}{h}
    .\end{aligned}\]</span></p>
<p><strong>Uwaga:</strong> do policzenia pochodnej czątkowej potrzebujemy układu współrzędnych. <span class="math display">\[\text{biegunowy } \to f(r,\varphi)
    .\]</span> <span class="math display">\[\begin{aligned}
    &amp;\frac{\partial f}{\partial r} = \lim_{h \to 0} \frac{f(r+h, \varphi) - f(r,\varphi)}{h}\\
    &amp;\frac{\partial f}{\partial \varphi} = \lim_{h \to 0} \frac{f(r, \varphi + h) - f(r,\varphi)}{h}
.\end{aligned}\]</span></p>
<p>Pochodna kierunkowa:<br />
Niech <span class="math inline">\(\mathcal{O}\subset\mathbb{R}^{n}\)</span>, <span class="math inline">\(\mathcal{O}\)</span> - otwarte, <span class="math inline">\(x_0\in\mathcal{O},e\in\mathcal{O},T:\mathcal{O}\to\mathbb{R}\)</span>.<br />
Mówimy, że <span class="math inline">\(T\)</span> ma w <span class="math inline">\(x_0\)</span> pochodną kierunkową (<em>spoiler:</em> pochodną słabą), jeżeli istnieje granica <span class="math display">\[g = \lim\limits_{t \to 0}\frac{T(x_0 +te) - T(x_0)}{t} \equiv \nabla_e T(x_0)
    .\]</span></p>
<p><strong>Obserwacja:</strong> Jeżeli np. <span class="math inline">\(T: \mathbb{R}^{2}\to\mathbb{R}, e_x=(1,0)= \begin{bmatrix} 1\\0 \end{bmatrix}\)</span> i <span class="math inline">\(e_y = (0,1) = \begin{bmatrix} 0\\1 \end{bmatrix}\)</span>, to <span class="math display">\[\nabla_{e_{x}} T = \frac{\partial}{\partial x} T \text{ i } \nabla_{e_{y}} T = \frac{\partial}{\partial y} T
.\]</span></p>
<p>Problemy z pochodną kierunkową:<br />
<span class="math inline">\(f(x,y) = \sqrt{|xy|}\)</span>. Wówczas <span class="math inline">\(x_0 + te = (0+t1,0)\)</span>, <span class="math inline">\(x_0 = (0,0), e_x = \begin{bmatrix} 1\\0 \end{bmatrix}\)</span></p>
<p><span class="math display">\[\left. \nabla_{e_x} f \right|_{x=(0,0)} = \lim\limits_{t \to 0}\frac{f(0+t,0) - f(0,0)}{t} = \lim\limits_{t \to 0}\frac{\sqrt{|t \cdot  0|} - \sqrt{|0 \cdot 0|}}{t} = \lim\limits_{t \to 0}\frac{0}{t} = 0 = \left. \frac{\partial}{\partial x} f\right |_{(0,0)}\]</span></p>
<p><strong>Uwaga:</strong> <span class="math inline">\(f(x) = \sqrt{x}, \mathbb{R}\to\mathbb{R}, f&#39;(0) = \lim\limits_{h \to 0}\frac{\sqrt{h}}{h} = \pm \infty\)</span></p>
<p><span class="math display">\[f(x,y) =
    \begin{cases}
        0 &amp; \quad x=y\\
        \frac{xy^2}{x^2+y^4} &amp; \quad x \neq y\\
    \end{cases}\]</span></p>
<p><span class="math inline">\(e = \begin{bmatrix} h_1\\h_2 \end{bmatrix}\)</span>. Pochodna: <span class="math inline">\(\left. \nabla_e f\right |_{x=(0,0)}, (x_0 + te = (th_1, th_2))\)</span></p>
<p><span class="math display">\[\lim\limits_{t \to 0}\frac{f(th_1, th_2) - f(0,0)}{t} = \frac{h_1 h_2^2}{h_1^2} = \frac{h_2^2}{h_1}\]</span></p>
<h1 id="wykład-01.03.2019">Wykład (01.03.2019)</h1>
<p>Norma<br />
Niech <span class="math inline">\(X\)</span> - przestrzeń wektorowa.<br />
Odwzorowanie <span class="math inline">\(||.||: \mathbb{X}\to \mathbb{R}\)</span> nazywamy normą, jeżeli:</p>
<p><span class="math display">\[\begin{aligned}
    \underset{x\in X}{\forall} \quad &amp;||x|| \geq 0\\
    \underset{\alpha\in\mathbb{R}}{\forall}, \underset{x\in \mathbb{X}}{\forall} \quad &amp;|| \alpha x|| = |\alpha| ||x||\\
    \underset{x,y \in X}{\forall} \quad &amp;||x+y|| \leq ||x|| + ||y||\\
    \underset{x\in X}{\forall} \quad &amp;||x|| = 0 \iff x = 0\end{aligned}\]</span></p>
<p>Przestrzeń <span class="math inline">\(X\)</span> wraz z normą <span class="math inline">\(||.||\)</span> nazywamy przestrzenią unormowaną (<em>spoiler:</em> przestrzenią Banacha).</p>
<p>Przykładowa norma:<br />
<span class="math display">\[||v|| = \sqrt{(x)^2 + (y)^2}, X = \mathbb{R}^{n}
    .\]</span> <span class="math display">\[X \ni v \implies||v|| = sup(|x^1|,\dots).\]</span> Jeżeli <span class="math inline">\(f\in \mathcal{C}([a,b])\)</span>, to norma wygląda tak: <span class="math display">\[||f|| = sup_{x\in{[a,b]}} (f(x))
    .\]</span></p>
<p><span class="math display">\[\mathbb{R}^2_2 \ni \begin{bmatrix} a&amp;b\\c&amp;d \end{bmatrix} = v\]</span> <span class="math display">\[\Vert v \Vert = \max \left\{ |a|, |b|, |c|, |d| \right\}
    .\]</span></p>
<p><strong>Uwaga:</strong> mając normę możemy zdefiniować metrykę <span class="math inline">\(\underset{x,y\in X}{\forall} d(x,y) = ||x-y||\)</span>, natomiast nie każdą metrykę da się utworzyć przy pomocy normy.</p>
<p>metryka zdefiniowana przy pomocy normy ma np. taką własność: <span class="math display">\[d(ax,ay) = ||ax - ay|| = |a| ||x-y|| = a d(x,y),\]</span> czyli taka metryka się skaluje natomiast funkcja <span class="math display">\[d(x,y) =
    \begin{cases}
        1 &amp; \quad x\neq y\\
        0 &amp; \quad x=y\\
    \end{cases}\]</span> jest metryką, ale tej własności nie posiada.</p>
<p>Pochodna mocna (trzecie podejście) <span class="math display">\[\lim\limits_{h \to 0}\frac{f(x+h) - f(x)}{h} = f&#39;(x_0), \text{ dla }x\in V\subset \mathbb{R}^{n}
 .\]</span> - taka definicja jest niemożliwa (nie mamy dzielenia wektorów).</p>
<p><span class="math display">\[f(x+h)-f(x)=f&#39;(x_0) h + r(x_0,h), \text{ gdzie } \frac{r(x_0,h)}{||h||}\to 0 \text{ przy } ||h||\to 0\]</span> ale to może mieć już inną dziedzinę</p>
<p>Niech <span class="math inline">\(U \subset X, V\subset Y\)</span><br />
<span class="math inline">\(U,V\text{ - otwarte, }\quad T:U\to V\\
    x,h\in U\)</span></p>
<p>Mówimy, że <span class="math inline">\(T\)</span> - różniczkowalne w punkcie <span class="math inline">\(x_0\)</span>, jeżeli prawdziwy jest wzór <span class="math display">\[\underset{h\in U}{\forall} \quad T(x_0+h) - T(x_0) = L_{x_0} (h) + r(x_0,h),\]</span> gdzie <span class="math inline">\(\frac{r(x_0,h)}{||h||}\to 0\)</span>, a <span class="math inline">\(L_{x_0}\)</span> - liniowe <span class="math inline">\(: X\to Y\)</span>.</p>
<p>Odwzorowanie <span class="math inline">\(L_{x_0} (h)\)</span> nazywamy pochodną T w punkcie <span class="math inline">\(x_0\)</span>. Czasami <span class="math inline">\(L_{x_0}(h)\)</span> możemy przedstawić w postaci <span class="math inline">\(L_{x_0} (h) = T&#39;(x_0) h\)</span>, to <span class="math inline">\(T&#39;(x_0)\)</span> nazywamy pochodną odwzorowania T.</p>
<p><strong>Uwaga:</strong> Dlaczego <span class="math inline">\(L_{x_0}(h)\)</span>, a nie <span class="math inline">\(T&#39;(x_0) h\)</span>?</p>
<p>Dlatego, że czasami pochodna może wyglądać tak:</p>
<p><span class="math display">\[\int_0^1 h(x)\sin{x}dx
,\]</span> a tego nie da sie przedstawić jako <span class="math display">\[\left ( \int_0^1 \sin{x}dx \right ) h(x).\]</span></p>
<p><span class="math inline">\(T(x+h) - T(x) = T&#39;(x_0)h+r(x_0,h)\)</span> <span class="math display">\[\begin{aligned}
        &amp;1. T: \mathbb{R}\to \mathbb{R}^{3}, \text{ czyli } x_0\in \mathbb{R}, h\in R \implies T(x) = \begin{bmatrix} -\\-\\- \end{bmatrix}  T&#39;(x) = \begin{bmatrix} -\\-\\- \end{bmatrix} \\
            &amp;2. T:\mathbb{R}^3 \to \mathbb{R} \quad x_0 = \begin{bmatrix} -\\-\\- \end{bmatrix}  h = \begin{bmatrix} -\\-\\- \end{bmatrix} , T&#39;(x) = \left[ -,-,- \right] \\
                &amp;3. T:\mathbb{R}^2 \to \mathbb{R}^3 \quad x_0 \begin{bmatrix} -\\- \end{bmatrix}  h = \begin{bmatrix} -\\- \end{bmatrix}, T(x) = \begin{bmatrix} -\\-\\- \end{bmatrix} , T&#39;(x) = \begin{bmatrix} -&amp;-&amp;-\\-&amp;-&amp;- \end{bmatrix} \\
    .\end{aligned}\]</span></p>
<p><span class="math display">\[f(x,y) = xy^2, h=\binom{h_x}{h_y}.\]</span> <span class="math display">\[\begin{aligned}
           &amp;f(x_0+h_x,y_0+h_y) - f(x_0,y_0) = \\
           &amp;= (x_0+hx)(y_0+hy)^2 - x_0y_0^2 = \\
           &amp;= x_0y_0^2 + 2y_0x_0h_y + x_0h_y^2 + h_yy_0^2 + h_xh_y 2y_0 + h_xh_y = \\
           &amp;= \left[ y_0^2, 2x\cdot x_0 \right] \begin{bmatrix}h_x\\h_y  \end{bmatrix} + x_0h_y^2+h_xh_y^2+2y_0h_xh_y
       .\end{aligned}\]</span></p>
<p>Czy <span class="math inline">\(\frac{r(x_0,h)}{||h||}\underset{h \to 0}{\longrightarrow} 0\)</span>?</p>
<p>Weźmy <span class="math inline">\(\left\Vert \begin{bmatrix} h_x\\h_y \end{bmatrix} \right\Vert = \sup\{|h_x|,|h_y|\}\)</span>, wówczas<br />
<span class="math display">\[x_0h_y^2 + h_xh_y^2 + 2y_0h_xh_y \leq x_0 ||h||^2 + ||h||^3 + 2y_0||h||^2 = ||h||^2(x_0 +2y_0 + ||h||),\]</span><br />
zatem <span class="math display">\[\frac{r(x_0,h)}{||h||} \leq \frac{||h||^2(|x_0|+2y_0+||h||)}{||h||} \to 0.\]</span></p>
<p><span class="math display">\[f(x,y)=xy^2, T&#39;(x)=[y^2,2xy].\]</span> zauważmy, że <span class="math display">\[y^2= \frac{\partial}{\partial x} f, 2xy = \frac{\partial}{\partial y} f.\]</span> <strong>Uwaga:</strong> skąd wiemy, że gdy <span class="math inline">\(h\to 0\)</span>, to <span class="math inline">\(||h||\to 0\)</span>?<br />
Czyli: czy norma jest odwzorowaniem ciągłym w <span class="math inline">\(h=0\)</span>?</p>
<p><em>odpowiedź za tydzień</em></p>
<p>Jeżeli <span class="math inline">\(f\)</span> - różniczkowalna w <span class="math inline">\(x_0 \in U\)</span>, to dla dowolnego <span class="math inline">\(e\in U\)</span>, <span class="math display">\[\nabla_e f(x_0) = f&#39;(x_0)e\]</span></p>
<p>skoro <span class="math inline">\(f\)</span> - różniczkowalna, to <span class="math display">\[\label{eq: eq_2.1}
        \underset{h\in U}{\forall} f(x_0+h) - f(x_0) = f&#39;(x_0)h + r(x_0,h), \frac{r(x,h)}{\Vert h \Vert } \underset{\Vert h \Vert \to 0}{\longrightarrow} 0\]</span></p>
<p><span class="math display">\[\underset{h_x, h_y}{\forall} \frac{\sqrt{h_x\cdot h_y} }{\left\Vert h \right\Vert } \underset{h\to 0}{\longrightarrow} 0
    .\]</span> Niech <span class="math inline">\(\left\Vert h \right\Vert = \sup \left\{ |h_x|, |h_y| \right\}, |h_x| &gt; |h_y| \implies \left\Vert h \right\Vert = |h_x|\)</span> <span class="math display">\[\frac{\sqrt{|h_x \cdot h_y|} }{\left\Vert h \right\Vert } = \frac{\sqrt{|h_x \cdot h_y}}{h_x} \not \to  0
.\]</span></p>
<p>Czy z faktu istnienia pochodnych cząstkowych wynika różniczkowalność funkcji?</p>
<p><span class="math inline">\(f(x,y) = \sqrt{|xy|}, x_0=\binom{0}{0}\)</span>, dla <span class="math inline">\(f(x,y)\)</span> policzyliśmy pochodne cząstkowe w <span class="math inline">\(x_0 \quad \frac{\partial}{\partial x} f = 0, \frac{\partial}{\partial y} f = 0\)</span>.</p>
<p><span class="math inline">\(h=\binom{h_x}{h_y}, x_0=\binom{0}{0}, f(x_0+h)-f(x_0) = \sqrt{h_xh_y} - \sqrt{0} = \sqrt{h_xh_y} = (0,0)\binom{h_x}{h_y} + \sqrt{h_xh_y}\)</span>, gdzie <span class="math inline">\(r(x_0,h) = \sqrt{h_xh_y}\)</span>.<br />
Czyli <span class="math inline">\(f\)</span> - różniczkowalna, jeżeli <span class="math inline">\(\underset{h_x,h_y}{\forall}\quad \frac{\sqrt{h_xh_y}}{||h||}\to0\)</span>.<br />
Niech <span class="math inline">\(||h|| = \sup\{|h_x|,|h_y|\}\)</span> i niech <span class="math inline">\(|h_x|&gt;|h_y|\)</span>. <span class="math inline">\(||h|| = |h_x|.\)</span><br />
Dalej mamy: <span class="math inline">\(\frac{\sqrt{h_xh_y}}{|h_x|}\sqrt{\frac{h_y}{h_x}} \not\to 0 \text{ przy }h_x\to0\)</span>, <span class="math inline">\(\sqrt{\frac{|h_y|}{|h_x|}}=\sqrt{\frac{1}{2}}\)</span></p>
<p><strong>Czyli istnienie pochodnych cząstkowych nie oznacza różniczkowalności.</strong></p>
<p>Niech <span class="math inline">\(O\subset\mathbb{R}^{n}, O\)</span> - otwarty. <span class="math inline">\(f: O\to Y, x_0\in O\)</span>.</p>
<p>Jeżeli istnieją pochodne cząstkowe <span class="math inline">\(\frac{\partial}{\partial x_i} f, i=1,\dots,n\)</span> i są ciągłe w <span class="math inline">\(x_0\)</span>, wtedy <span class="math display">\[\underset{h\in\mathbb{R}^n}{\forall} f(x_0+h)-f(x_0)=\sum_{i=1}^{n} \frac{\partial f}{\partial x_i} h^i+r(x_0,h)
    ,\]</span> gdzie <span class="math inline">\(\frac{r(x_0,h)}{||h||}\to0\)</span></p>
<p>(dla <span class="math inline">\(O=\mathbb{R}^3\)</span>)</p>
<p>Niech <span class="math inline">\(x_0 = \left [ \begin{matrix}
        x_0^1\\
        x_0^2\\
        x_0^3
    \end{matrix}
    \right ], h = \left [ \begin{matrix}
        h^1\\
        h^2\\
        h^3
    \end{matrix} \right ]\)</span></p>
<p><span class="math display">\[\begin{aligned}
        &amp;f(x_0^1+h^1,x_0^2+h_2,x_0^3+h^3)-f(x_0^1,x_0^2,x_0^3)=\\
        &amp;=f(x_0^1+h^1,x_0^2+h^2,x_0^3+h^3)-f(x_0^1+h^1,x_0^2+h^2,x_0^3)+\\
        &amp;+f(x_0^1+h^1,x_0^2+h^2,x_0^3)-f(x_0^1+h^1,x_0^2,x_0^3)+\\
        &amp;+f(x_0^1+h^1,x_0^2,x_0^3)-f(x_0^1,x_0^2,x_0^3) \underset{\text{tw. o w. średniej}}{=}\\
        &amp;\frac{\partial}{\partial x_0^1} f (c_1)h^1 + \frac{\partial}{\partial x_0^2} f (x_0^1+h^1,c_2,x_0^3)h^2+\frac{\partial}{\partial x_0^3} f (x_0^1+h^1,x_0^2+h_2,c_3)h^3=\\
        &amp;(\frac{\partial f}{\partial x^1} (c_1,x_0^2,x_0^3) - \frac{\partial f}{\partial x^1} (x_0^1,x_0^2,x_0^3))h^1 +\\
        &amp;+ (\frac{\partial f}{\partial x^2} (x_0^1 + h^1, c_2, x_0^3) - \frac{\partial f}{\partial x^2} (x_0^1, x_0^2, x_0^3))h^2 +\\
        &amp;+ (\frac{\partial f}{\partial x^3} (x_0^1+h^1,x_0^2+h^2,c_3) - \frac{\partial f}{\partial x^3} (x_0^1, x_0^2, x_0^3))h^3\\
    .\end{aligned}\]</span></p>
<p>gdzie <span class="math inline">\(c_1\in ]x_0^1,x_0^1+h^1[,\quad c_2\in ]x_0^2,x_0^2+h^2[,\quad c_3\in ]x_0^3,x_0^3+h^3[\)</span></p>
<p>Wystarczy pokazać, że <span class="math inline">\(\frac{r(x_0,h)}{||h||}\to 0\)</span>, gdy <span class="math inline">\(h\to 0\)</span>.</p>
<p>Zauważmy, że każde wyrażenie tworzące resztę jest postaci <em>coś</em> <span class="math inline">\(h^i\)</span>, a <span class="math inline">\(\lim\limits_{||h|| \to 0}\frac{h^i}{||h||} =\)</span> <em>dla normy np.</em> <span class="math inline">\(||h|| = max{|h^i|}\neq 0\)</span>. (np. <span class="math inline">\(\frac{h^1}{h^1} \to 1\)</span>)</p>
<p>Oznacza to, że jeżeli <span class="math inline">\(\frac{r(x,h)}{||h||}\to 0\)</span> - spełniony, to każde wyrażenie typu <span class="math display">\[\Big ( \frac{\partial f}{\partial x^1} (c_1,x_0^2,x_0^3) - \frac{\partial f}{\partial x^1} (x_0^1,x_0^2,x_0^3)\Big ) h^1 \to 0\]</span></p>
<p>Czyli np. <span class="math inline">\(\lim\limits_{||h|| \to 0}\frac{\partial f}{\partial x^1} (c_1,x_0^2,x_0^3) = \frac{\partial f}{\partial x^1} (x_0^1,x_0^2,x_0^3) \iff (\frac{\partial f}{\partial x^1}  \text{ - ciągła} )\)</span></p>
<h1 id="wykład-05.03.2019">Wykład (05.03.2019)</h1>
<p><strong>Uwaga:</strong> jeżeli np. <span class="math inline">\(f: \mathbb{R}^{2} \to \mathbb{R}^{2}\)</span>, to znaczy, że<br />
<span class="math inline">\(f(x,y) =
\left [
\begin{matrix}
    f_1 (x,y)\\
    f_2 (x,y)
\end{matrix}
\right ], f_1 : \mathbb{R}^{2} \to \mathbb{R}^{1}, f_2 : \mathbb{R}^{2} \to \mathbb{R}^{1}\)</span> , wówczas <span class="math display">\[\frac{\partial f}{\partial x} =
    \left [
        \begin{matrix}
            \frac{\partial}{\partial x} f_1 \\
            \frac{\partial}{\partial x} f_2
        \end{matrix}
    \right ],
    \frac{\partial}{\partial y} f =
    \left [
        \begin{matrix}
            \frac{\partial}{\partial y} f_1 \\
            \frac{\partial}{\partial y} f_2
        \end{matrix}
    \right ]\]</span></p>
<p><span class="math display">\[f(x,y) = \left [
        \begin{matrix}
            2xy^2\\
            x^3 y
        \end{matrix}\right ]\]</span> Wtedy pochodne czątkowe: <span class="math display">\[\frac{\partial f}{\partial x} =\left [ \begin{matrix} 2y^2\\ 3x^2 y \end{matrix} \right ], \frac{\partial f}{\partial y} = \left [ \begin{matrix} 4xy\\ x^3 \end{matrix}\right ]\]</span> <span class="math display">\[\begin{aligned}
        f(x+h)-f(x) &amp;= \\
        &amp;= \frac{\partial f}{\partial x} h^x + \frac{\partial f}{\partial y} h^y + r((x,y),h) = \\
        &amp;=\left [ \begin{matrix} 2y^2\\ 3x^2 y\\ \end{matrix}\right ] h^x + \left [ \begin{matrix} 4xy\\ x^3\\ \end{matrix}\right ] h^y + r((x,y),h) \\
        &amp;= \left [ \begin{matrix} 2y^2    &amp;4xy\\ 3x^2 y  &amp;x^3\\ \end{matrix}\right ] \left [ \begin{matrix} h^x\\ h^y\\ \end{matrix}\right ] + r((x,y),h)
.\end{aligned}\]</span></p>
<p>Czyli <span class="math display">\[f&#39; = \left [ \begin{matrix}
\frac{\partial f_1}{\partial x}     &amp;\frac{\partial f_1}{\partial y}\\
\frac{\partial f_2}{\partial x}     &amp;\frac{\partial f_2}{\partial y}\\
\end{matrix}\right ]\]</span></p>
<p>i ogólniej: jeżeli <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}^k\)</span>, to <span class="math display">\[f&#39; = \left [ \begin{matrix}
    \frac{\partial f_1}{\partial x^1}   &amp;\dots   &amp;\frac{\partial f_1}{\partial x^n}\\
    \vdots                              &amp;\ddots  &amp;\vdots\\
    \frac{\partial f_k}{\partial x^1}   &amp;\dots   &amp;\frac{\partial f_k}{\partial x^n}\\\end{matrix}\right ]\]</span></p>
<h2 id="uzupełnienie"> Uzupełnienie: </h2>
<p>Niech <span class="math inline">\(V\)</span> - przestrzeń wektorowa z normą <span class="math inline">\(||.||\)</span> i <span class="math inline">\(x_0\in V\)</span>, wówczas <span class="math display">\[f(x)=||x||, f: V\to\mathbb{R}^1 \text{ - ciągła w } x_0.\]</span></p>
<p>Chcemy pokazać, że <span class="math display">\[\underset{\varepsilon &gt; 0}{\forall} \quad\underset{\delta}{\exists} \quad\underset{x}{\forall} \quad d_x (x,x_0) &lt; \delta \implies d_{\mathbb{R}} (f(x),f(x_0)) &lt; \varepsilon\]</span> ale <span class="math display">\[d_x(x,y) = ||x-y||, d_{\mathbb{R}^1} (x,y) = |x-y|
    .\]</span> Czyli pokażemy, że <span class="math display">\[\underset{\varepsilon &gt; 0}{\forall} \underset{\delta}{\exists} \underset{x}{\forall} \quad ||x - x_0|| &lt; \delta \implies \big | ||x|| - ||x_0|| \big | &lt; \varepsilon
    .\]</span> Ale wiemy, że <span class="math display">\[||x|| = ||x-y+y|| \leq ||x-y|| + ||y||, ||x||-||y||\leq ||x-y||,\]</span> <span class="math display">\[||y|| = ||y-x+x||\leq ||y-x|| + ||x||,\]</span> <span class="math display">\[||y||-||x||\leq ||x-y||
    ,\]</span> czyli <span class="math inline">\(\left| \left\Vert x \right\Vert  - \left\Vert y \right\Vert  \right| \leq \left\Vert x-y \right\Vert\)</span>. Niech <span class="math inline">\(\delta = \frac{\varepsilon}{2}\)</span>, otrzymujemy <span class="math inline">\(\varepsilon &gt; \frac{\varepsilon}{2} &gt; ||x-y|| \geq \big | ||x|| - ||y|| \big | \geq 0\)</span></p>
<p>Niech <span class="math inline">\(f(x,y) = 7x+6y^2 \text{ i } g(t) = \left [ \begin{matrix}
cos(t) \\
sin(t) \\
\end{matrix}\right ]\)</span>. Wówczas <span class="math inline">\(h(t) = (f \circ g)(t) : \mathbb{R}\to \mathbb{R}\)</span>. Ile wynosi pochodna?</p>
<p><span class="math inline">\(f&#39; = [7,12y] , g&#39; = \left [ \begin{matrix}
-sin(t)\\
cos(t)\\
\end{matrix}\right ]\)</span></p>
<p>Niech <span class="math inline">\(G:U \to Y, U\subset X, U\)</span> - otwarte,<br />
<span class="math inline">\(X\)</span> - przestrzeń wektorowa unormowana,<br />
<span class="math inline">\(F: G(U) \to Z, G(U) \subset V\)</span><br />
<span class="math inline">\(G\)</span> - różniczkowalna w <span class="math inline">\(x_0\in U\)</span>,<br />
<span class="math inline">\(F\)</span> - różniczkowalna w <span class="math inline">\(G(x_0)\in U\)</span>.<br />
Wówczas: <span class="math inline">\((F \circ G )\)</span> - różniczkowalna w <span class="math inline">\(x_0\)</span> oraz <span class="math display">\[(F \circ G)&#39; (x_0) = \left . F&#39;(x)\right |_{x=G(x_0)} G&#39;(x_0).\]</span></p>
<p><span class="math display">\[G(x_0 + h_1) - G(x_0) = G&#39;(x_0)h_1+r_1(x_0,h_1)\text{, gdy }\frac{r(x_0,h_1)}{||h_1||_x} \to 0\]</span> <span class="math display">\[F(y_0 + h_2) - F(y_0) = F&#39;(y_0)h_2+r_2(y_0,h_2)\text{, gdy }\frac{r(y_0,h_2)}{||h_2||_y} \to 0\]</span></p>
<p><span class="math display">\[\begin{aligned}
    F\left(G(x_0 + h)\right) - F(G(x_0)) &amp;=\\
    &amp;= F(G(x_0) + G&#39;(x_0)h_1 + r_1(x_0,h_1)) - F(G(x_0)) =\\
    &amp;= F(G(x_0)) + F&#39;(G(x_0)) \cdot (G&#39;(x_0)h_1 + r_1(x_0,h_1)) + \\
    &amp;= r_2(G(x_0), G&#39;(x_0)h_1 + r_1(x_0,h_1)) - F(G(x_0)).\end{aligned}\]</span> zatem: <span class="math display">\[\begin{aligned}
    F(G(x_0+h)) - F(G(x_0)) &amp;=\\
    &amp;=F&#39;(G(x_0)) \cdot G&#39;(x_0)h_1+F&#39;(G(x_0)) \cdot r_1(x_0,h_1)+\\
    &amp;=r_2 \cdot (G(x_0),G&#39;(x_0)h_1+r_1(x_0,h_1))
.\end{aligned}\]</span></p>
<p>Wystarczy pokazać, że <span class="math display">\[\frac{r_3}{||h_1||}\to 0,\]</span> ale <span class="math display">\[\begin{aligned}
        \frac{r_3}{||h_1||} &amp;= F&#39;(G(x_0)) \frac{r_1(x_0,h_1)}{||h_1||} +\\
        &amp;+ \underbrace{\frac{r_2(G(x_0),G&#39;(x_0)h_1+r_1(x_0,h_1))}{||G&#39;(x_0)h_1 + r_1(x_0,h_1)||}}_{\to 0 \text{ kiedy } h_1 \to 0} \cdot
        \underbrace{\frac{||G&#39;(x_0)h_1+r_1(x_0,h_1)||}{||h_1||}}_{\text{jest ograniczony}}
    ,\end{aligned}\]</span> ale jeżeli <span class="math inline">\(h_1\to 0\)</span>, to <span class="math inline">\(h_2 = G&#39;(x_0)h_1+r_1(x_0,h_1)\)</span>, zatem <span class="math inline">\(F(G(x))\)</span> - różniczkowalna w <span class="math inline">\(x_0\)</span></p>
<p><span class="math inline">\(f(x,y) = \left [ \begin{matrix}
2xy^2\\
x^3 y\\\end{matrix}\right ],
\varphi(t) = \left [ \begin{matrix}
2t^2\\
t^3\\
\end{matrix}\right ],
h(t) = (f \circ \varphi) (t), h: \mathbb{R}\to \mathbb{R}^2\)</span>.</p>
<p>Policzmy <span class="math inline">\(H&#39;\)</span>. <span class="math inline">\(f&#39; = \left [ \begin{matrix}
2y^2    &amp;4xy\\
3x^2y   &amp;x^3\\
\end{matrix}\right ], \varphi &#39;(t) = \left [ \begin{matrix}
4t\\
3t^2\\
    \end{matrix}\right ]\)</span>, tzn. <span class="math display">\[H&#39; = \left [ \left .\begin{matrix}
2y^2    &amp;4xy\\
3x^2y   &amp;x^3\\
    \end{matrix}\right ]\right |_{x=2t^2, y=t^3} \cdot \left [ \begin{matrix}
4t\\
3t^2\\
\end{matrix}\right ] = \\
\left [ \begin{matrix} 2(2t^2)^2 4t + 4(2t^2)(t^3) 3t^2\\
3(2t^2)^2 t^3 4 + (2t^3)^3 3t^2\\\end{matrix}\right ]\]</span></p>
<p>Niech <span class="math inline">\(f: \mathbb{R}^2 \to \mathbb{R}\)</span>,<br />
<span class="math inline">\(\Psi: \mathbb{R}^2 \to \mathbb{R}^2\)</span>,<br />
<span class="math inline">\(\Psi(r,\varphi) = \left [ \begin{matrix} \Psi_1(r,\varphi)\\ \Psi_2(r,\varphi)\\\end{matrix}\right ]\)</span> <span class="math inline">\(\Psi_1: \mathbb{R}^2 \to \mathbb{R}\)</span> <span class="math inline">\(\Psi_2: \mathbb{R}^2 \to \mathbb{R}\)</span></p>
<p>Niech <span class="math inline">\(H(r,\varphi) = (f \circ \Psi) (r, \varphi)\)</span>, czyli <span class="math inline">\(H : \mathbb{R}^2 \to \mathbb{R}\)</span>.</p>
<p>Szukamy pochodnej <span class="math inline">\(H\)</span>, ale <span class="math display">\[f&#39; = [\frac{\partial f}{\partial x} , \frac{\partial f}{\partial y} ], \Psi &#39; = \left [ \begin{matrix}
\frac{\partial \Psi_1}{\partial r}  &amp;\frac{\partial \Psi_1}{\partial \varphi} \\
\frac{\partial \Psi_2}{\partial r}  &amp;\frac{\partial \Psi_2}{\partial \varphi}
        \end{matrix}\right ]\]</span></p>
<p>Czyli <span class="math display">\[H&#39; = \left . \Big [ \frac{\partial f}{\partial x} , \frac{\partial f}{\partial y} \Big ] \right |_{x=\Psi_1(r,\varphi), y=\Psi_1(r,\varphi)} \left [ \begin{matrix}
\frac{\partial \Psi_1}{\partial r}  &amp;\frac{\partial \Psi_1}{\partial \varphi} \\
\frac{\partial \Psi_2}{\partial r}  &amp;\frac{\partial \Psi_2}{\partial \varphi}
        \end{matrix}\right ]\]</span></p>
<p>Co daje: <span class="math display">\[\left . \Big [ \frac{\partial H}{\partial r} , \frac{\partial H}{\partial \varphi} \Big ] = \Big [ \frac{\partial f}{\partial x} \frac{\partial \Psi_1}{\partial r} + \frac{\partial f}{\partial y} \frac{\partial \Psi_2}{\partial r} , \frac{\partial f}{\partial x} \frac{\partial \Psi_1}{\partial \varphi} + \frac{\partial f}{\partial y} \frac{\partial \Psi_2}{\partial \varphi} \Big ] \right |_{x = \Psi_1 (r,\varphi), y = \Psi_2 (r,\varphi)}\]</span></p>
<h1 id="wykład-08.03.2019">Wykład (08.03.2019)</h1>
<h2 id="konwencja-z-ćwiczeń-z-fizyki">Konwencja z ćwiczeń z fizyki:</h2>
<p>Mamy funkcję <span class="math inline">\(H(r,\varphi) = (f \circ \Psi)(r,\varphi)\)</span> <span class="math display">\[\begin{aligned}
    &amp;\Psi_1(r,\varphi)=x(r,\varphi)\\
    &amp;\Psi_2(r,\varphi)=y(r,\varphi)\\
    &amp;\frac{\partial f}{\partial r} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial r}\\
    &amp;\frac{\partial f}{\partial \varphi} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial \varphi} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial \varphi}\end{aligned}\]</span></p>
<p><span class="math display">\[f(x,y):\mathbb{R}^2 \to \mathbb{R},\quad
\left [ \begin{matrix}
x=r\cos{\varphi}\\
y=r\sin{\varphi}\end{matrix}\right ]\]</span> <span class="math display">\[\frac{\partial f}{\partial r} = \cos{\varphi} \frac{\partial f}{\partial x} + \sin{\varphi} \frac{\partial f}{\partial y},\quad
\frac{\partial f}{\partial \varphi} = -r\sin{\varphi} \frac{\partial f}{\partial x} + r\cos{\varphi} \frac{\partial f}{\partial y}\]</span> <span class="math display">\[f(x,y):\mathbb{R}^2 \to \mathbb{R},\quad
f&#39;= \Big [ \frac{\partial f}{\partial x} , \frac{\partial f}{\partial y} \Big ]\]</span></p>
<h2 id="interpretacja-geometryczna-f">Interpretacja geometryczna <span class="math inline">\(f&#39;\)</span></h2>
<p>Rozważmy zbiór <span class="math display">\[P_c = \{(x,y)\in \mathbb{R}^2 : f(x,y) = c \} \text{ np. } f(x,y) = x^2 + y^2 : P_c = \{(x,y)\in\mathbb{R}^2 : x^2+y^2 = c \}.\]</span></p>
<p>Załóżmy, że <span class="math inline">\(f(x,y)\)</span> - taka, że <span class="math inline">\(P_c\)</span> można sparametryzować jako <span class="math display">\[~ \varphi(t) = \left [ \begin{matrix}
x(t)\\
y(t)\end{matrix}\right ], t\in D\text{, to znaczy, że }P_c = \{(x(t),y(t)), t\in D\}\]</span></p>
<p>Niech <span class="math inline">\(\varphi(t) = \left [ \begin{matrix}

\cos{t}\\
\sin{t}\end{matrix}\right ]\)</span>. Wtedy <span class="math inline">\(P_c = \{(c \cdot \cos{t},c \cdot \sin{t}): t\in[0,2\pi]\}\)</span></p>
<p><span class="math inline">\(f(x(t),y(t)) = c \quad\underset{t\in D}{\forall}\)</span> - powierzchnie ekwipotencjalne</p>
<p><span class="math display">\[\Big [ \frac{\partial f}{\partial x} , \frac{\partial f}{\partial y} \Big ] \left [ \begin{matrix}
x&#39;(t)\\
    y&#39;(t)\end{matrix}\right ] = 0,\]</span> <span class="math display">\[\Big [ 2x, 2y \Big ] \left [ \begin{matrix}
-c \cdot \sin{t}\\
c \cdot \cos{t}\end{matrix}\right ] = 0.\]</span></p>
<figure>
<img src="img/fig_3.png" alt="Trajektoria kluki" id="fig:kulka" style="width:70.0%" /><figcaption>Trajektoria kluki<span label="fig:kulka"></span></figcaption>
</figure>
<figure>
<img src="img/fig_4.png" alt="Powierzchnia ekwipotencjalna I" id="fig:ekwiI" style="width:70.0%" /><figcaption>Powierzchnia ekwipotencjalna I<span label="fig:ekwiI"></span></figcaption>
</figure>
<figure>
<img src="img/fig_5.png" alt="Powierzchnia ekwipotencjalna II" id="fig:ekwiII" style="width:70.0%" /><figcaption>Powierzchnia ekwipotencjalna II<span label="fig:ekwiII"></span></figcaption>
</figure>
<p>Pochodna mieszana <span class="math display">\[f(x,y) = x^2y^3, \quad \frac{\partial f}{\partial x} = 2xy^3, \frac{\partial f}{\partial y} = 3x^2y^2,\quad
\frac{\partial }{\partial x} \big (\frac{\partial f}{\partial x} \big ) = 2y^3,\quad
\frac{\partial }{\partial y} \big (\frac{\partial f}{\partial y} \big ) = 6x^2y\]</span></p>
<p><span class="math display">\[\frac{\partial^2 f}{\partial x \partial y} = 6xy^2 \quad \frac{\partial^2 f}{\partial y \partial x} = 6xy^2\]</span> <strong>Przypadek???</strong></p>
<p>(Uogólnione twierdzenie Schwarza)<br />
Niech <span class="math inline">\(f: \mathcal{O}\to\mathbb{R}, \mathcal{O}\subset \mathbb{R}^n\)</span>, otwarty i <span class="math inline">\(f\in\mathcal{C}^2(\mathcal{O})\)</span>, wówczas <span class="math display">\[\frac{\partial^2 f}{\partial x^i \partial x^j} = \frac{\partial^2 f}{\partial x^j \partial x^i}; i,j=1,\dots,n\]</span></p>
<p>Dowód dla <span class="math inline">\(n=2\)</span> Niech <span class="math display">\[w(x,y) = f(x+h,y+k) - f(x+h,y) - f(x,y+k) + f(x,y),\]</span> <span class="math display">\[\varphi(x) = f(x,y+k) - f(x,y)\]</span> wówczas <span class="math display">\[\begin{aligned}
    w &amp;=\varphi(x+h) - \varphi(x) = \frac{\partial \varphi}{\partial x} (\xi)h = \\
&amp;=\left[ \frac{\partial f}{\partial x} (\xi, y+k) - \frac{\partial f}{\partial x} (\xi, y) \right]h = \\
    &amp;= \frac{\partial }{\partial y} \Big ( \frac{\partial f}{\partial x} (\xi,\eta) \Big ) hk
,\\
    &amp; \text{gdzie }x&lt;\xi&lt;x+h,\quad y&lt;\eta&lt;y+k\end{aligned}\]</span></p>
<p>Niech <span class="math display">\[\Psi(y)=f(x+h,y)-f(x,y)\]</span> <span class="math display">\[\begin{aligned}
    w(x,y) &amp;= \Psi(y+k) - \Psi(y) = \frac{\partial \Psi}{\partial y} (\eta_1)k = \\
    &amp;=  \left[\frac{\partial f}{\partial y} (x+h,\eta_1) - \frac{\partial f}{\partial y} (x,\eta_1) \right] k = \\
    &amp;= \frac{\partial }{\partial x} \Big ( \frac{\partial f}{\partial y} (\xi,\eta) \Big ) kh
,\end{aligned}\]</span> czyli <span class="math display">\[\underset{\xi}{\exists}\quad \xi\in]x,x+h[,\quad \xi_1\in]x,x+h[,\quad \eta\in]y,y+k[,\quad \eta_1\in]y,y+k[.\]</span> Jeżeli <span class="math inline">\(h\to 0\)</span>, <span class="math display">\[\left( \frac{\partial^2 f}{\partial y \partial x} (\xi,\eta) = \frac{\partial^2 f}{\partial x \partial y} (\xi_1,\eta_1) \right)
,\]</span> to <span class="math display">\[\xi \to x, \xi_1 \to x, \eta \to y, \eta_1 \to y
,\]</span> czyli: <span class="math display">\[\frac{\partial^2 f}{\partial y \partial x} (x,y) = \frac{\partial^2 f}{\partial x \partial y} (x,y)
,\]</span> jeżeli każda z tych wielkości jest ciągła.</p>
<h2 id="wzór-taylora-konstrukcja">Wzór Taylora (konstrukcja)</h2>
<p>Niech <span class="math inline">\(f: \mathcal{O}\to\mathbb{R}, \mathcal{O}\subset \mathbb{R}^n\)</span> - otwarty<br />
<span class="math inline">\(\varphi(t) = f(x_0+th), h\in \mathbb{R}^n, t\in[0,1]\)</span>.<br />
Dla <span class="math display">\[h=\left [ \begin{matrix}
h^1\\
\vdots\\
h^n\end{matrix}\right ] ,
x_0 = \left [ \begin{matrix}
x_0^1\\
\vdots\\
x_0^n\end{matrix}\right ],
\varphi(t) = f(x_0^1+th^1,x_0^2+th^2,\dots,x_0^n+th^n)
,\]</span> mamy <span class="math display">\[\begin{aligned}
    \frac{\partial \varphi}{\partial t} &amp;= \left .\frac{\partial f}{\partial x^1} \right |_{x=x_0+th} h_1 + \left .\frac{\partial f}{\partial x^2} \right |_{x=x_0+th} h_2 + \dots + \left .\frac{\partial f}{\partial x^n} \right |_{x=x_0+th} h_n = \left .\sum_{i=1}^n \frac{\partial f}{\partial x^i} \right |_{x_0+th}h_i\\
    \frac{\partial^2 \varphi}{\partial t^2} &amp;= \left .\sum_{j=1}^n \sum_{i=1}^n \frac{\partial f}{\partial x^i \partial x^j} \right |_{x_0+th} h_j h_i\\
    &amp;\vdots\\
    \frac{\partial^k \varphi}{\partial t^k} &amp;= \sum_{i^1,\dots,i^k}^n \frac{\partial^{(k)} f}{\partial x^{i_{1}} \dots \partial x^{i}} h_{i_1}\dots h_{i_k}\end{aligned}\]</span></p>
<p><span class="math display">\[\varphi(t) = \varphi(0) = \varphi&#39;(0)(t-0)+\frac{\varphi&#39;&#39;(0)}{2!}(t-0)^2+\dots+\frac{\varphi^k(0)}{k}(t-0)^k + r(\dots)
,\]</span> czyli: <span class="math display">\[\varphi(1)-\varphi(0) = \varphi&#39;(0)+\frac{\varphi&#39;&#39;(0)}{2!} + \frac{\varphi&#39;&#39;&#39;(0)}{3!} + \dots + \frac{\varphi^k(0)}{k!}+r(\dots)\]</span> <span class="math display">\[f(x_0+h)-f(x_0)=\sum_{i=1}^n \frac{\partial f}{\partial x^i} (x_0) h_i + \frac{1}{2!} \sum_{i,j=1}^n \frac{\partial^2 f}{\partial x^i \partial x^j} (x_0)h_ih_j + \dots \dots\]</span></p>
<h1 id="wykład-12.03.2019">Wykład (12.03.2019)</h1>
<p><strong>Z poprzedniego wykładu:</strong> <span class="math display">\[\begin{aligned}
    f(x_0+h) &amp;= f(x_0) + \sum_{i=1}^n \frac{\partial f}{\partial x^i} (x_0) h^i + \frac{1}{2!}\sum_{\substack{i=1\\ j=1}}^n \frac{\partial^2 f}{\partial x^i \partial x^j} (x_0) h^i h^j  + \dots +  \\
&amp;+ \frac{1}{p!} \sum_{\substack{i_1 = 1\\ \vdots \\ i_p = 1}}^n \frac{\partial^p f}{\partial x^{i_1} \dots \partial x^{i_p+1}} (x_0) h^{i_1} \dots h^{i_p} + R_{p+1} (x_0, h),\end{aligned}\]</span> gdzie reszta wygląda tak: <span class="math display">\[R_{p+1} (h) = \frac{1}{(p+1)!} \sum_{\substack{i_1 = 1\\ \dots \\i_{p+1} = 1}}^n \frac{\partial^{p+1} f}{\partial x^{i_1} \dots \partial x^{i_p+1}} \underset{\substack{0&lt;\theta&lt;1\\ \text{ wersja } \mathbb{R}^n \text{ dla }\\&quot;x_0 &lt; c &lt; x_0 +h&quot;}}{(x_0+\theta h)} h^{i_1} \dots h^{i_{p+1}}.\]</span></p>
<p>(bardzo ważna zależność!)<br />
<span class="math display">\[\lim\limits_{h \to 0}\frac{R_{p+1} (x_0,h)}{||h||^p} \to 0.\]</span></p>
<p><span class="math inline">\(f: \mathbb{R}^2 \to \mathbb{R}, \quad f(x,y) = x^2y^3, f&#39;(x,y) = \Big [ 2xy^3, 3x^2y^2 \Big ]\)</span>.</p>
<p>Jeżeli <span class="math inline">\(h = \left [ \begin{matrix}
h_1\\
h_2\\
 \end{matrix}\right ]\)</span>, to wtedy</p>
<p><span class="math display">\[\begin{aligned}
\sum_{\substack{i=1 \\ j=1}}^2 \frac{\partial^2 f}{\partial x^i \partial x^j} h^i h^j
&amp;=\frac{\partial^2 f}{\partial x^1 \partial x^1} h^1 h^1 + \frac{\partial^2 f}{\partial x^1 \partial x^2} h^1 h^2 + \frac{\partial^2 f}{\partial x^2 \partial x^1} h^2 h^1 + \frac{\partial^2 f}{\partial x^2 \partial x^2} h^2 h^2 =\\
&amp;= \Big [ h_1, h_1 \Big ] \left [ \begin{matrix}
\frac{\partial^2 f}{\partial x_1^2}  &amp;\frac{\partial^2 f}{\partial x_1 \partial x_2} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1}  &amp;\frac{\partial^2 f}{\partial x_2^2}  \end{matrix}\right ] \left [ \begin{matrix}
h_1\\
h_2 \end{matrix}\right ]\end{aligned}\]</span> <strong>To czy ta macierz jest uśmiechnięta etc. (dodatnio/ujemnie określona) na algebrze.</strong></p>
<h2 id="minima-i-maksima">Minima i maksima</h2>
<p><strong>Przypomnienie:</strong> Niech <span class="math inline">\(f: \mathcal{O}\to \mathbb{R}, \mathcal{O}\subset \mathbb{R}^n, \mathcal{O}\)</span> - otwarty, <span class="math inline">\(x_0 \in \mathcal{O}\)</span><br />
Mówimy, że <span class="math inline">\(f\)</span> ma w <span class="math inline">\(x_0\)</span> minimum lokalne, jeżeli: <span class="math display">\[\underset{\eta &gt; 0}{\exists} \quad
\underset{\substack{
x\in K(x_0,\eta)\\
K(x_0,\eta) \subset \mathcal{O}\\
x \neq x_0
}}{\forall} \quad f(x) &gt; f(x_0), \underbrace{\left(f(x) &lt; f(x_0)\right)}_{\text{albo maksimum}}.\]</span></p>
<p>Albo inaczej: <span class="math display">\[\underset{\eta &gt; 0}{\exists} \quad\underset{h}{\forall}\quad ||h|| &lt; \eta, \quad x_0+h \in \mathcal{O}, h \neq 0\text{, to wtedy } f(x_0+h)&gt;f(x_0).\]</span></p>
<figure>
<img src="img/fig_12.png" alt="istnieje otoczenie, dla którego f(x)&gt;f(x_0) (nie musi być styczne!)" style="width:50.0%" /><figcaption>istnieje otoczenie, dla którego <span class="math inline">\(f(x)&gt;f(x_0)\)</span> (nie musi być styczne!)</figcaption>
</figure>
<p>jeżeli <span class="math inline">\(f: \mathcal{O} \rightarrow \mathbb{R}, \mathcal{O}\)</span> - otwarty, <span class="math inline">\(x_0 \in \mathcal{O}, f\)</span> - posiada w <span class="math inline">\(x_0\)</span> minimum lub maksimum lokalne, to <span class="math display">\[\frac{\partial f}{\partial x^i} (x_0) = 0, i = 1,\dots,n\]</span> działa tylko w prawo, bo możliwe są punkty przegięcia (siodła)</p>
<p><strong>Uwaga:</strong> jeżeli <span class="math inline">\(f: U\to \mathbb{R}\)</span> i <span class="math inline">\(U\)</span> - domknięta, to należy zbadać zachowanie funkcji osobno na <span class="math inline">\(int(U)\)</span> oraz na <span class="math inline">\(U - \{int(U)\}\)</span></p>
<p>Niech <span class="math inline">\(g_h (t) = f(x_0+th) \text{ i } g: [0,\epsilon [ \to \mathbb{R}\)</span>.<br />
Zauważmy, że jeżeli <span class="math inline">\(f\)</span> ma minimum lub maksimum w <span class="math inline">\(x_0\)</span>, to znaczy, że <span class="math inline">\(g_h (t)\)</span> ma minimum lub maksimum w <span class="math inline">\(t = 0\)</span>, czyli <span class="math display">\[\left . \frac{\partial}{\partial t} g_h(t) \right |_{t=0} = 0,\]</span></p>
<p>czyli dla <span class="math inline">\(x_0 = \big ( x_0^1, x_0^2, \dots, x_0^n),\quad h = \big ( h^1, h^2, \dots, h^n)\)</span></p>
<p><span class="math display">\[\begin{aligned}
    \left . \frac{d}{dt} g_h (t) \right |_{t=0} &amp;= \left .\frac{d}{dt} f(x_0^1 + th^1, \dots, x_0^n + th^n) \right |_{t=0} = \\
&amp;= \frac{\partial f}{\partial x^1} (x_0 + th^1) h^1 + \frac{\partial f}{\partial x^2} (x_0 + th^2) h^2 + \dots + \left .\frac{\partial f}{\partial x^n} (x_0 + th^n) \right |_{t=0}=\\
&amp;=\sum_{i=1}^n \frac{\partial f}{\partial x^i} (x_0) h^i = 0 \quad |\underset{h}{\forall}: ||h|| &lt; \eta
,\end{aligned}\]</span> to znaczy: <span class="math display">\[\frac{\partial f}{\partial x^i} (x_0) = 0,\quad i = 1,\dots,n.\]</span></p>
<p><strong>Uwaga:</strong> jest to warunek konieczny, a nie dostateczny!</p>
<p>Niech <span class="math display">\[\begin{aligned}
    &amp;f: \mathcal{O} \to \mathbb{R},\\
    &amp;\mathcal{O}\subset\mathbb{R}^n,\\
    &amp;x_0\in\mathcal{O}, \quad \mathcal{O} \text{ - otwarty},\\
    &amp;f \in C^{2p} (\mathcal{O}),\\
    &amp;f&#39;(x_0) = 0, f&#39;&#39;(x_0) = 0,\dots,f^{(2p-1)} (x_0) = 0
.\end{aligned}\]</span> oraz spełniony jest warunek <span class="math display">\[\underset{c &gt; 0}{\exists} \quad\underset{\eta &gt; 0}{\exists} \quad\underset{h\in K(x_0,\eta)}{\forall}: \quad \sum_{\substack{i_1 = 1\\ \vdots \\ i_{2p} = 1}}^n \frac{\partial^{(2p)} f}{\partial x^{i_1} \dots \partial x^{i_{2p}}} (x_0) h^{i_1} \dots h^{i_{2p}} \geq c ||h||^{2p} (\leq c||h||^{2p})\]</span> to wtedy <span class="math inline">\(f\)</span> ma w <span class="math inline">\(x_0\)</span> minimum (maksimum) lokalne.</p>
<p>(wersja uproszczona dla minimum i dla <span class="math inline">\(f\)</span> klasy <span class="math inline">\(C^{2p+1}(\mathcal{O}))\)</span>.<br />
Jeżeli <span class="math inline">\(f\)</span> spełnia założenie z twierdzenia, to wtedy <span class="math display">\[\label{eq: eq_5.1}
    f(x_0+h)-f(x_0) = \underbrace{\frac{1}{(2p)!} \sum_{i_1 = 1\ldots i_{2p} = 1}^{2p} \frac{\partial^{(2p)} f(x_0)}{\partial x^{i_1} \dots x^{i_{(2p)}}} h^{i_1} \dots h^{i_{(2p)}}}_{(*)} + r_{2p + 1} (x_0 + h)\]</span> Wiemy też , że <span class="math display">\[\underset{c &gt; 0}{\exists}\quad \underset{\eta &gt; 0}{\exists} \underset{\substack{\text{Chodzi o to, żeby reszta}\\ \text{ nie mogła tego przekroczyć}}}{\quad(\ref{eq: eq_5.1})(*)) \geq c ||h||^{2p}}
    .\]</span> Chcemy pokazać, że <span class="math display">\[\underset{\eta}{\exists} \quad \underset{||h||&lt;\eta}{\forall} \Big | r_{2p+1} (x,h) \Big | \leq \underset{\substack{\text{albo 7,}\\ \text{albo 2019}}}{\frac{c}{2} ||h||^{2p}}
    .\]</span> Czyli chcemy zbadać wielkość: <span class="math display">\[\frac{1}{(2p+1)!} \sum_{i_1 = 1 \ldots i_{2p+1} = 1}^n \underset{0 &lt; \theta &lt; 1}{\frac{\partial^{(2p+1)} f (x_0 + \theta h)}{\partial x^{i_1} \dots \partial x^{i_{(2p+1)}}}} h^{i_1} \dots h^{i_{(2p+1)}} = \left| \text{tu potrzebne założenie, że } f \in C^{2p+1} (\mathcal{O})\right| = r_{2p+1} (x,h)
    .\]</span> Zauważmy, że <span class="math inline">\(\lim\limits_{h \to 0} \frac{r_{2p+1}(x_0, h)}{||h||^{2p}} \to 0\)</span>, ale zatem <span class="math display">\[\underset{M&gt;0}{\forall}\quad \underset{\eta}{\exists}\quad \underset{||h|| &lt; \eta}{\forall} \frac{r_{2p+1}(x_0+h)}{||h||^{2p}} &lt; M
    ,\]</span> czyli <span class="math display">\[\left| \frac{r_{2p+1} (x_0,h)}{||h||^{2p}} \right| &lt; M.\]</span> <span class="math display">\[\underset{M}{\forall}\quad \underset{\eta}{\exists}\quad \underset{||h|| &lt; \eta}{\forall}\quad \Big | r_{2p+1} (x_0,h) \Big | &lt; M \big | \big |h \big | \big |^{2p}\]</span> czyli jak przyjmiemy <span class="math inline">\(M = \frac{c}{2}\)</span> to dostajemy <span class="math display">\[\underset{\eta}{\exists}\quad \underset{||h||&lt;\eta}{\forall}\quad f(x_0+h)-f(x_0) \geq \frac{c}{2} ||h||^{2p}\]</span></p>
<p><strong>Uwaga:</strong> dlaczego warunek <span class="math inline">\(( - \big| - ) &gt; c ||h||^{2p}\)</span>, a nie po prostu <span class="math inline">\(( - \big| - ) &gt; 0\)</span>?</p>
<p><span class="math display">\[f(x,y) = x^2 + y^4,\quad \frac{\partial f}{\partial x} = 2x,\quad \frac{\partial f}{\partial y} = 4y^3
    .\]</span> <span class="math display">\[f&#39;() = 0 \iff (x,y) = (0,0)\]</span> Badamy: <span class="math inline">\(f(0+h) - f(0) = \big [ h_1, h_2 \big ] \left [ \begin{matrix}
2 &amp;0\\
0 &amp;2y^2\\
\end{matrix}\right ]
\left [ \begin{matrix}
h_1\\
h_2\\\end{matrix}\right ]
= \big [ h_1, h_2 \big ] \left [ \begin{matrix}
2 &amp;0\\
0 &amp;0\\ \end{matrix}\right ]
\left [ \begin{matrix} h_1\\
h_2\end{matrix}\right ] = 2h_1^2\)</span> Czyli <span class="math inline">\(f(0+h) - f(0) \star  2h_1^2\)</span> - minimum? maksimum? - zależy w którą stronę.<br />
<span class="math inline">\(h = \left [ \begin{matrix}
h_1\\
0\\
\end{matrix}\right ]\)</span> - minimum, <span class="math inline">\(h = \left [ \begin{matrix}
0\\
h_2\\
\end{matrix}\right ]\)</span> - równo, coś takiego - punkt siodłowy.<br />
Widzimy zatem, że nie jest spełniony warunek <span class="math display">\[\underset{c}{\exists}\left[ h_1, h_2 \right ] \left[ \begin{matrix}
2 &amp;0\\
0 &amp;0\\
\end{matrix}\right ]
\left [ \begin{matrix}
h_1\\
h_2\\
    \end{matrix}\right ] \geq c \left\Vert h \right\Vert ^2
,\]</span> bo dla <span class="math display">\[h = \left [ \begin{matrix}
0\\
h_2\\
    \end{matrix}\right ] \quad 0 \not\geq c \left\Vert \left [ \begin{matrix}
0\\
h_2\\
\end{matrix}\right ] \right\Vert\]</span></p>
<h2 id="kilka-fajnych-zastosowań">Kilka fajnych zastosowań</h2>
<p><span class="math display">\[\frac{mv^2}{2} =
\left [ \begin{matrix}
 &amp;v &amp;\\ \end{matrix}\right ]
\left [ \begin{matrix}
    \frac{m}{2}\\
&amp;\frac{m}{2}\\
   &amp; &amp; \ddots\\
    \end{matrix}\right ]  \left [ \begin{matrix}
\\
v\\
 \\\end{matrix}\right ]\]</span> <span class="math display">\[\frac{I \omega^2}{2} =
    \left [ \begin{matrix}
    &amp;\omega &amp;
    \\ \end{matrix}\right ]
    \left [ \begin{matrix}
    \frac{I}{2} &amp; &amp;\\
     &amp;\frac{I}{2} &amp;\\
     &amp; &amp;\ddots\\
     \end{matrix}\right ] \left [ \begin{matrix}
    \\
    \omega\\
    \\
    \end{matrix}\right ]\]</span></p>
<h1 id="wykład-15.03.2019">Wykład (15.03.2019)</h1>
<figure>
<img src="img/fig_14.png" alt="Inne podejście: iterujemy funkcję na jej wyniku" style="width:60.0%" /><figcaption>Inne podejście: iterujemy funkcję na jej wyniku</figcaption>
</figure>
<p>Niech <span class="math inline">\(L: V\to W, L\)</span> - liniowe, <span class="math inline">\((V,||.||_v),(W,||.||_w)\)</span> - unormowane. Mówimy, że <span class="math inline">\(L\)</span> jest ograniczone, jeżeli <span class="math display">\[\underset{A&gt;0}{\exists}\quad\underset{x\in V}{\forall} ||L(x)||_w \leq A||x||_v.\]</span></p>
<p><span class="math inline">\(\text{dla }
\left [ \begin{matrix}
x\\
y\\
\end{matrix}\right ] \in \mathbb{R}^2, f(x,y) : \mathbb{R}^2 \to \mathbb{R}^2\)</span><br />
<span class="math display">\[\underset{A}{\exists}\quad \forall
    \left [ \begin{matrix}
    x\\
    y\\
     \end{matrix}\right ] \in \mathbb{R}^2,\quad \left\Vert \left [ \begin{matrix}
    \frac{1}{2} &amp;0\\
    0 &amp;\frac{1}{2}\\
     \end{matrix}\right ]
    \left [ \begin{matrix}
    x\\
    y\\ \end{matrix}\right ] \right\Vert
    \leq A \left\Vert \left [ \begin{matrix}
    x\\
    y\\ \end{matrix}\right ]
    \right\Vert\]</span></p>
<p>ale <span class="math display">\[\forall \left [ \begin{matrix}
    x\\
    y\\
     \end{matrix}\right ] \in \mathbb{R}^2, \Bigg | \Bigg |
    \left [ \begin{matrix}
    \frac{1}{2} &amp;0\\
    0 &amp; \frac{1}{2}\\
     \end{matrix}\right ]
    \left [ \begin{matrix} x\\
    y\\ \end{matrix}\right ]
    \Bigg | \Bigg | &lt; \frac{1}{2} \Bigg | \Bigg |  \left [ \begin{matrix}
    x\\
    y\\ \end{matrix}\right ] \Bigg | \Bigg|\]</span></p>
<p>(<span class="math inline">\(L\)</span> - ograniczone) <span class="math inline">\(\iff\)</span> (<span class="math inline">\(L\)</span> - ciągłe)</p>
<p><span class="math inline">\(\impliedby\)</span><br />
Wiemy, że <span class="math display">\[\underset{\varepsilon &gt; 0}{\forall}, \underset{\delta}{\exists}, \underset{x,x&#39;\in V}{\forall},\quad ||x-x&#39;||_v &lt; \delta \implies ||L(x) - L(x&#39;)||(*)&lt; \varepsilon
,\]</span> chcemy pokazać, że: <span class="math display">\[\underset{A&gt;0}{\exists}.\underset{x,x&#39;\in V}{\forall}\quad ||L(x-x&#39;)|| \leq A||x-x&#39;||,\]</span> zatem wiemy, że para <span class="math inline">\((\varepsilon, \delta)\)</span> spełniająca warunek <span class="math inline">\((*)\)</span> istnieje.<br />
Ale <span class="math display">\[||L(x-x&#39;)|| = \underbrace{\Bigg |\Bigg |L\left ( \frac{x-x&#39;}{||x-x&#39;||}\right ) \frac{\delta}{2}\Bigg |\Bigg | \frac{||x-x&#39;|| 2}{\delta}}_{\text{własność liniowości i normy}} \leq \varepsilon \frac{||x-x&#39;|| 2}{\delta}\]</span> Co wiemy o <span class="math inline">\(\left\Vert \frac{x-x&#39;}{||x-x&#39;||} \frac{\delta}{2} \right\Vert_v &lt; \delta?\)</span></p>
<p><span class="math display">\[\underset{x,x&#39;\in V}{\forall}||L(x-x&#39;)||_w \leq \frac{2 \varepsilon}{\delta} ||x-x&#39;||_v\]</span> Szukane <span class="math inline">\(A=\frac{2\varepsilon}{\delta}\)</span> istnieje!</p>
<p><span class="math inline">\(\implies\)</span><br />
Wiemy, że <span class="math display">\[\label{eq:d1}
    \underset{A}{\exists}\quad \underset{x,x&#39;\in V}{\forall}\quad \left\Vert L(x-x&#39;)\right\Vert \leq A\left\Vert x-x&#39; \right\Vert\]</span></p>
<p>Chcemy pokazać, że jeżeli <span class="math inline">\(x_n\to x_0\)</span>, to <span class="math inline">\(L(x_n)\to L(x_0)\)</span>, ale <span class="math display">\[0 \leq ||L(x_n)-L(x_0)||_w = ||L(x_n-x_0)||_w \leq A||x_n-x_0||(\text{ bo (\ref{eq:d1})})\]</span> <span class="math display">\[0\leq ||L(x_n) - L(x_0)||_w \leq A||x_n - x_0||(\text{ wszystko dąży do 0)}\]</span></p>
<p>Wielkość <span class="math display">\[\underset{A}{inf} \{\underset{x\in V}{\forall}||L(x)||_w \leq A||x||_v\}\]</span> nazywamy normą odwzorowania <span class="math inline">\(L\)</span> i oznaczamy <span class="math inline">\(A\overset{\text{ozn}}{=}||L||\)</span>.</p>
<p>Niech <span class="math inline">\(U\subset \mathbb{R}^m\)</span> - jest zbiorem wypukłym, jeżeli <span class="math display">\[\underset{a,b\in U}{\forall}\quad [a,b]\overset{\text{def}}{=} \left \{ a(1-t)+bt, t\in[0,1] \right \} \subset U\]</span></p>
<figure>
<img src="img/fig_15.png" alt="zbiór wklęsły" style="width:60.0%" /><figcaption>zbiór wklęsły</figcaption>
</figure>
<figure>
<img src="img/fig_16.png" alt="zbiór wypukły" style="width:60.0%" /><figcaption>zbiór wypukły</figcaption>
</figure>
<p>Niech <span class="math inline">\(f: U\subset \mathbb{R}^m \to \mathbb{R}^n, U\)</span> - wypukłe, <span class="math display">\[\underset{M}{\exists}\quad \underset{x\in U}{\forall}||f&#39;(x)||\leq M
    ,\]</span> to <span class="math display">\[\underset{a,b\in U}{\forall}||f(b)-f(a)||_n \leq M||b-a||_m\]</span></p>
<p>(jakiekolwiek skojarzenia z Twierdzeniem Lagrange zupełnie przypadkowe *wink* *wink*)</p>
<p>niech <span class="math inline">\(\gamma (t) = a(1-t) + bt, t\in [0,1], \quad g(t) = f(\gamma (t)),g: \mathbb{R}^1 \to \mathbb{R}^n\)</span>, czyli <span class="math display">\[g(t) = \left [ \begin{matrix}
    g_1(t)\\
    g_2(t)\\
    \vdots\\
    g_n(t)\\ \end{matrix}\right ]
,\]</span> zatem <span class="math display">\[\begin{aligned}
        \left \Vert g(1)-g(0)\right \Vert &amp;= \left \Vert \left [ \begin{matrix}
    g_1(1) - g_1(0)\\
    g_2(1) - g_2(0)\\
    \vdots\\
    g_n(1) - g_n(0)\\ \end{matrix}\right ] \right \Vert \overset{\text{Tw. Lagrange!}}{=}\\
        &amp;=\left\Vert \underset{0&lt;c_i&lt;1}{\left [ \begin{matrix}
    g_1&#39;(c_1)(1-0)\\
    g_2&#39;(c_2)(1-0)\\
    \vdots\\
        g_n&#39;(c_n)(1-0)\\ \end{matrix}\right ]}\right\Vert \leq \left\Vert \left [ \begin{matrix}
    g_1&#39;(c_1)\\
    g_2&#39;(c_2)\\
    \vdots\\
    g_n&#39;(c_n)\\ \end{matrix}\right ] \right\Vert \left\Vert 1 - 0 \right\Vert
    \end{aligned}\]</span></p>
<p><span class="math display">\[\text{Ale } g&#39;(t) = f&#39;(\gamma(t))\gamma&#39;(t) \to \left\Vert g&#39;(t)\right\Vert = \left\Vert f&#39;(\gamma(t))(b-a)\right\Vert \leq \left\Vert f&#39;(\gamma(t))\right\Vert \left\Vert b-a \right\Vert \underset{\text{z zał. stw.}}{\leq} M\]</span> <span class="math display">\[\text{Czyli }\underset{t\in[0,1]}{\forall}\left\Vert g&#39;(t) \right\Vert \leq M\left\Vert b-a \right\Vert \implies \left\Vert f(b) - f(a) \right\Vert \leq M \left\Vert b - a \right\Vert\]</span></p>
<p>Niech <span class="math inline">\(X\)</span> - unormowana: <span class="math inline">\(P: X\to X, P\)</span> - ciągła na <span class="math inline">\(X\)</span>.<br />
Interesuje nas zbieżność ciągów typu <span class="math inline">\(\{x_0, P(x_0), P(P(x_0)),\dots \}, x_0\in X\)</span><br />
<span class="math inline">\(\tilde x \in X\)</span> nazywamy punktem stałym, jeżeli <span class="math inline">\(P(\tilde x) = \tilde x\)</span></p>
<p><img src="img/fig_17.png" alt="image" style="width:50.0%" /></p>
<p>Jeżeli ciąg <span class="math inline">\(\{x_0, P(x_0), \dots \}\)</span> - zbieżny i <span class="math inline">\(P\)</span> - ciągłe, to jest on zbieżny do punktu stałego.</p>
<p>Niech <span class="math inline">\(x_n = P^{(n)} (x_0)\)</span>. Wiemy, że <span class="math inline">\(x_n\)</span> - zbieżny, oznaczmy granicę tego ciągu przez <span class="math inline">\(\tilde x\)</span>. Mamy: <span class="math display">\[\label{eq:d2}
    \underset{\varepsilon_1 &gt; 0}{\forall}\quad \underset{N_1}{\exists}\quad \underset{n&gt;N_1}{\forall}\quad d(x_n,\tilde x) &lt; \varepsilon_1\]</span> <span class="math display">\[\label{eq:d3}
    \underset{\varepsilon_2&gt;0}{\forall}\quad \underset{N_2}{\exists}\quad \underset{n&gt;N_2}{\forall}\quad d(x_{n-1},\tilde x) &lt; \varepsilon_2\]</span> <span class="math inline">\(P\)</span> - ciągłe, czyli <span class="math display">\[\underset{\varepsilon&gt;0}{\forall}\quad \underset{\delta}{\exists}\quad \underset{x&#39;}{\forall}:\quad d(x,x&#39;)&lt;\delta \implies d(P(x),P(x&#39;))&lt;\varepsilon \text{, bo (\ref{eq:d2})}\]</span> Chcemy pokazać, że <span class="math display">\[\label{eq:d4}
    \underset{\varepsilon&gt;0}{\forall}\quad d(\tilde x,P(\tilde x))&lt;\varepsilon\]</span> Ale <span class="math display">\[\label{eq:ep}
    d(\tilde x, P(\tilde x)) \leq d(\tilde x,x_n) + d(x_n, P(\tilde x)) = d(\tilde x, x_n) + d(P(x_{n-1}),P(\tilde x)) &lt; \varepsilon + \varepsilon = 2 \varepsilon\]</span> <span class="math display">\[\text{Ale z (\ref{eq:d2}) wynika, że }
    \underset{\varepsilon&gt;0}{\forall}\quad \underset{\delta}{\exists}\quad d(x_{n-1},\tilde x) &lt; \delta \implies d(P(x_{n-1}),P(\tilde x)) &lt; \varepsilon\]</span> Zatem znając <span class="math inline">\(\varepsilon\)</span> z (<a href="#eq:d4" data-reference-type="ref" data-reference="eq:d4">[eq:d4]</a>) przyjmujemy <span class="math inline">\(\varepsilon_1 = \varepsilon\)</span>, oprócz tego znajdujemy <span class="math inline">\(\delta\)</span> przyjmując <span class="math inline">\(\varepsilon_1 = \varepsilon\)</span>, a potem położymy <span class="math inline">\(\varepsilon_2 = \delta\)</span> z (<a href="#eq:d3" data-reference-type="ref" data-reference="eq:d3">[eq:d3]</a>) i dzięki temu mamy (<a href="#eq:ep" data-reference-type="ref" data-reference="eq:ep">[eq:ep]</a>)</p>
<p>Niech <span class="math inline">\(X\)</span> - przestrzeń metryczna, odwzorowanie <span class="math inline">\(P: X\to X\)</span> nazywamy zwężającym, jeżeli: <span class="math display">\[\label{eq:zwezajace}
        \underset{q\in [0,1[}{\exists}\quad \underset{x,y\in X}{\forall} d(P(x),P(y)) \leq q d(x,y)\]</span></p>
<p>(Zasada Banacha o lustrach)<br />
Jeżeli <span class="math inline">\(P: X \to X, P\)</span> - zwężające, to <span class="math display">\[\begin{aligned}
\label{eq:banach}
        &amp;\text{1. } \underset{x_0 \in X}{\forall}\quad \{x_0,P(x_0),P(P(x_0)),\dots\}) \text{ - Zbieżny do punktu stałego } \tilde x\\
        &amp;\text{2. Istnieje tylko jedno }\tilde x\\
        &amp;\text{3. } \underset{m}{\forall}\quad d(x_m,\tilde x) &lt; \frac{q^m}{1-q} d(x_1, x_0)
    \end{aligned}\]</span></p>
<p><strong>(Uwaga)</strong><br />
(<span class="math inline">\(P\)</span> - nie musi być ciągłe) - potem się okaże, że ciągłość gdzieś tutaj siedzi <em>implicite</em><br />
- lustra w łazience koło sali 1.01 <span class="math inline">\(\rightarrow\)</span> można stanąć tak, że jedno jest przed tobą a drugie za tobą i wtedy te odbicia się ciągną w nieskończoność i zbiegają do punktu<br />
- telewizor + kamera która go nagrywa a on wyświetla ten obraz<br />
- mapa położona na podłodze zawiera dokładnie jeden punkt, który się pokrywa z miejscem na którym leży</p>
<p>ad. 2<br />
Załóżmy, że <span class="math display">\[\underset{\tilde x_1, \tilde x}{\exists} P(\tilde x_1) = \tilde x_1, P(\tilde x_2) = \tilde x_2, \tilde x_1 \neq \tilde x_2\]</span> Wtedy <span class="math display">\[d(\tilde x_1, \tilde x_2) = d(P(\tilde x_1),P(\tilde x_2)) &lt; qd(\tilde x_1, \tilde x_2)\]</span> Dalej: <span class="math display">\[d(\tilde x_1, \tilde x_2) \leq q d(\tilde x_1, \tilde x_2)\text{, ale }0\leq q \leq 1,\tilde x_1 \neq \tilde x_2 \implies \text{ sprzeczność! }\]</span></p>
<p><span class="math display">\[\begin{aligned}
    d(x_{n+1},x_n) &amp;= d(P(x_n),P(x_{n-1})) \leq qd(x_n, x_{n-1}) = qd(P(x_{n-1}),P(x_{n-2})) \leq\\
    &amp;\leq q^2 d(x_{n-1},x_{n-2}) \leq q^n d(x_1,x_0)
.\end{aligned}\]</span> Co, jeżeli zamiast <span class="math inline">\(n+1\)</span> weźmiemy <span class="math inline">\(n+m\)</span>? <span class="math display">\[\begin{aligned}
    d(x_{n+m},x_n) &amp;\leq d(x_{n+m},x_{n+m+1}) + d(x_{n+m-1},x_n) \leq\\
    &amp;\leq d(x_{n+m},x_{n+m-1}) + d(x_{n+m-1},x_{n+m-2}) + d(x_{n+m-2},x_n)\leq\\
    &amp;\leq \dots\leq d(x_{n+m},x_{n+m-1}+ \dots+d(x_{n+1},x_n)\leq \\
    &amp;\leq (q^{n+m-1} + \dots + q^{n+2} + q^{n+1} + q^n)d(x_1,x_0) \leq\\
    &amp;\leq q^n \left ( \frac{1-q^n}{1-q} \right ) d(x_1,x_0) \underset{0\leq q&lt;1}{\leq} \frac{q^n}{1-q} d(x_1,x_0)
.\end{aligned}\]</span> Czyli <span class="math inline">\(d(x_{n+m},x_n) \leq \frac{q^n}{1-q} d(x_1,x_0)\)</span><br />
</p>
<p>Skoro <span class="math inline">\(X\)</span> - zupełna, to jeżeli <span class="math inline">\(x_n\)</span> - Cauchy, to znaczy, że jest zbieżny w <span class="math inline">\(X\)</span>. Czyli czy <span class="math display">\[\underset{\varepsilon&gt;0}{\forall}\underset{N}{\exists}\underset{n,m&gt;N}{\forall}\quad d(x_n,x_m) &lt;\varepsilon ?\]</span></p>
<p>Załóżmy, że <span class="math inline">\(m&gt;n\)</span> i <span class="math inline">\(m=n+k\)</span>. Wtedy <span class="math display">\[\underset{\varepsilon&gt;0}{\forall}\underset{N}{\exists}\underset{n&gt;N}{\forall}\quad d(x_n,x_{n+k}) &lt; \varepsilon? \text{ TAK!}\]</span> Dla <span class="math inline">\(N\)</span> takiego, że <span class="math inline">\(\frac{q^N}{1-q} d(x_1,x_0) &lt; \varepsilon\)</span>. Stąd wiadomo, że <span class="math inline">\(x_n\)</span> - Cauchy, czyli jest zbieżny. <span class="math inline">\(x_n \to \tilde x\)</span>, zatem jeżeli <span class="math inline">\(d(x_{n+m},x_n)\leq \frac{q^n}{1-q} d(x_1,x_0) \rightarrow d(\tilde x,x_n) \leq \frac{q^n}{1-q} d(x_1,x_0).\)</span></p>
<h1 id="wykład-19.03.2019">Wykład (19.03.2019)</h1>
<p>(o lokalnej odwracalności)<br />
Niech <span class="math display">\[\begin{aligned}
        &amp;f: E \to E, E\text{ - otwarty, } E\subset \mathbb{R}^n,f \in \mathcal{C}^1 (E), \\
        &amp;\underset{a,b\in E}{\exists}:f(a) = b \text{ i } f&#39;(a) \text{ - odwracalna } (det(f&#39;(a))\neq 0),
    \end{aligned}\]</span> to wtedy: <span class="math display">\[\begin{aligned}
        &amp;\text{1. } \underset{U,V\subset E}{\exists}\quad \underset{a\in U, b\in V}{\exists}\quad U,V \text{ - otwarte, }f \text{ - bijekcja między } U,V\\
        &amp;\text{2. } \underset{g: V\to U}{\exists}\quad \underset{x\in V}{\forall}\quad f(g(x)) = x,\\
        &amp;\text{3. } g \in \mathcal{C}^1(V).
    \end{aligned}\]</span></p>
<p><strong>Uwaga:</strong> dowód składa się z trzech części:</p>
<ul>
<li><p>Pokażemy, że <span class="math inline">\(\underset{U,V}{\exists}: f \text{ - bijekcja na }U,V\)</span></p></li>
<li><p>Pokażemy, że <span class="math inline">\(U,V\)</span> - otwarte</p></li>
<li><p>Pokażemy, że <span class="math inline">\(\underset{g: V\to U}{\exists}, g\)</span> - różniczkowalna na <span class="math inline">\(V\)</span> i ciągła.</p></li>
</ul>
<p><span class="math inline">\(f(x,y) = \left [ \begin{matrix}
e^x \cos y\\
e^x \sin y\\ \end{matrix}\right ]
,
f&#39;(x,y) = \left [ \begin{matrix}
e^x \cos y &amp;-e^x \sin y\\
e^x \sin y &amp;e^x \cos y\\
 \end{matrix}\right ]\)</span></p>
<p><span class="math inline">\(det(f&#39;(x,y)) = e^{2x} \neq 0\)</span>, ale <span class="math inline">\(f(x,y) = f(x,y+2\pi)\)</span></p>
<p>(czyli funkcja jest okresowa)</p>
<p><strong>Część I</strong><br />
Szukamy <span class="math inline">\(U,V: f\)</span> - bijekcja miedzy <span class="math inline">\(U\)</span> i <span class="math inline">\(V\)</span>.<br />
Skoro <span class="math inline">\(f&#39;(a)\)</span> - odwracalne, to znaczy, że <span class="math inline">\(\underset{(f&#39;(a))^{-1}}{\exists}\)</span>, zatem <span class="math display">\[\underset{\lambda}{\exists}: 2\lambda \Vert (f&#39;(a))^{-1} \Vert = 1.\]</span></p>
<p>Wiemy, że <span class="math inline">\(f&#39;(x)\)</span> - ciągła w <span class="math inline">\(x=a\)</span>, czyli <span class="math display">\[\underset{\varepsilon&gt;0}{\forall}.\underset{\delta}{\exists}.\underset{x}{\forall},d(x,a)&lt;\delta \implies \Vert f&#39;(x) - f(a) \Vert &lt; \varepsilon\]</span> Połóżmy <span class="math inline">\(\varepsilon = \lambda\)</span>.<br />
Oznacza to, że <span class="math display">\[\underset{\delta_\lambda}{\exists}.\underset{x}{\forall}x\in K(a,\delta_\lambda) \implies \Vert f&#39;(x) - f&#39;(a) \Vert &lt; \lambda\]</span> Więc <span class="math inline">\(U = K(a,\delta_\lambda)\)</span>, niech <span class="math inline">\(V = f(U)\)</span>. Chcemy pokazać, że <span class="math inline">\(f\)</span> - bijekcja między <span class="math inline">\(U\)</span> i <span class="math inline">\(V\)</span>.<br />
Wprowadźmy funkcję pomocniczą: <span class="math display">\[\varphi_y(x) = x+[f&#39;(a)]^{-1} (y-f(x)), x,y\in E\]</span></p>
<p>Co by było gdyby <span class="math inline">\(\varphi_y(x)\)</span> posiadała punkt stały? (jakie własności <span class="math inline">\(x\)</span> by z tego faktu wynikały)<br />
dla <span class="math inline">\(x\in U, y\in V, (y\in f(a))?\)</span></p>
<p>Z zasady Banacha wiemy, że odwzorowanie zwężające ma dokładnie jeden punkt stały, czyli <span class="math display">\[\underset{y\in V}{\forall}\quad \underset{x\in U}{\exists}: f(x) = y\]</span></p>
<p><strong>Uwaga:</strong> o <span class="math inline">\(f\)</span> - z taką własnością mówimy, że jest 1-1 na <span class="math inline">\(U\)</span>.<br />
Policzmy <span class="math inline">\(\varphi_y&#39;(x)\)</span> <span class="math display">\[\varphi_y &#39;(x) = \mathbb{I} + (f&#39;(a))^{-1} (-f&#39;(x)) = (f&#39;(a))^{-1}(f&#39;(a) - f&#39;(x)),\]</span> więc <span class="math display">\[\begin{aligned}
        &amp;\Vert \varphi_y &#39;(x) \Vert = \Vert f&#39;(a)^{-1} (f&#39;(a) - f&#39;(x))\Vert \leq\\
        &amp;\leq \Vert (f&#39;(a)^{-1})\Vert \Vert f&#39;(a) - f&#39;(x) \Vert \leq\\
        &amp;\leq \underset{x\in U}{\forall}\frac{1}{2\lambda} \lambda = \frac{1}{2}
    .\end{aligned}\]</span></p>
<p>Pamiętamy, że jeżeli <span class="math inline">\(\underset{M}{\exists}\Vert\varphi_y &#39;(x)\Vert \leq M\)</span>, to <span class="math inline">\(\underset{x,y}{\forall}\Vert \varphi(x) - \varphi(y)\Vert &lt; M\Vert x-y \Vert\)</span></p>
<p>Zatem skoro <span class="math inline">\(\Vert \varphi_y &#39;(x) \Vert \leq \frac{1}{2}\)</span>, to <span class="math display">\[\underset{x_1,x_2\in U}{\forall}\Vert \varphi_y(x_1) - \varphi_y (x_2) \Vert \leq \frac{1}{2} \Vert x_1 - x_2 \Vert,\]</span> więc <span class="math inline">\(\varphi\)</span> - zwężający na <span class="math inline">\(U\)</span>, więc posiada dokładnie jeden punkt stały <span class="math inline">\(\underset{y\in V}{\forall}\)</span>. Zatem <span class="math inline">\(f\)</span> - bijekcja między <span class="math inline">\(U\)</span> i <span class="math inline">\(V\)</span>. <span class="math inline">\(\Box\)</span></p>
<p><strong>Część II</strong><br />
Zbiór <span class="math inline">\(U\)</span> - otwarty (bo tak go zdefiniowaliśmy) <span class="math inline">\(U = K(a,\delta_1)\)</span>, więc <span class="math display">\[\underset{x_0\in U}{\exists}\quad \underset{r}{\exists}K(x_0,r)\subset U\]</span> lub równoważnie <span class="math display">\[\Vert x-x_0 \Vert \leq r \land x\in U
    .\]</span> Chcemy pokazać, że dla <span class="math inline">\(y_0 = f(x_0) \quad\exists\quad K(y_0,\lambda r)\subset V\)</span>, czyli że <span class="math inline">\(V\)</span> - otwarty.<br />
</p>
<figure>
<img src="img/fig_18.png" alt="Trochę jak listy do św. Mikołaja (??)" style="width:80.0%" /><figcaption>Trochę jak listy do św. Mikołaja (??)</figcaption>
</figure>
<p><span id="fig:fig_18" label="fig:fig_18">[fig:fig_18]</span></p>
<p>Weźmy <span class="math inline">\(y\in K(y_0,\lambda r)\)</span>. Zauważmy, że <span class="math inline">\(\varphi_{y_1}(x_1)\)</span> - zwężające, jeżeli <span class="math inline">\(y_1\in V, x_1\in U\)</span><br />
Jeżeli pokażemy, że dla <span class="math inline">\(\Vert y-y_0\Vert &lt; \lambda r, \varphi_y(x)\)</span> - zwężająca na <span class="math inline">\(K(x_0,r)\subset U\)</span>, to będziemy wiedzieli, że <span class="math inline">\(\Vert y - y_0 \Vert &lt; \lambda r\)</span> oraz <span class="math inline">\(y\in V \iff K(y_0,\lambda r)\subset V\)</span></p>
<figure>
<img src="img/fig_19.png" alt="Nie ok." style="width:60.0%" /><figcaption>Nie ok.</figcaption>
</figure>
<p><span id="fig:fig_19" label="fig:fig_19">[fig:fig_19]</span></p>
<p>Żeby pokazać, że <span class="math inline">\(\varphi_y(x)\)</span> - zwężające na <span class="math inline">\(K(x_0,r)\)</span>, zbadamy tę wielkośc dla <span class="math inline">\(x\in K(x_0,r)\)</span>. <span class="math inline">\(\Vert \varphi_y(x) - x_0 \Vert\)</span>, chcielibyśmy, aby <span class="math inline">\(\Vert \varphi_y(x) - x_0 \Vert \leq r\)</span> i <span class="math inline">\(\Vert y - y_0 \Vert &lt; \lambda r\)</span>, ale z drugiej strony <span class="math display">\[\Vert \varphi_y(x) - x_0 \Vert = \Vert \varphi_y(x) - \varphi_y(x_0) + \varphi_y(x_0) - x_0 \Vert \leq \Vert \varphi_y(x) - \varphi_y(x_0) \Vert + \Vert \varphi y(x_0 - x_0) \Vert.\]</span></p>
<p>Ale <span class="math display">\[\Vert \varphi_y(x_0) - x_0 \Vert \leq \Vert (f&#39;(a))^{-1} \Vert \Vert y - y_0 \Vert \leq \frac{1}{2\lambda} \lambda r = \frac{r}{2},\]</span> więc <span class="math display">\[\Vert \varphi_y(x) - x_0 \Vert \leq r,\]</span> jeżeli <span class="math display">\[\Vert y - y_0 \Vert &lt; \lambda r, \Vert x - x_0 \Vert \leq r
.\]</span></p>
<p>Stąd wiemy , że punkt stały dla <span class="math inline">\(\varphi_y(x):x\in K(x_0,r)\)</span> należy do <span class="math inline">\(K(x_0,r)\)</span> i <span class="math inline">\(\Vert y - y_0 \Vert &lt;\lambda r\)</span>, zatem <span class="math inline">\(y=f(x)\)</span>, czyli <span class="math inline">\(V\)</span> - otwarty.</p>
<figure>
<img src="img/fig_20.png" style="width:50.0%" />
</figure>
<p><span id="fig:fig_20" label="fig:fig_20">[fig:fig_20]</span></p>
<p><strong>Część III</strong><br />
Szukamy <span class="math inline">\(g: V\to U\)</span><br />
Skoro <span class="math inline">\(f\)</span> - bijekcja między <span class="math inline">\(U\)</span> i <span class="math inline">\(V\)</span>, to znaczy, że <span class="math inline">\(\underset{g: V\to U}{\exists}f(g(x)) = x \underset{x\in V}{\forall}\)</span>.<br />
Chcemy pokazać, że <span class="math inline">\(g(x)\)</span> - różniczkowalne. Wiemy, że <span class="math inline">\(f\)</span> - różniczkowalna w <span class="math inline">\(x\in U\)</span>, czyli <span class="math display">\[\frac{f(x+h) - f(x) - f&#39;(x)h}{\Vert h \Vert} \overset{h\to 0}{\to} 0, x,x+h\in V\]</span> Jeżeli pokażemy, że <span class="math display">\[\frac{g(y+k) - g(y) - [f&#39;(x)]^{-1} k}{\Vert k \Vert} \overset{k\to 0}{\to} 0\]</span> to będziemy wiedzieli, że:<br />
1. <span class="math inline">\(g\)</span> - różniczkowalne dla <span class="math inline">\(y\in V\)</span><br />
2. <span class="math inline">\(g&#39;(y)=[f&#39;(x)]^{-1}\)</span>.<br />
W tym celu pokażemy, że:<br />
1. <span class="math inline">\((\Vert k \Vert \to 0) \implies (\Vert h \Vert \to 0)\)</span><br />
2. <span class="math inline">\([f&#39;(x)]^{-1}\)</span> istnieje dla <span class="math inline">\(x\in U\)</span>. (na razie wiemy, że <span class="math inline">\((f&#39;(a))^{-1}\)</span> istnieje)</p>
<p><em>Ad 1.</em></p>
<p>Zauważmy, że <span class="math display">\[\varphi_y(x+h) - \varphi_y(x) = x+h+[f&#39;(a)]^{-1}(y-f(x+h)) - x - [f&#39;(a)]^{-1}(y-f(x))=\]</span> <span class="math display">\[=h+[f&#39;(a)]^{-1}(y-f(x+h)-y+f(x)) = h - (f&#39;(a))^{-1}(f(x+h) - f(x)),\]</span> <span class="math display">\[czyli \Vert \varphi_y(x+h) - \varphi_y(x) \Vert = \Vert h - (f&#39;(a))^{-1}(k) \Vert \leq \frac{1}{2} \Vert h \Vert,\]</span> zatem <span class="math inline">\(\Vert h - (f&#39;(a))^{-1}k\Vert \leq \frac{1}{2}\Vert h \Vert \implies \Vert k \Vert \geq \Vert h \Vert, k = f(x+h) - f(x)\)</span></p>
<p>Stąd ostatecznie mamy: <span class="math inline">\(\frac{g(y+k)-g(y) - [f&#39;(x)]^{-1}k}{\Vert k \Vert} = [f&#39;(x)]^{-1} \frac{hf&#39;(x) - f(x+h) + f(x)}{\Vert k \Vert} \leq \frac{[f&#39;(x)]^{-1}}{\lambda} \frac{hf&#39;(x) - f(x+h) + f(x)}{\Vert h \Vert} \to 0\)</span>, o ile <span class="math inline">\(\underset{[f&#39;(x)]^{-1}}{\exists}\)</span></p>
<p>skąd wiadomo, że <span class="math inline">\((f&#39;(x))^{-1}\)</span>?<br />
</p>
<p>Wiemy, że <span class="math inline">\(f&#39;(a)\)</span> jest odwracalna, więc <span class="math inline">\((f&#39;(a))^{-1}\)</span> istnieje, <span class="math inline">\(a  \in U\)</span>.<br />
Chcemy pokazać, że <span class="math inline">\(f&#39;(x)\)</span> jest odwracalna dla <span class="math inline">\(x\in U\)</span>. Oznacza to, że <span class="math display">\[0 &lt; \Vert f&#39;(x) y \Vert \text{ dla } y\neq 0, x\in U
.\]</span> Pamiętamy, że <span class="math inline">\(2\lambda \Vert (f&#39;(a))^{-1} = 1\)</span> oraz <span class="math inline">\(U\)</span> - taka, że <span class="math display">\[\underset{x\in U}{\forall} \Vert f&#39;(x) - f&#39;(a) \Vert &lt; \lambda
.\]</span> Zatem <span class="math display">\[0 \le \frac{1}{\Vert (f&#39;(a))^{-1} \Vert } \Vert y \Vert = \Vert (f&#39;(x) + f&#39;(a) - f&#39;(x))y \Vert \le \Vert f&#39;(a) - f&#39;(x) \Vert \Vert y \Vert + \Vert f&#39;(x) \Vert \Vert y \Vert
.\]</span></p>
<p>Dalej <span class="math inline">\(2\lambda \Vert y \Vert \le \lambda \Vert y \Vert + \Vert f&#39;(x) y \Vert\)</span> dla <span class="math inline">\(x\in U\)</span><br />
<span class="math inline">\(0 \le \lambda \Vert y \Vert \le \Vert f&#39;(x) y \Vert\)</span> dla <span class="math inline">\(y = 0\)</span><br />
Czyli <span class="math display">\[\underset{x\in U}{\forall} \Vert f&#39;(x) y \Vert &gt; 0
.\]</span></p>
<h1 id="wykład-22.03.2019">Wykład (22.03.2019)</h1>
<h2 id="zabawki-działające-dzięki-wnioskom-z-tw.-wyżej---funkcje-uwikłane">Zabawki działające dzięki wnioskom z Tw. wyżej - funkcje uwikłane</h2>
<p><span class="math display">\[x+y=1 \quad\text{(a)}
.\]</span> <span class="math display">\[x^2+y^2=1 \quad\text{(b)}
.\]</span> <span class="math display">\[H(x,y) = \sin x e^{xy}+ \tg y - x = 0
.\]</span></p>
<figure>
<img src="img/fig_7.png" alt="(a)" style="width:80.0%" /><figcaption>(a)</figcaption>
</figure>
<figure>
<img src="img/fig_8.png" alt="(b)" style="width:80.0%" /><figcaption>(b)</figcaption>
</figure>
<figure>
<img src="img/fig_9.png" alt="(c)" style="width:60.0%" /><figcaption>(c)</figcaption>
</figure>
<p>Równanie gazowe</p>
<p><span class="math display">\[H(p,V,T) = 0, H: \mathbb{R}^{3} \to \mathbb{R}^{1}
    .\]</span> <span class="math display">\[p(V,T) = 0, \mathbb{R}^{2}\to\mathbb{R}^{1}
    .\]</span> <span class="math display">\[V(p,T) = 0, \mathbb{R}^{2}\to\mathbb{R}^{1}
    .\]</span> <span class="math display">\[T(p,V) = 0, \mathbb{R}^{2}\to\mathbb{R}^{1}
    .\]</span> <em>istnienie przedziałów, w których funkcja uwikłana zadaje inne funkcje</em></p>
<p><span class="math display">\[H(x,y): U\subset\mathbb{R}^2\to\mathbb{R}^1
        .\]</span></p>
<p>Czy istnieje <span class="math inline">\(y(x): H(x,y(x)) = 0\)</span>, dla <span class="math inline">\(x\in V\)</span>?</p>
<p><span class="math display">\[\frac{dH}{dx}(x,y(x)) = \frac{d}{dx}(H(x,y)\circ g(x))
    .\]</span> <span class="math display">\[H&#39; = \left[   \frac{\partial H}{\partial x} ,\frac{\partial H}{\partial y} \right]
    .\]</span> <span class="math display">\[g(x): \mathbb{R}^{1}\to\mathbb{R}^{2}, g(x) = \begin{bmatrix}
        x\\
    y(x)\\\end{bmatrix}, g&#39;(x) = \begin{bmatrix}
1\\
y&#39;(x)\\\end{bmatrix}
    .\]</span> <span class="math display">\[H&#39;(x,y) g&#39;(x) = 0 \implies \frac{\partial H}{\partial x} + \frac{\partial H}{\partial y} \frac{\partial y}{\partial x} =0 \implies \frac{\partial y}{\partial x} = - \frac{\frac{\partial H}{\partial x} }{\frac{\partial H}{\partial y} }
    .\]</span> Więc <span class="math display">\[\frac{\partial y}{\partial x}  = \frac{-\cos y + ye^{xy} -1}{xe^y + \frac{1}{\cos^2 y }}
    .\]</span></p>
<p><span class="math display">\[H(x_{1},x_{2},x_{3},x_{4},x_{5}) = \begin{bmatrix}   2e^{x_1}+x_{2}x_{3} - 4x_{3}+3\\
            x^2 \cos x_{1} - 6x_{1}+2x_{3}-x_{5}, H:\mathbb{R}^{5}\to\mathbb{R}^{3}
\end{bmatrix}
    .\]</span> <span class="math display">\[H(x_{1},\dots,x_{5}) = 0 \text{ może zadać funkcję }g:\mathbb{R}^{3}\to\mathbb{R}^{2}
    .\]</span> <span class="math display">\[x_{4}(x_{1},x_{2},x_{3}),x_{5}(x_{1},x_{2},x_{3})
    .\]</span> <span class="math display">\[g(x_{1},g_{2},g_{3}) = \begin{bmatrix}
        g_1(x_{1},x_{2},x_{3})\\
    g_2(x_{1},x_{2},x_{3}\\
\end{bmatrix}
    .\]</span></p>
<p><span class="math inline">\(H(0,1,3,2,7) = 0\)</span></p>
<p><span class="math display">\[H: \mathbb{R}^{5}\to\mathbb{R}^{2}, H(x_{1},x_{2},y_{1},y_{2},y_{3}) = 0
    .\]</span> <span class="math display">\[H(x_{1},x_{2},y_{1},y_{2},y_{3}) = \begin{bmatrix}
        H_1(x_{1},x_{2},y_{1},y_{2},y_{3})\\
    H_2(x_1,x_2,y_1,y_2,y_2)\\\end{bmatrix}
    .\]</span></p>
<p>Czy <span class="math inline">\(H(x_1,x_2,y_1,y_2,y_3) = 0\)</span> zadaje nam <span class="math display">\[g_1(y_1,y_2,y_3)\\
        .\]</span> <span class="math display">\[g_2(y_1,y_2,y_3)?\]</span></p>
<p>czyli <span class="math inline">\(g(y_1,y_2,y_3) = \begin{bmatrix}
    g_1(y_1,y_2,y_3)\\
g_2(y_1,y_2,y_3)\\\end{bmatrix}, g:\mathbb{R}^{3}\to\mathbb{R}^{2}\)</span></p>
<p><span class="math display">\[H_1(g_1(y_1,y_2,y_3),g_2(y_1,y_2,y_3),y_1,y_2,y_3) = 0
.\]</span> <span class="math display">\[H_2(g_1(y_1,y_2,y_3),g_2(y_1,y_2,y_3),y_1,y_2,y_3) = 0
.\]</span> Szukamy <span class="math inline">\(g&#39;\)</span>. <span class="math display">\[g&#39; = \begin{bmatrix}
\frac{\partial g_1}{\partial y_1} &amp; \frac{\partial g_1}{\partial y_2} &amp;\frac{\partial g_1}{\partial y_3} \\
\frac{\partial g_2}{\partial y_1} &amp;\frac{\partial g_2}{\partial y_2} &amp;\frac{\partial g_3}{\partial y_3} \\\end{bmatrix}
.\]</span></p>
<p><span class="math display">\[\frac{\partial H_1}{\partial x_1} \frac{\partial g_1}{\partial y_2} +\frac{\partial H_1}{\partial x_2} \frac{\partial g_2}{\partial y_1} +\frac{\partial H_1}{\partial y_1} = 0
.\]</span></p>
<p><span class="math display">\[\frac{\partial H_1}{\partial x_1} \frac{\partial g_1}{\partial y_3} +\frac{\partial H_1}{\partial x_2} \frac{\partial g_2}{\partial y_1} +\frac{\partial H_1}{\partial y_1} = 0
.\]</span></p>
<p><span class="math display">\[\frac{\partial H_1}{\partial x_1} \frac{\partial g_1}{\partial y_3} +\frac{\partial H_1}{\partial x_2} \frac{\partial g_2}{\partial y_3} +\frac{\partial H_1}{\partial y_3} = 0
.\]</span></p>
<p><span class="math display">\[\frac{\partial H_2}{\partial x_1} \frac{\partial g_1}{\partial y_1} +\frac{\partial H_2}{\partial x_2} \frac{\partial g_2}{\partial y_1} +\frac{\partial H_2}{\partial y_1} = 0
.\]</span></p>
<p><span class="math display">\[\frac{\partial H_2}{\partial x_1} \frac{\partial g_1}{\partial y_2} +\frac{\partial H_2}{\partial x_2} \frac{\partial g_2}{\partial y_2} +\frac{\partial H_2}{\partial y_2} = 0
.\]</span></p>
<p><span class="math display">\[\frac{\partial H_2}{\partial x_1} \frac{\partial g_1}{\partial y_3} +\frac{\partial H_2}{\partial x_2} \frac{\partial g_2}{\partial y_3} +\frac{\partial H_2}{\partial y_3} = 0
.\]</span> napięcie rośnie (6 równań oho)</p>
<p><span class="math display">\[\underset{H_x&#39;}{\begin{bmatrix}
\frac{\partial H_1}{\partial x_1} &amp;\frac{\partial H_1}{\partial x)2} \\
\frac{\partial H_2}{\partial x_1} &amp;\frac{\partial H_2}{\partial x_2} \end{bmatrix}}
\underset{g&#39;}{\begin{bmatrix}
\frac{\partial g_1}{\partial y_1} &amp;\frac{\partial g_1}{\partial y_2} &amp;\frac{\partial g_1}{\partial y_3} \\
\frac{\partial g_2}{\partial y_1} &amp;\frac{\partial g_2}{\partial y_2} &amp;\frac{\partial g_2}{\partial y_3} \end{bmatrix}}
= -
\underset{H_y&#39;}{\begin{bmatrix}
\frac{\partial H_1}{\partial y_1} &amp;\frac{\partial H_1}{\partial y_2} &amp;\frac{\partial H_1}{\partial y_3} \\
\frac{\partial H_2}{\partial y_1} &amp;\frac{\partial H_2}{\partial y_2} &amp;\frac{\partial H_2}{\partial p_3} \end{bmatrix}}
.\]</span></p>
<p><span class="math display">\[H_x&#39; g&#39; = -H_y&#39; \implies g&#39; = -(H_x&#39;)^{-1}H_y&#39;
.\]</span></p>
<p>(o funkcji uwikłanej)<br />
Niech <span class="math display">\[\begin{aligned}
        &amp;H:E\subset\mathbb{R}^{n+m}\to\mathbb{R}^{m},\\
        &amp;H\in\mathcal{C}^{1} \text{ na } E. (x_0,y_0)\in E, \\
        &amp;H(x_0,y_0)=0, \\
        &amp;(x_0,y_0) = (x_0^1,\ldots,x_0^n,y_0^1,\ldots,y_0^m), \\
        &amp;H \text{ - odwracalna}.
    .\end{aligned}\]</span> Wówczas istnieje <span class="math inline">\(U\subset E\)</span> takie, że <span class="math inline">\((x_0,y_0)\in U, \underset{W\subset \mathbb{R}^{n}}{\exists}\)</span>, że <span class="math display">\[x_0\in W, \underset{x\in W}{\forall} \underset{y}{\exists !} H(x,y) = 0, (x,y) \in U.\]</span> Jeżeli <span class="math inline">\(y= \varphi(x)\)</span>, to <span class="math display">\[\varphi:\mathbb{R}^{n}\to\mathbb{R}^{m} \text{ i } \varphi\in \mathcal{C}^{1}(W),\]</span> <span class="math display">\[\varphi&#39;(x) = -(H_y&#39;)^{-1}H_x&#39;
     .\]</span></p>
<p>Oznaczenia: <span class="math display">\[H(x^1,\ldots,x^n,y^1,\ldots,y^m) = \begin{bmatrix}
        H^1(x^1,\ldots,x^n,y^1,\ldots,y^m)\\
        \vdots\\
        H^2(x^1,\ldots,x^n,y^1,\ldots,y^m)\\
    \end{bmatrix}
    .\]</span> <span class="math display">\[H_y&#39; = \begin{bmatrix}
    \frac{\partial H^1}{\partial y^1} &amp;\ldots&amp;\frac{\partial H^1}{\partial y^n} \\
    \vdots&amp;\ddots&amp;\vdots\\
\frac{\partial H^m}{\partial y^1} &amp;\ldots&amp;\frac{\partial H^m}{\partial y^n} \end{bmatrix},
    H_x&#39; = \begin{bmatrix}
    \frac{\partial H^1}{\partial x^1} &amp;\ldots&amp;\frac{\partial H^1}{\partial x^n} \\
    \vdots&amp;\ddots&amp;\vdots\\
\frac{\partial H^m}{\partial x^1} &amp;\ldots&amp;\frac{\partial H^m}{\partial x^n} \end{bmatrix}
    .\]</span> Wprowadźmy funkcję <span class="math inline">\(F:\mathbb{R}^{n+m}\to\mathbb{R}^{n+m}\)</span> <span class="math display">\[F(x^1,\ldots,x^n,y^1,\ldots,y^m) = \begin{bmatrix}
        x^1\\
    x^2\\
\vdots\\
x^n\\
H^1(x^1,\ldots,x^n,y^1,\ldots,y^m)\\
\vdots\\
H^m(x^1,\ldots,x^n,y^1,\ldots,y^m)\\
\end{bmatrix}
    .\]</span> Jakie własności ma <span class="math inline">\(F\)</span>? <span class="math display">\[F(x_0,y_0) = \begin{bmatrix}
        x_0\\
        y_0\end{bmatrix} = \begin{bmatrix}
        x_0\\
        0\end{bmatrix}
    .\]</span> Ale <span class="math display">\[F&#39; = \begin{bmatrix}
        1&amp;&amp;&amp;&amp;0\\
         &amp;\ddots&amp;&amp;&amp;\\
         &amp;&amp;1&amp;&amp;\\
         &amp;&amp;&amp;&amp;\\
         &amp;H_x&#39;&amp;&amp;&amp;H_y&#39;
    \end{bmatrix}, \det F&#39; = \det H_y&#39;
    .\]</span></p>
<figure>
<img src="img/fig_10.png" alt="" id="fig:" style="width:70.0%" /><figcaption><span label="fig:"></span></figcaption>
</figure>
<p>Jeżeli <span class="math inline">\(H_y&#39;(x_0,y_0)\)</span> - odwracalna, to <span class="math inline">\(F&#39;(x_0,y_0)\)</span> - też. Oznacza to (na podstawie tw. o lokalnej odwracalności), że <span class="math display">\[\underset{U\subset\mathbb{R}^{n+m}}{\exists},
        (x_0,y_0)\in U, \underset{V\subset\mathbb{R}^{n+m}}{\exists} ,(x_0,0)\in V
    .,\]</span> że <span class="math inline">\(F\)</span> jest bijekcją między <span class="math inline">\(U\)</span> i <span class="math inline">\(V\)</span> oraz <span class="math inline">\(\exists F^{-1}:V\to U, F^{-1}\)</span> - różniczkowalna taka, że <span class="math display">\[F^{-1}(x,\alpha) = (a(x,\alpha),b(x,\alpha)), x,\alpha\in V
     .,\]</span> gdzie <span class="math inline">\(a(x,\alpha): \mathbb{R}^{m+n}\to\mathbb{R}^n,\quad b(x,\alpha):\mathbb{R}^{m+n}\to\mathbb{R}^m\)</span></p>
<p>Dla <span class="math inline">\((x&#39;,y&#39;)\in \mathcal{V},\)</span> <span class="math display">\[F^{-1}(x&#39;,y&#39;) = (a(x&#39;,y&#39;),b(x&#39;,y&#39;))
    .\]</span> Wiemy, że <span class="math inline">\(a: \mathbb{R}^{n+m}\to\mathbb{R}^{n}\)</span> i <span class="math inline">\(b: \mathbb{R}^{n+m}\to\mathbb{R}^m\)</span> istnieją i są różniczkowalne, bo <span class="math inline">\(F^{-1}\)</span> istnieje. Co jeszcze wiemy o funkcjach <span class="math inline">\(a\)</span> i <span class="math inline">\(b\)</span>?<br />
Wiemy że <span class="math display">\[(x&#39;,y&#39;) = F(F^{-1}(x&#39;,y&#39;) = F(\underbrace{a(x&#39;,y&#39;)}_{n} , \underbrace{b(x&#39;,y&#39;)}_{m})
    .\]</span> Oznacza to, że <span class="math display">\[a(x&#39;,y&#39;) = x&#39;
    .\]</span> Czyli <span class="math inline">\(a(x&#39;,y&#39;)\)</span> jest identycznością, czyli: <span class="math display">\[(x&#39;,y&#39;) = F(x&#39;,b(x&#39;,y&#39;)) \implies x&#39;=x \implies (x,y&#39;) = F(x,b(x,y&#39;))
    .\]</span> Czyli jeżeli <span class="math inline">\(y=b(x,0)\)</span>, to wtedy <span class="math display">\[F(x,y) = (x,0)\text{, czyli } (x,H(x,y)) = (x,0)
    .\]</span> Czyli dla <span class="math inline">\(y = (x,0)\)</span> otrzymujemy, że <span class="math display">\[H(x,y)=0
    .\]</span> Jeżeli oznaczymy <span class="math inline">\(b(x,0) \overset{\text{ozn}}{=} \varphi(x)\)</span>, to znaczy, że znaleźliśmy funkcję <span class="math inline">\(\varphi(x), \varphi:\mathbb{R}^n\to\mathbb{R}^m\)</span> taką, że <span class="math display">\[H(x,\varphi(x))=0
    .\]</span></p>
<figure>
<img src="img/fig_11.png" alt="" id="fig:" style="width:80.0%" /><figcaption><span label="fig:"></span></figcaption>
</figure>
<h1 id="wykład-26.03.2019">Wykład (26.03.2019)</h1>
<figure>
<img src="img/fig_21.png" alt="do poprzedniego wykładu" /><figcaption>do poprzedniego wykładu</figcaption>
</figure>
<p><span id="fig:fig_21" label="fig:fig_21">[fig:fig_21]</span></p>
<h2 id="ekstrema-związane">Ekstrema związane</h2>
<p>przykład: <span class="math display">\[f(x,y) = x+y,\quad G(x,y) = (x-1)^1 + (y-1)^2 - 1,\quad M= \left \{ (x,y)\in\mathbb{R}^2,G(x,y) = 0\right \}
    .\]</span> Szukamy minimum lub maksimum <span class="math inline">\(f\)</span> na <span class="math inline">\(M\)</span><br />
Rozważmy linię o stałej wartości <span class="math inline">\(x+y\)</span></p>
<figure>
<img src="img/fig_22.png" alt="G(x,y) i sznurek o stałej długości w polu grawitacyjnym" style="width:80.0%" /><figcaption><span class="math inline">\(G(x,y)\)</span> i sznurek o stałej długości w polu grawitacyjnym</figcaption>
</figure>
<p><span id="fig:fig_22" label="fig:fig_22">[fig:fig_22]</span></p>
<figure>
<img src="img/fig_23.png" alt="Biedronka łazi po obręczy rowerowej z włączonym wentylatorem, gdzie wyląduje???" style="width:70.0%" /><figcaption>Biedronka łazi po obręczy rowerowej z włączonym wentylatorem, gdzie wyląduje???</figcaption>
</figure>
<p><span id="fig:fig_23" label="fig:fig_23">[fig:fig_23]</span></p>
<p>Niech <span class="math inline">\(f:\mathbb{R}^{n}\to\mathbb{R}^1\)</span> i <span class="math inline">\(M\subset \mathbb{R}^n\)</span> - zbiór.<br />
Mówimy, że <span class="math inline">\(f\)</span> ma minimum/maksimum związane na zbiorze <span class="math inline">\(M\)</span>, w punkcie <span class="math inline">\(x_0\in M\)</span>, jeżeli <span class="math display">\[\underset{r}{\exists} \underset{\substack{h\\  \Vert h \Vert &lt; r \\ (x_0+h)\in M}}{\forall} f(x_0+h)\leq f(x_0)
    .\]</span></p>
<figure>
<img src="img/fig_24.png" style="width:60.0%" />
</figure>
<p><span id="fig:fig_24" label="fig:fig_24">[fig:fig_24]</span></p>
<p>Niech <span class="math inline">\(f(x,y):\mathbb{R}^2\to\mathbb{R}^1\)</span><br />
<span class="math inline">\(G(x,y): \mathbb{R}^2\to\mathbb{R}^1\)</span><br />
<span class="math inline">\(M = \left\{ (x,y), G(x,y) = 0 \right\}\)</span> Szukamy minimum/maksimum <span class="math inline">\(f\)</span>. Można wyliczyć <span class="math inline">\(y(x)\)</span> z więzów, wstawić do <span class="math inline">\(f\)</span> i zbadać ekstrema funkcji jednej zmiennej <span class="math inline">\(g(x) = f(x,y(x))\)</span>. Kiedy nie umiemy wyliczyć <span class="math inline">\(y(x)\)</span> z więzów, możemy założyć, że <span class="math inline">\(y(x)\)</span> jednak istnieje i <span class="math inline">\(G(x,y(x)) = 0\)</span>. Wtedy: <span class="math display">\[\frac{\partial G}{\partial x} + \frac{\partial G}{\partial y} \frac{\partial y}{\partial x} = 0
    .\]</span> <span class="math display">\[\frac{\partial y}{\partial x}  = - \frac{\frac{\partial G}{\partial x}}{\frac{\partial G}{\partial y} }
    .\]</span></p>
<p><span class="math display">\[\text{czyli: } g&#39;(x) = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial x} = 0
    .\]</span> <span class="math display">\[\frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \frac{-\frac{\partial G}{\partial x}}{\frac{\partial G}{\partial y} } = 0
        \implies \underline{\frac{\partial G}{\partial y} \frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial G}{\partial x}}
    .\]</span></p>
<p>A co by było, gdyby <span class="math inline">\(G(x,y) = 0\)</span> zadawał funkcję <span class="math inline">\(x(y)\)</span>?<br />
<span class="math display">\[G(x(y),y) = 0
    .\]</span> Czyli badalibyśmy wtedy funkcję <span class="math display">\[P(y) = f(x(y),y)\quad P&#39;(y) = 0 \implies \frac{\partial f}{\partial x} \frac{\partial x}{\partial y} + \frac{\partial f}{\partial y}  = 0
    .\]</span> ale <span class="math display">\[\label{eq:eqx}
        \frac{\partial x}{\partial y}  = - \frac{\frac{\partial G}{\partial y}}{\frac{\partial G}{\partial x}} \implies \frac{\partial f}{\partial x} \frac{-\frac{\partial G}{\partial y} }{\frac{\partial G}{\partial x} }+\frac{\partial f}{\partial y} = 0 \implies \frac{\partial f}{\partial x} \frac{\partial G}{\partial y} = \frac{\partial f}{\partial y} \frac{\partial G}{\partial x}\]</span></p>
<p>Co oznacza warunek <a href="#eq:eqx" data-reference-type="ref" data-reference="eq:eqx">[eq:eqx]</a>?<br />
Wiemy, że <span class="math display">\[f&#39; = \left[ \frac{\partial f}{\partial x} , \frac{\partial f}{\partial y}  \right] G&#39; = \left[ \frac{\partial G}{\partial x} , \frac{\partial G}{\partial y}  \right]\text{, czyli}
    .\]</span> <span class="math display">\[V = \left[ A,B \right] \quad W = \left[ C,D \right] \text{ i } AD = BC
    .\]</span> <span class="math display">\[\frac{A}{B} = \frac{C}{D} = \lambda\in \mathbb{R}
    .\]</span> Stąd wiadomo, że <span class="math display">\[A = B\lambda, \quad C = D\lambda
    .\]</span> <span class="math display">\[V = \left[ B\lambda, B \right] ,\quad B = \left[ \lambda, 1 \right], \quad W = \left[ D\lambda, D \right], \quad D = \left[ \lambda, 1 \right]
    .\]</span> Czyli warunek na to, aby <span class="math inline">\(G&#39;(x) = 0\)</span>, albo <span class="math inline">\(P&#39;(y) = 0\)</span> oznacza, że <span class="math display">\[\underset{\substack{\lambda\in\mathbb{R}\\ \lambda\neq 0}}{\exists} f&#39; = \lambda G&#39;,  \quad G(x,y) = C
    .\]</span> <span class="math display">\[\label{eq:eqy}
    \quad \frac{\partial f}{\partial x} - \lambda \frac{\partial G}{\partial x} = 0, \quad \frac{\partial f}{\partial y} - \lambda \frac{\partial G}{\partial y} = 0\]</span> Wielkość <span class="math inline">\(\lambda\)</span> często nazywa się <em>mnożnikiem Lagrange</em></p>
<p>Do warunku (<a href="#eq:eqy" data-reference-type="ref" data-reference="eq:eqy">[eq:eqy]</a>) można dojść na skróty przez funkcję <span class="math inline">\(H(x,y) = f(x,y) - \lambda G(x,y)\)</span> i badanie <span class="math inline">\(H(x,y)\)</span> tak, jakby była to funkcja <span class="math inline">\(\mathbb{R}^2\to \mathbb{R}^1\)</span> bez żadnych więzów.<br />
<span class="math display">\[\frac{\partial H}{\partial x} = 0, \quad \frac{\partial H}{\partial y} = 0 \left( \text{ + warunek } G(x,y) = 0 \right)
        .\]</span></p>
<p>Co ze zbadaniem <span class="math inline">\(G&#39;&#39;(x)\)</span> lub <span class="math inline">\(P&#39;&#39;(y)\)</span>?<br />
Odpowiedź: lepiej inaczej…(XD), to znaczy, potrzebujemy nowego języka.</p>
<p>Przy liczeniu ekstremów funkcji jednej zmiennej badaliśmy <span class="math display">\[f(x_0+h) - f(x_0) \tilde = f&#39;&#39;(x_0)(h,h)
    .\]</span></p>
<figure>
<img src="img/fig_25.png" style="width:80.0%" />
</figure>
<p><span id="fig:fig_25" label="fig:fig_25">[fig:fig_25]</span></p>
<h1 id="wykład-29.03.2019">Wykład (29.03.2019)</h1>
<p><strong>Problem:</strong> <span class="math display">\[f:\mathbb{R}^n\to\mathbb{R}^1,\quad G:\mathbb{R}^n\to\mathbb{R}^m,\quad M=\left\{ x: G(x) = 0 \right\}
    .\]</span> Badamy różnicę <span class="math inline">\(f(x_0+h) - f(x_0)\)</span> (jest fajna bo możemy ją rozwinąć ze wzoru Taylora)<br />
Próbujemy ożenić te języki. Zbadajmy <span class="math inline">\(G&#39;(x)\)</span>.</p>
<ul>
<li><p><span class="math inline">\(G&#39;(x)\)</span> - jest macierzą <span class="math inline">\(\left[ G&#39; \right]_{m,n}\)</span> <span class="math display">\[G(x_1,\ldots,x_n) =
                \begin{bmatrix}
                G^1(x_1,\ldots,x_n)\\
                \vdots\\
                G^m(x_1,\ldots,x_n)
                \end{bmatrix}
            .\]</span> <span class="math display">\[\left[ G&#39;(x) \right] =
                \begin{bmatrix}
                \frac{\partial G^1}{\partial x^1} &amp; \ldots &amp; \frac{\partial G^1}{\partial x^n} \\
                \vdots\\
                \frac{\partial G^m}{\partial x^1} &amp; \ldots &amp; \frac{\partial G^m}{\partial x^n}  \end{bmatrix}
            .\]</span> <span class="math display">\[\left[ G&#39;(x) \right] : \mathbb{R}^n\to\mathbb{R}^m
            .\]</span></p>
<p>Jaki jest &quot;wymiar&quot; zbioru <span class="math inline">\(M\)</span>?</p>
<p>Albo, jeżeli <span class="math inline">\(\mathbb{R}^n\to\mathbb{R}^m\)</span>, to wiąż <span class="math inline">\(G(x) = 0\)</span> zadaje funkcję <span class="math display">\[\varphi(x): \mathbb{R}^{n-m}\to\mathbb{R}^{m}
        .\]</span> Taką, że <span class="math inline">\(G(x^1,\ldots,x^{n-m}, \varphi^1(x^1,\ldots,x^{n-m}),\ldots,\varphi^m(x^1,\ldots,x^{n-m})),\)</span> (jeżeli <span class="math inline">\(\det G_y(x) \neq 0\)</span>)</p></li>
</ul>
<p>Jeżeli <span class="math inline">\(\det G&#39;_y (x) \neq 0\)</span>, to znaczy, że w macierzy <span class="math display">\[G&#39; =
    \begin{bmatrix}
        \frac{\partial G_1}{\partial x^1} &amp;\ldots &amp;\frac{\partial G_1}{\partial x^{n-m}} &amp; \frac{\partial G_1}{\partial y^1} &amp;\ldots &amp;\frac{\partial G_1}{\partial y^m}\\
    \vdots \\
\frac{\partial G_m}{\partial x^1} &amp; \ldots &amp; \frac{\partial G_m}{\partial x^{n-m}} &amp; \frac{\partial G_m}{\partial y^1} &amp;\ldots &amp;\frac{\partial G_m}{\partial y^m}
\end{bmatrix}
    .\]</span> Gdzie <span class="math inline">\(x \overset{\text{ozn}}{=} (x^1,\ldots,x^{n-m},y^1,\ldots,y^{m})\)</span>. Żeby podkreślić to, że niektóre współrzędne <span class="math inline">\((y^1,\ldots,y^m)\)</span> można uzyskać z innych <span class="math inline">\((x^1,\ldots,x^{n-m})\)</span> poprzez funkcję <span class="math inline">\(\varphi: x = \varphi(y)\)</span></p>
<p>Gdy założymy, że <span class="math inline">\(\det G&#39;_y \neq 0\)</span>, to znaczy, że</p>
<p>m-liniowo niezależnych kolumn, bo <span class="math display">\[\dim im G&#39;(x) = m = \dim \mathbb{R}^m \text{ i } G&#39;(x): \mathbb{R}^n\to\mathbb{R}^m
    .\]</span> Oznacza to, że <span class="math display">\[\dim \ker G&#39;(x) = n-m
.\]</span> (tw. o rzędzie (paweł odpalił kiedyś))</p>
<p>Oznaczmy <span class="math inline">\(X_1 = \ker G&#39;(x)\)</span> i <span class="math inline">\(X_2 = im G&#39;(x)\)</span> (<span class="math inline">\(\dim X_1 = n-m, \dim X_2 = m\)</span>) Oznacza to, że każdy wektor <span class="math inline">\(h\in \mathbb{R}^n\)</span> da się przedstawić jako <span class="math inline">\(h = h_1 + h_2, h_1\in X_1, h_2\in X_2\)</span> czyli <span class="math inline">\(\mathbb{R}^{n} = X_1 \bigoplus X_2\)</span></p>
<p>Oznacza to, że możemy tak wybrać bazę, że <span class="math display">\[X_1 = \left\{ \begin{bmatrix}
    x^1\\
    \vdots\\
    x^{n-m}\\
    0\\
    \vdots\\
    0\end{bmatrix}  \right\}
    , X_2 = \left\{ \begin{bmatrix} 0\\
    \vdots\\
    0\\
    y^1\\
    \vdots\\
    y^m
    \end{bmatrix}  \right\},\quad x^1,\ldots,x^{n-m},y_1,\ldots,y_m \in \mathbb{R}
.\]</span> Co więcej, <span class="math display">\[X_2 = \begin{bmatrix}
0\\
\vdots\\
0\\
\varphi^1(x^1,\ldots,x^{n-m})\\
\vdots\\
\varphi^m(x^1,\ldots,x^{n-m})
\end{bmatrix}, \quad x^i \in \mathcal{O} : \det (G&#39;_y) \neq 0
.\]</span> A co możemy powiedzieć o wierszach <span class="math inline">\(G&#39;(x)\)</span>? - jest ich <span class="math inline">\(m\)</span> i są liniowo niezależne</p>
<p>Jeżeli <span class="math inline">\(h = h_1 + h_2,\quad h_1\in X_1, h_2\in X_2\)</span>, to możemy powiedzieć, że <span class="math display">\[h_2 = \varphi(h_1)
.\]</span> Zatem dalej piszemy <span class="math display">\[h_2 = \varphi(0 + h_1) = \varphi(0) + \varphi&#39;(0)h_1 + r(0,h_1)
.\]</span> gdzie <span class="math inline">\((r\frac{0,h_1}{\Vert h_1 \Vert} \underset{\Vert h_1 \Vert \to 0}{\rightarrow} 0)\)</span> (bo z tw. o funkcji uwikłanej wiemy, że <span class="math inline">\(\varphi\)</span> - różniczkowalna, co więcej <span class="math inline">\(\varphi&#39; = -(G&#39;_y)^{-1} G&#39;_x\)</span> a <span class="math inline">\(\varphi&#39;(0) = -(G&#39;_y(0))^{-1} G&#39;_x(0)\)</span> czyli <span class="math inline">\(\varphi&#39;(0) h_1 = -(G&#39;_y(0))^{-1} G&#39;_x(0)h_1 = 0\)</span></p>
<p>Zatem <span class="math display">\[h_2 = \varphi(h_1) = r(0,h_1)
.\]</span> gdzie <span class="math display">\[\frac{r(0,h_1)}{\Vert h_1 \Vert} \underset{h_1 \to 0}{\to} 0
.\]</span> czyli <span class="math inline">\(h_2\)</span> maleje szybciej niż <span class="math inline">\(\Vert h_1 \Vert\)</span></p>
<p>Chcemy zbadać różnicę <span class="math display">\[f(x_0+h) - f(x_0)
.\]</span> Skoro <span class="math inline">\(h\in \mathbb{R}^n\)</span>, to możemy przedstawić <span class="math inline">\(h\)</span> jako <span class="math display">\[h = h_{\parallel} + h_{\perp},\quad h_{\parallel} \in X_1, h_{\perp} \in X_2
.\]</span> czyli <span class="math display">\[G&#39;(x_0)h_{\parallel} = 0?
.\]</span></p>
<p>niech <span class="math inline">\(G(x,y) = (x-1)^2 + (y-1)^2 - 1\)</span>, <span class="math inline">\(G&#39; = (2(x-1),2(y-1))\)</span></p>
<figure>
<img src="img/fig_26.png" alt="biedronka i szprycha" style="width:80.0%" /><figcaption>biedronka i szprycha</figcaption>
</figure>
<p><span id="fig:fig_26" label="fig:fig_26">[fig:fig_26]</span></p>
<p><span class="math display">\[f(x_0+h) - f(x_0) = f(x_0 + h_\perp + h_\parallel) - f(x_0)
.\]</span> W małym otoczeniu <span class="math inline">\(h\)</span> będzie bardziej decydował <span class="math inline">\(h_\parallel\)</span>, bo zawsze mogę zmniejszyć <span class="math inline">\(h\)</span> i w efekcie <span class="math inline">\(h_\perp\)</span> się zmniejszy</p>
<p>Wiemy, że <span class="math display">\[f(x_0+h) - f(x_0) = f&#39;(x_0) h + \frac{1}{2!} f&#39;&#39;(x_0)(h,h) + r_1(x_0,h)
.\]</span> bo <span class="math inline">\(f\)</span> - różniczkowalna <span class="math display">\[G(x_0+h) - G(x_0) = G&#39;(x_0)h + \frac{1}{2!} G&#39;&#39;(x_0)(h, h) + r_2(x_0,h)
.\]</span> bo <span class="math inline">\(G\)</span> - różniczkowalna</p>
<p>niech <span class="math display">\[\Lambda = \left[ \lambda_1,\ldots,\lambda_m \right] , \lambda_i \in \mathbb{R}
.\]</span> Wtedy <span class="math display">\[f(x_0+h) - f(x_0) = f(x_0 + h) - f(x_0) - \Lambda (G (x_0+h) - G(x_0)) = (f&#39;(x_0) - \Lambda G&#39;(x_0)) h
.\]</span> Dalej dostajemy <span class="math display">\[(f&#39;(x_0) - \Lambda G&#39;(x_0))h + \frac{1}{2!} (f&#39;&#39;(x_0) - \Lambda G&#39;&#39;(x_0))(h, h) + r_1(x_0,h) + r_2 (x_0,h)
.\]</span> Ale dla minimum lub maksimum chcemy, aby <span class="math display">\[f&#39;(x_0) = \Lambda G&#39;(x_0)
.\]</span> więc dla minimum / maksimum <span class="math inline">\(f(x_0+h) - f(x_0) = \frac{1}{2} (f&#39;&#39;(x_0) - \Lambda G&#39;&#39;(x_0))(h,h) + r_1(x_0,h)+r_2(x_0,h)\)</span></p>
<p>Zatem jako, że <span class="math inline">\(\frac{r_1(0,h)}{\Vert h \Vert ^2} \underset{\Vert h \Vert^2}{\to}  0\)</span>, <span class="math inline">\(\frac{r_2(0,h)}{\Vert h \Vert ^2} \underset{\Vert h \Vert ^2}{\to} 0\)</span>, to o znaku <span class="math inline">\(f(x_0+h) - f(x_0)\)</span> decyduje znak <span class="math display">\[(f&#39;&#39;(x_0) - \Lambda G&#39;&#39;(x_0))(h,h)
.\]</span> Wiemy, że <span class="math inline">\(h\in \mathbb{R}^{n} i \mathbb{R}^n = X_1 \bigoplus X_2\)</span>, czyli <span class="math inline">\(h = h_\perp + h_\parallel\)</span></p>
<p><span class="math display">\[f&#39;&#39;(x_0) - \Lambda G&#39;&#39;(x_0))(h,h) = \underbrace{f&#39;&#39;(x_0) \Lambda G&#39;&#39;(x_0)}_{\Box} (h_\parallel + h_\perp, h_\perp + h_\parallel)
.\]</span> <span class="math display">\[= (\Box)(h_\perp, h_\perp) + (\Box)(h_\perp, h_\parallel) + (\Box)(h_\parallel, h_\perp) + (\Box)(h_\parallel, h_\parallel)
.\]</span></p>
<p>Króre z powyższych wyrażeń jest najmniejsze (które z powyższych wyrażeń są o rząd mniejsze od pozostałych dla <span class="math inline">\(\Vert h \Vert \to 0\)</span></p>
<p>Wiemy, że <span class="math display">\[\Vert h_\perp \Vert ~ \Vert h_\parallel \Vert
.\]</span> Oznacza to, że dla małych <span class="math inline">\(\Vert h_\parallel \Vert\)</span> o znaku decyduje <span class="math display">\[(f&#39;&#39;(x_0) - \Lambda G&#39;&#39;(x_0))(h_\parallel,h_\parallel)
.\]</span></p>
<p>Niech <span class="math display">\[\begin{aligned}
        &amp;f:U\subset \mathbb{R}^n \to \mathbb{R}^1,\quad f\in \mathcal{C}^2(U), \\
        &amp;G: U_2\subset\mathbb{R}^n\to\mathbb{R}^m,\quad G\in \mathcal{C}^2(U_2),\\
        &amp;\underset{x_0}{\exists} G(x_0) = 0, \quad G&#39;(x_0) \text{ - ma rząd maksymalny }(m)
    .\end{aligned}\]</span> oraz <span class="math display">\[\underset{\Lambda}{\exists} \Lambda = \left[ \lambda_1,\ldots,\lambda_m \right], \lambda_i \in \mathbb{R}, f&#39;(x_0) - \Lambda G&#39;(x_0) = 0
    .\]</span> to jeżeli <span class="math display">\[(f&#39;&#39;(x_0)-\Lambda G&#39;&#39;(x_0))(h_\parallel,h_\parallel) &gt; 0,
        h_\parallel \overset{\text{def}}{=} \left \{G&#39;(x_0) h_\parallel = 0 \right \}
    .\]</span> to <span class="math inline">\(f\)</span> posiada w <span class="math inline">\(x_0\)</span> minimum lokalne <span class="math inline">\((&lt;0\)</span>, to maksimum lokalne<span class="math inline">\()\)</span> na zbiorze <span class="math display">\[M = \left\{ x\in \mathbb{R}^n, G(x) = 0 \right\}\]</span></p>
<h1 id="wykład-02.04.2019">Wykład (02.04.2019)</h1>
<h2 id="równania-różniczkowe">Równania różniczkowe</h2>
<p>Interesuje nas następująca sytuacja:</p>
<p><span class="math display">\[\begin{aligned}
        &amp;\frac{dx}{dt} = f(t,x) \\
        &amp;x(t_0)= x_0\\
        &amp;x(t): [a,b]\to\mathbb{R}\\
        &amp;f: [a,b]\times \mathbb{R}^n\to\mathbb{R}^n
    .\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
            &amp;\frac{dx}{dt} = -kx(t)\\
            &amp;x(t) = c e^{-kt}
        .\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
            &amp;\ddot{x} + \omega^2 x = 0\\
            &amp;\dot{x} = p\\
            &amp;\dot{p} = \ddot{x} = -\omega^2 x\\
            &amp;\frac{d}{dt} \underbrace{\begin{bmatrix} x\\p \end{bmatrix}}_{\frac{d}{dt}x} = \underbrace{\begin{bmatrix} 0&amp;1\\ -\omega^2&amp;0 \end{bmatrix} \begin{bmatrix} x\\p \end{bmatrix}}_{f(x,t)}
        .\end{aligned}\]</span></p>
<p>Niech <span class="math inline">\(I\subset \mathbb{R}, \mathcal{O}\subset \mathbb{R}^n\)</span><br />
<span class="math inline">\(f: I\times\mathbb{R}^n \to \mathcal{O}\)</span> taka, że <span class="math inline">\(t\in I, x\in \mathbb{R}^n, f(t,x) \to f(t,x&#39;)\)</span><br />
Mówimy, że <span class="math inline">\(f\)</span> spełnia warunek Lipschitza, jeżeli <span class="math display">\[\underset{L&gt;0}{\exists}. \underset{t\in I}{\forall}. \underset{x,x&#39;\in \mathcal{O}}{\forall}. \Vert f(t,x) - f(t,x&#39;)  \Vert \leq L \Vert x - x&#39; \Vert
        .\]</span></p>
<p>Zmienne <span class="math inline">\(t,x\)</span> nie występują w warunku Lipschitza na równych prawach</p>
<p>Czy jeżeli <span class="math display">\[f:\mathcal{O}\to\mathcal{O}\text{ takie, że } \underset{L&gt;0}{\exists}
            .\]</span> że <span class="math display">\[\underset{x,x&#39;}{\forall}  \Vert f(x) - f(x&#39;) \Vert \leq L \Vert x - x&#39; \Vert
            .\]</span> to czy <span class="math inline">\(f\)</span> jest ciągła?</p>
<p>(problem Cauchy)<br />
Niech <span class="math inline">\([a,b] \subset \mathbb{R}, \mathcal{O}\subset\mathbb{R}^n, \mathcal{O}\)</span> - domknięty i <span class="math inline">\(f:[a,b]\times\mathcal{O}\to\mathcal{O}\)</span> takie, że <span class="math inline">\(f\)</span> - ciągła na <span class="math inline">\([a,b]\times\mathcal{O}\)</span> oraz <span class="math inline">\(f\)</span> spełnia warunek Lipschitza na <span class="math inline">\(\mathcal{O}\)</span>, to znaczy: <span class="math display">\[\underset{L&gt;0}{\exists}\quad \underset{t\in[a,b]}{\forall}\quad \underset{x,x&#39;\in\mathcal{O}}{\forall} \Vert f(t,x) - f(t,x&#39;) \Vert \leq L \Vert x-x&#39; \Vert
            .\]</span> Wówczas <span class="math display">\[\underset{t_0\in[a,b]}{\forall}\quad \underset{x_0\in\mathcal{O}}{\forall}\quad \underset{\varepsilon&gt;0}{\exists} \text{, że dla } t\in ]t_0-\varepsilon, t_0+\varepsilon[\]</span> równanie ma jednoznaczne rozwiązania, które są ciągłe ze względu na <span class="math inline">\(x_0\)</span> <span class="math display">\[\begin{cases}\label{eq:cauchy}
                \frac{dx}{dt} = f(t,x)\\
                x(t_0) = x_0
            \end{cases}\]</span></p>
<p>Ciągłość <span class="math inline">\(f\)</span> na <span class="math inline">\([a,b]\times\mathcal{O}\)</span> jest mocniejszym warunkiem niż Lipschytzowalność na <span class="math inline">\(\mathcal{O}\)</span></p>
<figure>
<img src="img/fig_27.png" style="width:80.0%" />
</figure>
<p><span id="fig:fig_27" label="fig:fig_27">[fig:fig_27]</span></p>
<p>Skoro <span class="math inline">\(f\)</span> - ciągła na <span class="math inline">\([a,b]\times\mathcal{O}\)</span>, to znaczy, że <span class="math inline">\(f\)</span> jest ograniczona, czyli <span class="math display">\[\underset{M&gt;0}{\exists}\quad \underset{r_1&gt;0}{\exists}\quad \underset{r_2&gt;0}{\exists}\quad \Vert f(t,x) \Vert \leq M
            .\]</span> <span class="math inline">\(t\in K(t_0,r_1), x\in K(x_0,r_2)\)</span>.<br />
Zauważmy, że problem (<a href="#eq:cauchy" data-reference-type="ref" data-reference="eq:cauchy">[eq:cauchy]</a>) możemy zapisać jako <span class="math display">\[\label{eq:eq22}
                x(t) = x_0 + \int_{t_0}^t f(s,x(s)) ds\]</span> Czyli, jeżeli znajdziemy <span class="math inline">\(x(t)\)</span> takie, co spełnia (<a href="#eq:eq22" data-reference-type="ref" data-reference="eq:eq22">[eq:eq22]</a>), to rozwiążemy problem <a href="#eq:cauchy" data-reference-type="ref" data-reference="eq:cauchy">[eq:cauchy]</a>.<br />
Rozważmy odwzorowanie <span class="math display">\[\underset{g\in a}{P(g)(t)} = x_0 + \int_{t_0}^t f(s,g(s))ds
            .\]</span> <span class="math display">\[A = \left\{ C: [t-r_1, t_0+r_1]\to \mathbb{R}^n \right\}\text{ funkcja ciągła na kuli o wartościach w }\mathbb{R}^n
            .\]</span></p>
<figure>
<img src="img/fig_28.png" style="width:80.0%" />
</figure>
<p><span id="fig:fig_28" label="fig:fig_28">[fig:fig_28]</span></p>
<p>Co by było, gdyby <span class="math inline">\(P\)</span> miało punkt stały? Czyli <span class="math inline">\(\underset{x(t)\in A}{\exists}\)</span> takie, że <span class="math inline">\(P(x(t)) = x(t)\)</span><br />
Oznaczałoby to, że <span class="math display">\[x(t) = -x_0 + \int_{t_0}^t f(s,x(s))ds
            .\]</span> Co więcej, gdyby <span class="math inline">\(P\)</span> było zwężające, to z zasady Banacha wiemy, że punkt stały jest tylko jeden. Zatem, jeżeli znajdziemy podzbiór <span class="math inline">\(A\)</span> taki, że <span class="math inline">\(P\)</span> - zwężające, to udowodnimy jednoznaczność. Problem (<a href="#eq:eq22" data-reference-type="ref" data-reference="eq:eq22">[eq:eq22]</a>)<br />
Niech <span class="math inline">\(E = \left\{ g\in A, \Vert g(t) - \overset{g_0(t)}{x_0} \Vert \underset{\text{ważne!}}{\leq} r_2 \right\}\)</span>, czyli <span class="math display">\[g\in E \iff \underset{t_0-\varepsilon\leq t\leq t_0+\varepsilon}{sup} \Vert g(t) - x_0 \Vert \leq r_2
            .\]</span> i <span class="math display">\[g: [t_0-\varepsilon, t_0+\varepsilon]\to\mathbb{R}^n
            .\]</span> i <span class="math inline">\(g\)</span> - ciągła.<br />
(domkniętość ze względu na zasdę Banacha <span class="math inline">\((x_0 \overset{\text{ozn}}{=} g_0(t) )\)</span>)</p>
<p>Szukamy takiego <span class="math inline">\(\varepsilon\)</span>, żeby:</p>
<ol>
<li><p><span class="math inline">\(P(g)\in E \quad g\in E\)</span><span id="eq:eq23" label="eq:eq23">[eq:eq23]</span></p></li>
<li><p><span class="math inline">\(P\)</span> - zwężająca na <span class="math inline">\(E\)</span><span id="eq:eq24" label="eq:eq24">[eq:eq24]</span></p></li>
</ol>
<p>bo jeżeli (<a href="#eq:eq24" data-reference-type="ref" data-reference="eq:eq24">[eq:eq24]</a>) jest spełniona, to wiemy, że istnieje punkt stały.<br />
Jeżeli (<a href="#eq:eq23" data-reference-type="ref" data-reference="eq:eq23">[eq:eq23]</a>) jest spełniona, to wiemy, że punkt stały należy do <span class="math inline">\(E\)</span><br />
Warunek (<a href="#eq:eq23" data-reference-type="ref" data-reference="eq:eq23">[eq:eq23]</a>): <span class="math inline">\(P(g)\in E\)</span>, czyli <span class="math display">\[\underset{t_0-\varepsilon\leq t_0\leq t_0+\varepsilon}{sup} \Vert P(g(t)) - x_0 \Vert \leq r_2
            .\]</span> czyli <span class="math display">\[\begin{aligned}
                &amp;\underset{t_0-\varepsilon\leq t_0\leq t_0+\varepsilon}{sup} \Vert x_0 + \int_{t_0}^t f(s,g(s))ds - x_0 \Vert \leq \underset{t_0-\varepsilon\leq t_0\leq t_0+\varepsilon}{sup} \int_{t_0}^t \Vert f(s,g(s)) \Vert ds \leq\\
                &amp; \le \underset{t_0-\varepsilon\leq t_0\leq t_0+\varepsilon}{sup} |t-t_0| M = \varepsilon M
            .\end{aligned}\]</span> Jeżeli chcemy aby <span class="math inline">\(\varepsilon M \leq r_2\)</span>, to znaczy, że <span class="math inline">\(\varepsilon \leq \frac{r_2}{M}\)</span> i jednocześnie <span class="math inline">\(\varepsilon\leq r_1\)</span>.<br />
Czyli aby warunek (<a href="#eq:eq23" data-reference-type="ref" data-reference="eq:eq23">[eq:eq23]</a>) był spełniony to musi być: <span class="math display">\[\varepsilon &lt; min \left\{ \frac{r_2}{M}, r_1 \right\}
            .\]</span> Warunek (<a href="#eq:eq24" data-reference-type="ref" data-reference="eq:eq24">[eq:eq24]</a>). Chcemy aby <span class="math inline">\(P\)</span> było zwężające, czyli: <span class="math display">\[\underset{g_1,g_2\in E}{\forall}  \left\Vert P(g_1) - P(g_2) \right\Vert \le q\Vert q_1-q_2 \Vert
            .\]</span> Zatem: <span class="math display">\[\Vert P(g_1) - P(g_2) \Vert  = \underset{t_0-\varepsilon\leq t_0\leq t_0+\varepsilon}{sup} \Vert x_0 + \int_{t_0}^t f(s,g_1(s))ds - (x_0 + \int_{t_0}^t f(s,g_2(s))ds \Vert =
            .\]</span> <span class="math display">\[\underset{t_0-\varepsilon\leq t_0\leq t_0+\varepsilon}{sup} \Vert \int_{t_0}^t f(s,g_1(s)) - f(s,g_2(s)) ds \Vert \le \underset{t_0-\varepsilon\leq t_0\leq t_0+\varepsilon}{sup} \int_{t_0}^t \Vert f(s,g_1(s)) - f(s,g_2(s)) \Vert ds \le
            .\]</span> <span class="math display">\[\underset{t_0-\varepsilon\leq t_0\leq t_0+\varepsilon}{sup} \int_{t_0}^t L \Vert g_1-g_2 \Vert = \varepsilon L  \underset{\substack{\in E\\ \Vert g_1 - g_2 \Vert &lt; 2r_2}}{\Vert g_1 - g_2 \Vert}
            .\]</span> Zatem, jeżeli <span class="math inline">\(P\)</span> ma być zwężające na <span class="math inline">\(E\)</span>, to <span class="math inline">\(\varepsilon L &lt; 1\)</span>, czyli <span class="math inline">\(\varepsilon&lt; \frac{1}{L}\)</span> i <span class="math inline">\(g\in E\)</span><br />
Zatem, aby istniało rozwiązanie jednoznaczne problemu <a href="#eq:cauchy" data-reference-type="ref" data-reference="eq:cauchy">[eq:cauchy]</a> <span class="math display">\[\varepsilon &lt; min \left\{ \frac{r_2}{M}, r_1, \frac{1}{L} \right\}
            .\]</span></p>
<p>Do pełnego dowodu brakuje nam ciągłości rozwiązania ze względu na zmiany <span class="math inline">\(x_0\)</span></p>
<p>Lemat:<br />
niech <span class="math inline">\(A,X\)</span> - przestrzenie metryczne, <span class="math inline">\(P_a(x), a\in A, x\in X\)</span> - odwzorowanie zwężające i ciągłe ze względu na <span class="math inline">\(a\in A\)</span><br />
Niech <span class="math inline">\(\tilde x(a)\)</span> taki, że <span class="math inline">\(P(\tilde x(a)) = \tilde x(a)\)</span>. Zwężające, to znaczy, że <span class="math display">\[\underset{a\in A}{\forall}. \underset{x,x&#39;}{\forall}. \Vert P_a(x) - P_a(x&#39;) \Vert \le q \Vert x-x&#39; \Vert
        .\]</span> Wówczas funkcja <span class="math inline">\(\tilde x(a)\)</span> jest ciągła na <span class="math inline">\(A\)</span>.</p>
<p>Odwzorowanie <span class="math inline">\(P(g)\)</span> wygląda tak: <span class="math display">\[P(g(t)) = x_0 + \int_{t_0}^t f(s,g(s))ds
            .\]</span> Więc rolę parametru <span class="math inline">\(a\)</span> pełnią <span class="math inline">\(x_0,t_0\)</span> i <span class="math inline">\(P(g(t))\)</span> jest ciągłe ze względu na <span class="math inline">\(x_0\)</span> i <span class="math inline">\(t_0\)</span>.</p>
<h1 id="wykład-05.04.2019">Wykład (05.04.2019)</h1>
<p>Ostatnio zastanawialiśmy się nad taką sytuacją, że mieliśmy operator <span class="math inline">\(P_a(x)\)</span> i on miał być zwężający. <span class="math display">\[P_a(x): X\to X \text{ - zwężający }
.\]</span> <span class="math display">\[c\in X: \left\{ c,P_a(c),P_a(P_a(c)) \to \tilde x(a) \right\}\text{, gdzie }P(\tilde x(a)) = \tilde x(a)
.\]</span></p>
<p>Chcemy pokazać, że <span class="math display">\[\underset{\varepsilon &gt;0}{\forall} . \underset{\delta &gt; 0}{\exists}. \underset{a&#39;}{\forall}  d(a,a&#39;) &lt; \delta \implies d(\tilde x(a),\tilde x(a&#39;)) &lt; \varepsilon
    .\]</span> Wiemy, że <span class="math inline">\(P_a\)</span> - ciągła ze względu na <span class="math inline">\(a\)</span>: <span class="math display">\[\label{eq:del}
        \underset{\varepsilon_1&gt;0}{\forall}. \underset{\delta_1&gt;0}{\exists} .\underset{a&#39;}{\forall} d(a,a&#39;) &lt; \delta_1 \implies d(P_a, P_{a&#39;}) &lt; \varepsilon\]</span> Wiemy, że <span class="math inline">\(\underset{c&#39;\in X}{\forall}\)</span> ciąg <span class="math inline">\(\left\{ c&#39;,P_{a&#39;}(c&#39;), P_{a&#39;}(P_{a&#39;}(c&#39;))\ldots \right\} \to \tilde x(a&#39;)\)</span> Ale, jeżeli przyjmiemy za <span class="math inline">\(c = \tilde x(a&#39;)\)</span>, to ciąg: <span class="math display">\[\left\{ \tilde x(a&#39;),P_{a}(\tilde x(a&#39;)), P_{a}(P_{a}(\tilde x(a&#39;))) \right\} \to \tilde x(a)
    .\]</span> Ale z zasady banacha wiemy, że jeżeli <span class="math inline">\(P_{a}\)</span> - zwężający, to <span class="math display">\[d(\tilde x(a),x_0) \le \frac{1}{1-q} d(x_1,x_0)
    .\]</span> Wybierzmy <span class="math inline">\(x_0 = \tilde x(a&#39;)\)</span>. Wówczas <span class="math display">\[\begin{aligned}
        d(\tilde x(a),\tilde x(a&#39;)) &amp;\le \frac{1}{1-q}d(P_a(\tilde x(a&#39;)) ,\tilde x(a&#39;)) =\\
        &amp;= \frac{1}{1-q} d(P_a(\tilde x(a&#39;)), P_{a&#39;}(\tilde x(a&#39;)))
    .\end{aligned}\]</span></p>
<p>Jak ten obiekt ma się do <span class="math inline">\(d(P_{a},P_{a&#39;})\)</span>?<br />
<span class="math display">\[d(P_a,P_{a&#39;}) = \underset{x\in X}{\sup} \quad d(P_a(x), P_{a&#39;}(x))
        .\]</span></p>
<p>Więc, jeżeli <span class="math inline">\(d(P_{a&#39;},P_a)&lt;\varepsilon_1\)</span>, to znaczy, że <span class="math inline">\(d(P_a(\tilde x(a&#39;)),P_{a&#39;}(\tilde x(a&#39;))) &lt; \varepsilon_1\)</span><br />
<span class="math display">\[\text{Czyli } d(\tilde x(a),\tilde x(a&#39;)) \le \frac{1}{1-q} \varepsilon_1
        .\]</span> Czyli jeżeli otrzymamy <span class="math inline">\(\varepsilon_1\)</span>, to biorąc <span class="math inline">\(\varepsilon_1\)</span> taki, że <span class="math inline">\(\varepsilon_1 \frac{1}{1-q}&lt;\varepsilon\)</span> i znajdujemy <span class="math inline">\(\delta_1\)</span> z zależności <a href="#eq:del" data-reference-type="ref" data-reference="eq:del">[eq:del]</a> i wiemy, że jeżeli <span class="math display">\[d(a&#39;,a) &lt; \delta_1 \implies d(\tilde x(a&#39;),\tilde x(a))&lt;\varepsilon \quad\Box
       .\]</span></p>
<p>(odwzorowanie zwężające)<br />
<span class="math display">\[\int \frac{dx(t)}{dt} = f(t,x), x(t_0) = x_0
    .\]</span> Wiemy, że <span class="math inline">\(x(t)\)</span> jest punktem stałym odwzorowania <span class="math display">\[P(g) = x_0 + \int_{t_0}^{t} f(s,g(s)) ds \implies g_0,P(g_0),P(P(g_0))\ldots\to x(t)
    .\]</span> <span class="math display">\[\frac{dx}{dt} = t+x, x(0) = 0
    .\]</span> <span class="math display">\[f(t,x) = t+x. t_0 = 0, x_0 = 0
    .\]</span> Czy <span class="math inline">\(f\)</span> jest lipszycowalna? <span class="math display">\[\underset{t\in [a,b]}{\forall} \Vert t+x - (t+x&#39;) \Vert = \Vert x-x&#39; \Vert = 1 \Vert x-x&#39; \Vert \implies L = 1
    .\]</span> Czyli jest. Policzymy kilka wyrazów ciągu <span class="math display">\[\underset{x^0(t)}{g_0},\underset{x^1(t)}{P(g_0)}, \underset{x^2(t)}{P(P(g_0))},\ldots
    .\]</span> <span class="math display">\[\begin{aligned}
        &amp;x^0(t) = x_0(t) = 0\\
        &amp;x^1(t) = P(x^0(t)) = P(0) = 0 + \int_0^t f(s,x^0(s))ds = \int_0^t s ds = \frac{t^2}{2}\\
        &amp;x^2(t) = P(x^1(t)) = P(\frac{t^2}{2}) = 0 + \int_0^t f(s,x^1(s)) ds = \int_0^t (s+\frac{s^2}{2})ds = \frac{t^2}{2} + \frac{t^3}{2 \times 3}\\
        &amp;x^3(t) = P(x^2(t)) = 0 + \int_0^t \left(s+\frac{s^2}{2}+\frac{s^3}{2\times 3} \right)ds  = \frac{t^2}{2}+\frac{t^3}{2\times 3}+\frac{t^4}{2\times 3\times 4}\\
        &amp;\vdots\\
        &amp;\vdots \to \infty\\
        &amp;e^t - t - 1
    .\end{aligned}\]</span></p>
<p><span class="math inline">\(\frac{dx}{dt} = 2tx,\quad x(0) = 1\)</span>, czyli <span class="math inline">\(f(t,x) = 2tx,\quad t_0 = 0\)</span></p>
<p>dla <span class="math inline">\(\underset{t\in[a,b]}{\forall}\)</span> <span class="math display">\[\Vert 2tx - 2tx&#39; \Vert \le \underset{t\in[a,b]}{\sup} |t| 2 \Vert x-x&#39; \Vert
    .\]</span> Czyli <span class="math inline">\(f\)</span> - lipszycowalna z <span class="math inline">\(L = \underset{t\in[a,b]}{\sup} |t| \times 2\)</span> <span class="math display">\[\begin{aligned}
        &amp;x^0(t) = 1\\
        &amp;x^1(t) = P(x^0(t)) = 1 + \int_0^t f(s,1)ds = 1 + \int_0^t 2sds = 1+t^2\\
        &amp;x^2(t) = P(x^1(t)) = 1 + \int_0^t 2s(1+s^2)ds = 1 + t^2 + \frac{t^4}{2}\\
        &amp;x^3(t) = P(x^2(t)) = 1 + \int_0^t 2s(1+s^2 + \frac{t^4}{2}) =  1 + t^2 + \frac{t^4}{2} + \frac{t^6}{3}\\
        &amp;\vdots \to \infty\\
        &amp;e^{t^2}
    .\end{aligned}\]</span></p>
<p><span class="math display">\[\frac{d}{dt} \begin{bmatrix} x_1(t)\\x_2(t) \end{bmatrix} = \begin{bmatrix} x_2(t)\\-x_1(t) \end{bmatrix} ,
        x_1(0) = 0, x_2(0) =  1
    .\]</span> <span class="math display">\[f(t,x) = f(t,\begin{bmatrix} x_1\\x_2 \end{bmatrix} = \begin{bmatrix} 0&amp;1\\-1&amp;0 \end{bmatrix} \begin{bmatrix} x_1\\x_2 \end{bmatrix}
    .\]</span> <span class="math display">\[\begin{aligned}
        &amp;x^0(t) = \begin{bmatrix} x_1^0(t)\\x_2^0(t) \end{bmatrix} = \begin{bmatrix} 0\\1 \end{bmatrix}\\
        &amp;x^1(t) = P(x^0(t)) = \begin{bmatrix} 0\\1 \end{bmatrix} + \int_0^t \begin{bmatrix} 1\\-0 \end{bmatrix} ds = \begin{bmatrix} 0\\1 \end{bmatrix} + \begin{bmatrix} t\\0 \end{bmatrix} = \begin{bmatrix} t\\1 \end{bmatrix}\\
        &amp;x^2(t) = P\left(\begin{bmatrix} x_1^1\\x_1^2 \end{bmatrix} \right) = P\left( \begin{bmatrix} t\\1 \end{bmatrix}  \right)  = \begin{bmatrix} 0\\1 \end{bmatrix} + \int_0^1 \begin{bmatrix} 1\\-s \end{bmatrix} ds = \begin{bmatrix} 0\\1 \end{bmatrix} +\begin{bmatrix} t\\-\frac{t^2}{2} \end{bmatrix} = \begin{bmatrix} t\\1-\frac{t^2}{2} \end{bmatrix}\\
        &amp;x^3 = P\left( \begin{bmatrix} x_1^2\\x_2^2 \end{bmatrix}  \right) = P\left( \begin{bmatrix} t\\1-\frac{t^2}{2} \end{bmatrix}  \right) = \begin{bmatrix} 0\\1 \end{bmatrix} + \int_0^t \begin{bmatrix} 1-\frac{s^2}{2}\\-s \end{bmatrix} ds = \begin{bmatrix} t-\frac{t^3}{2\times 3}\\1-\frac{t^2}{2} \end{bmatrix}\\
        &amp;\vdots \to \infty\\
        &amp;\begin{bmatrix} \sin{t}\\ \cos{t} \end{bmatrix}
    .\end{aligned}\]</span></p>
<figure>
<img src="img/fig_29.png" style="width:80.0%" />
</figure>
<p><span id="fig:fig_29" label="fig:fig_29">[fig:fig_29]</span></p>
<p>Jeżeli odwzorowania <span class="math display">\[\begin{aligned}
            &amp;t\in [a,b]\to A(t)\\
            &amp;t\in [a,b]\to b(t)
        .\end{aligned}\]</span> Gdzie <span class="math inline">\(A(t)\in L(x,x), b(t) : \mathbb{R}^1\to X\)</span> są ciągłe, to równanie <span class="math display">\[\frac{d}{dt}x(t) = A(t)x(t) + b(t),\quad x(t_0) = x_0
        .\]</span> Ma dla dowolnych <span class="math inline">\(t_0\in[a,b], x_0\in X\)</span> jednoznacznie określone rozwiązanie na <span class="math inline">\(t\in]a,b[\)</span><br />
Czym to się różni od twierdzenia o jednoznaczności warunku Cauchy? Nie ma tutaj mowy o żadnej lipszycowalności. Zawężono za to klasę funkcji występującej w równaniu.<br />
Zamiast <span class="math inline">\(]t_0-\varepsilon,t_0+\varepsilon[\times \mathcal{O}\)</span>, mamy <span class="math inline">\(]a,b[ \times X\)</span></p>
<p>Chcemy sprawdzić, czy <span class="math inline">\(f(t,x) = A(t) x(t)+b(t)\)</span> spełnia warunek Lipschitza. Wiemy, że <span class="math inline">\(A(t)\)</span> i <span class="math inline">\(b(t)\)</span> są ciągłe na przedziale domkniętym <span class="math inline">\([a,b]\)</span>. Zatem, istnieje <span class="math inline">\(\underset{t\in[a,b]}{\sup} \Vert b(t) \Vert = C\)</span>, a <span class="math inline">\(A: X\to X\)</span> i <span class="math inline">\(A\)</span> jest liniowe zatem istnieje norma tego odwzorowania <span class="math display">\[\underset{t\in[a,b]}{\sup} \Vert A(t) \Vert = L
    .\]</span> Zatem <span class="math display">\[\underset{t\in[a,b]}{\forall} \Vert A(t)x + b(t) - (A(t) x&#39; + b(t) \Vert = \Vert A(t)(x-x&#39;) \Vert \le \underset{t\in[a,b]}{\sup} \Vert A(t) \Vert \Vert x-x&#39; \Vert = L \Vert x-x&#39; \Vert
    .\]</span> Z twierdzenia o jednoznaczności wiemy, że istnieją przedziały <span class="math inline">\(]t_0-\varepsilon,t_0+\varepsilon[\)</span> oraz <span class="math inline">\(\mathcal{O} = K(x_0,r_2)\)</span> takie, że dla <span class="math display">\[\label{eq:esion}
        \varepsilon = min\left\{|a-t_0|,r_1,\frac{r_2}{M},|b-t_0|,\frac{1}{L} \right\}\]</span> Gdzie <span class="math inline">\(r_1,r_2\)</span> były takie, że na zbiorze <span class="math inline">\(K(t_0,r_1)\times K(x_0,r_2)\)</span> funkcja <span class="math inline">\(f(t,x)\)</span> była ograniczona. Zależy nam na tym, aby w warunku <a href="#eq:esion" data-reference-type="ref" data-reference="eq:esion">[eq:esion]</a> wyeliminować <span class="math inline">\(r_2\)</span><br />
Ale <span class="math inline">\(\Vert A(t)x + b(t) \Vert \le \Vert A(t)x\Vert + \Vert b(t) \Vert\)</span> dla <span class="math inline">\(x\in K(x_0,r_2)\)</span> <span class="math display">\[\begin{aligned}
        &amp;= \Vert A(t)x \Vert + C \le L\Vert x \Vert + C = \\
        &amp;= L\Vert x-x_0+x_0 \Vert +C \le\\
        &amp;\le L \Vert x-x_0 \Vert + L \Vert x_0 \Vert  + C\le\\
        &amp;\le L r_2 + L \Vert x_0 \Vert +C
    .\end{aligned}\]</span></p>
<h1 id="wykład-09.04.2019">Wykład (09.04.2019)</h1>
<p><span class="math display">\[\varepsilon = min \left\{ |t_0-a|,|t_0-b|,\frac{1}{L},\frac{r_2}{M} \right\}\]</span> <span class="math display">\[]t_0-\varepsilon,t_0+\varepsilon[\]</span> Chcielibyśmy, żeby <span class="math inline">\(\varepsilon\)</span> nie zależał od punktu w którym zaczniemy.<br />
Zauważmy, że <span class="math display">\[\Vert  A(t)x(t)+b(t)  \Vert \le L(\Vert x_0\Vert +r_2 ) +c\]</span> zatem <span class="math display">\[\begin{aligned}
    &amp;\frac{r_2}{M} \ge \frac{r_2}{L(\Vert x_0 \Vert +r_2)+c}= \Bigg| \text{Połóżmy }r_2 = \Vert x_0 \Vert +c \Bigg| =\\
    &amp;= \frac{\Vert x_0 \Vert +c}{L(\Vert x_0 \Vert +\Vert x_0 \Vert +c)+c}=\\
    &amp;\frac{\Vert x_0 \Vert +c}{L(2\Vert x_0 \Vert +c)+c}\ge \frac{\Vert x_0 \Vert +c}{L(2\Vert x_0 \Vert +c+c) + c + \Vert x_0 \Vert } = \\
    &amp;\frac{1}{2L+1},\end{aligned}\]</span> zatem <span class="math display">\[\varepsilon = min \left\{ |t_0-a|,|t_0-b|,\frac{1}{L},\frac{1}{2L+1} \right\}\]</span> (<span class="math inline">\(r_1\)</span> - pomijamy, bo <span class="math inline">\(A(t)\)</span> - ciągła na <span class="math inline">\([a,b]\)</span>).<br />
Oznacza to, że wartość <span class="math inline">\(\varepsilon\)</span> nie zależy od <span class="math inline">\(x\)</span>, zatem rozwiązanie początkowo określone na <span class="math display">\[]t_0-\varepsilon,t_0+\varepsilon[ \times K(x_0,r_2)\]</span> możemy przedłużyć do określonego na całym <span class="math inline">\([a,b] \times X\)</span> !</p>
<h2 id="rezolwenta">Rezolwenta</h2>
<p>Rozwiązaniem problemu Cauchy <span class="math display">\[\begin{aligned}
        &amp;\frac{dx}{dt} = A(t)x(t)+b(t)\\
        &amp;x(t_0)=x_0
    .\end{aligned}\]</span> jest funkcja <span class="math inline">\(x(t,t_0,x_0)\)</span></p>
<figure>
<img src="img/fig_31.png" alt="Mała zmiana może dać rozwiązanie w podobnym miejscu ale nie musi" style="width:80.0%" /><figcaption>Mała zmiana może dać rozwiązanie w podobnym miejscu ale nie musi</figcaption>
</figure>
<p><span id="fig:fig_31" label="fig:fig_31">[fig:fig_31]</span></p>
<p>Czy istnieje <span class="math display">\[R(t,t_0): \mathbb{R}^n\to\mathbb{R}^n
    .\]</span> Takie, że <span class="math display">\[x(t) = R(t,t_0)x_0?
    .\]</span>(Jeżeli <span class="math inline">\(x_0,x(t)\in\mathbb{R}^n\)</span>)</p>
<figure>
<img src="img/fig_32.png" alt="Jak pośpimy minutę dłużej to nic się nie stanie (świat jest ciągły)" style="width:80.0%" /><figcaption>Jak pośpimy minutę dłużej to nic się nie stanie (świat jest ciągły)</figcaption>
</figure>
<p><span id="fig:fig_32" label="fig:fig_32">[fig:fig_32]</span></p>
<p>Jakie własności <span class="math inline">\(R(t,t_0)\)</span> powinno posiadać?</p>
<ol>
<li><p><span class="math inline">\(R(t,t_0): \mathbb{R}^n\to\mathbb{R}^n, R\)</span> - liniowy<span id="eq:131" label="eq:131">[eq:131]</span><br />
Bo jeżeli <span class="math inline">\(x_1(t), x_1(t_0)=x_0^1\)</span> i <span class="math inline">\(x_2(t),x_2(t_0) = x_0^2\)</span> są rozwiązaniem, to chcielibyśmy, by <span class="math inline">\(x_1(t) + x_2(t)\)</span> też było rozwiązaniem z wartością początkową <span class="math inline">\(x_0^1+x_0^2\)</span>. Rys <a href="#fig:fig_32" data-reference-type="ref" data-reference="fig:fig_32">[fig:fig_32]</a></p></li>
<li><p>funkcja <span class="math inline">\(R(t,t_0)\)</span></p></li>
<li><p><span class="math inline">\(R(t,t_0) = R(t,s) R(s,t_0)\)</span><br />
<span class="math inline">\(\underset{t,t_0,s\in\mathcal{O}\subset\mathbb{R}}{\forall}\)</span></p></li>
<li><p><span class="math inline">\(R(t_0,t_0) = \mathbb{I}\)</span>, bo <span class="math inline">\(x(t) = R(t,t_0)x_0\)</span> <span class="math inline">\(\underset{t_0\in\mathcal{O}}{\forall}\)</span><br />
Ad 3. Wstawiając <span class="math inline">\(t_0\)</span> do trzeciej kropki otrzymujemy <span class="math display">\[R(t_0,t_0)=R(t_0,s)R(s,t_0)\rightarrow \underset{t,s\in\mathcal{O}}{\forall} R(s,t) = R(t,s)^{-1}\]</span></p></li>
<li><p><span class="math display">\[\begin{aligned}
                &amp;\frac{dR(t,to)}{dt} = A(t)R(t,t_0),\\
                &amp;R(t_0,t_0) = \mathbb{I}
            .\end{aligned}\]</span> bo wtedy <span class="math inline">\(x(t) = R(t,t_0)x_0\)</span> jest rozwiązaniem problemu <span class="math display">\[\begin{aligned}
                &amp;\frac{dx}{dt}=A(t)x(t)\\
                &amp;x(t_0)=x_0
            .\end{aligned}\]</span> bo <span class="math inline">\(\frac{dx}{dt} = \frac{d}{dt}(R(t,t_0)x_0) = A(t)R(t,t_0)x_0=A(t)x(t)\)</span> i <span class="math inline">\(x(t_0)=R(t_0,t_0)x_0 = \mathbb{I}x_0 = x_0\)</span></p></li>
</ol>
<p>Zatem na mocy twierdzenia o jednoznaczności rozwiązań wiemy, że założenie <span class="math inline">\(x(t) = R(t,t_0)x_0\)</span> da nam jednoznaczne rozwiązanie.</p>
<p>A co z <span class="math inline">\(b(t)\)</span>? (ten wektorek co by to był, ale go nie ma)</p>
<p>Chcemy rozwiązać problem <span class="math display">\[\begin{aligned}
        &amp;\frac{dx}{dt}=A(t)x(t)+b(t)\\
        &amp;x(t_0)=x_0
    .\end{aligned}\]</span> Załóżmy, że rozwiązanie tego problemu możemy przedstawić jako <span class="math display">\[x(t) = R(t,t_0)C(t), C(t):\mathbb{R}\to\mathbb{R}^n
    .\]</span> Ale <span class="math display">\[\frac{d}{dt}x(t) = \frac{d}{dt} \left( R(t,t_0)c(t) \right) = \frac{dR(t,t_0)}{dt}c(t)+R(t,t_0)\frac{dc}{dt} = A(t)R(t,t_0)c(t_)+R(t,t_0)\frac{dc}{dt}
    .\]</span> Zatem mogę napisać, że <span class="math display">\[A(t)R(t,to)c(t)+R(t,t_0)\frac{dc}{dt}=A(t)R(t,t_0)c(t)+b(t)
    .\]</span>(cudowne skrócenie) <span class="math display">\[R(t,t_0)\frac{dc}{dt}=b(t) \quad\quad/R(t,t_0)^{-1}
    .\]</span> <span class="math display">\[\frac{dc}{dt}=R(t,t_0)^{-1}b(t)
    .\]</span> <span class="math display">\[\frac{dc}{dt} = R(t,t_0)b(t)
    .\]</span> <span class="math display">\[c(t) -\alpha = \int_{t_0}^t R(t_0,s)b(s) ds, \alpha\in\mathbb{R}
    .\]</span> Ale <span class="math inline">\(c(t_0)=x_0\)</span>, więc <span class="math inline">\(\alpha = x_0\)</span>. <span class="math display">\[c(t) = x_0+\int_{t_0}^t R(t_0,s)b(s)ds
    .\]</span> Zatem <span class="math display">\[x(t) = R(t,t_0)c(t) = R(t,t_0)\left( x_0+\int_{t_0}^t R(t_0,s)b(s)ds \right)  =
    .\]</span> <span class="math display">\[R(t,t_0)x_0+R(t,t_0)\int_{t_0}^t R(t_0,s)b(s)ds =
    .\]</span> <span class="math display">\[R(t,t_0)x_0+\int_{t_0}^t \underset{R(t,s)}{R(t,t_0)R(t_0,s)}b(s)ds
    .\]</span> Zatem rozwiązanie problemu wygląda tak: <span class="math display">\[x(t) = R(t,t_0)x_0+\int_{t_0}^t R(t,s)b(s)ds
    .\]</span> dygresja:<br />
dają nam rozkład gęstości masy <span class="math inline">\(\rho(x&#39;)\)</span>. Jak wygląda potencjał? <span class="math display">\[\varphi(x) = \int \frac{\rho(x&#39;) dv&#39;}{\Vert x-x&#39; \Vert }
    .\]</span> W tym przypadku rezolwenta to <span class="math inline">\(\frac{1}{\Vert x-x&#39; \Vert }\)</span><br />
</p>
<p>Czy rezolwenta istnieje?</p>
<p>Funkcja <span class="math inline">\(R(t,t_0) = e^{\int_{t_0}^t A(s) ds}\)</span> spełnia warunki <span class="math inline">\(1-5\)</span> dla rezolwenty</p>
<ul>
<li><p><span class="math inline">\(R(t,t_0): \mathbb{R}^n\to\mathbb{R}^n\)</span></p></li>
<li><p><span class="math inline">\(R(t,t_0)\)</span> - jest ciągła względem <span class="math inline">\(t\)</span> i <span class="math inline">\(t_0\)</span></p></li>
<li><p><span class="math inline">\(R(t,\alpha)R(\alpha,t_0) = R(t,t_0)\)</span>, bo <span class="math inline">\(e^{\int_{t_0}^t A(s)d s} = e^{\int_{t_0}^\alpha A(s)d s+\int_{\alpha}^t A(s)d s}\)</span><br />
<span class="math inline">\(R(t,t_0) = R(t,\alpha)R(\alpha,t_0)\)</span></p></li>
<li><p><span class="math inline">\(R(t_0,t_0) = e^{\int_{t_0}^{t_0}A(s)d s} = \mathbb{I}\)</span></p></li>
<li><p><span class="math inline">\(\frac{dR}{dt}=A(t)R(t,t_0)\)</span><br />
Dowód: <span class="math display">\[\frac{R(t+h,t_0) -R(t,t_0)}{h} = \frac{1}{h}\left(e^{\int_{t_0}^{t+h}A(s)d s} - e^{\int_{t_0}^{t}A(s)d s}\right) =
            .\]</span> <span class="math display">\[=\frac{1}{h}\left[ e^{\int_{t_0}^{t+h}A(s)d s}e^{\int_{t_0}^{t}A(s)d s}-e^{\int_{t_0}^{t}A(s)d s} \right] =
            .\]</span> <span class="math display">\[\frac{1}{h}\left[ e^{\int_{t}^{t+h}A(s)d s}-\mathbb{I} \right] e^{\int_{t_0}^t A(s)d s} - \frac{1}{h}\left[ e^{hA(\beta) - \mathbb{I}} \right] R(t,t_0) =
            .\]</span> <span class="math display">\[\frac{1}{h}\left[ \mathbb{I}+\frac{hA(\beta)}{1} + \frac{(hA(\beta))^2}{2!}+\ldots = \mathbb{I} \right] R(t,t_0) =
            .\]</span> <span class="math display">\[\underset{t&lt;\beta&lt;t+h}{A(\beta)R(t,t_0)} + h[...]\to A(t)R(t,t_0)
            .\]</span> ((((<span class="math inline">\(\int_{t}^{t+h}A(s)d s = (t+h-t)A(\beta)\)</span>))))</p></li>
</ul>
<p><span class="math display">\[\frac{d}{dt}\begin{bmatrix} x(t)\\p(t) \end{bmatrix}  = \begin{bmatrix} 1&amp;0\\0&amp;-1 \end{bmatrix} \begin{bmatrix} x(t)\\p(t) \end{bmatrix} , \begin{bmatrix} x(0)\\p(0) \end{bmatrix} = \begin{bmatrix} x_0\\p_0 \end{bmatrix}
        .\]</span> <span class="math display">\[\begin{bmatrix} x(t)\\p(t) \end{bmatrix} e^{\int_{0}^t \begin{bmatrix} 1&amp;0\\0&amp;-1 \end{bmatrix} ds}\begin{bmatrix} x_0\\p_0 \end{bmatrix} = e^{t\begin{bmatrix} 1&amp;0\\0&amp;-1 \end{bmatrix} }\begin{bmatrix} x_0\\p_0 \end{bmatrix}
        .\]</span></p>
<h1 id="wykład-12.04.2019">Wykład (12.04.2019)</h1>
<p><span class="math display">\[\frac{d}{dt}\begin{bmatrix} x(t)\\p(t) \end{bmatrix} = \begin{bmatrix} 1&amp;0\\0&amp;-1 \end{bmatrix} \begin{bmatrix} x(t)\\p(t) \end{bmatrix},\quad \begin{bmatrix} x(t=0)\\p(t=0) \end{bmatrix} = \begin{bmatrix} x_0\\p_0 \end{bmatrix}
    .\]</span> <span class="math display">\[\begin{bmatrix} x(t)\\p(t) \end{bmatrix} = e^{(t-0)\begin{bmatrix} 1&amp;0\\0&amp;-1 \end{bmatrix} }\begin{bmatrix} x_0\\p_0 \end{bmatrix}
    .\]</span> <span class="math display">\[A = \begin{bmatrix} 1&amp;0\\0&amp;-1 \end{bmatrix}
    .\]</span> <span class="math display">\[w(\lambda) = \det \begin{bmatrix} 1-\lambda&amp;0\\0&amp;-1-\lambda \end{bmatrix} = -(1-\lambda)(1+\lambda) = -(1-\lambda^2) = 0 \iff \lambda_1 = 1, \lambda_2 = 1
    .\]</span> <span class="math display">\[f(\lambda) = e^{\lambda t}, f(\lambda) = q(\lambda)w(\lambda) + a\lambda+b
    .\]</span> <span class="math display">\[f(-1) = -a+b, f(1) = a+b
    .\]</span> <span class="math display">\[b = \frac{f(-1)+f(1)}{2}= \frac{e^{-t}+e^{t}}{2}, a = \frac{f(1)-f(-1)}{2} = \frac{e^t - e^{-t}}{2}
    .\]</span> <span class="math display">\[\begin{bmatrix} x(t)\\p(t) \end{bmatrix} = \underbrace{ \left( \frac{e^t-e^{-t}}{2}\begin{bmatrix} 1&amp;0\\0&amp;-1 \end{bmatrix} + \frac{e^t+e^{-t}}{2}\begin{bmatrix} 1&amp;0\\0&amp;1  \end{bmatrix}\right) }_{R(t,t_0)} \begin{bmatrix} x_0\\p_0 \end{bmatrix}
    .\]</span></p>
<p>Czy można znaleźć rozwiązanie bez liczenia <span class="math inline">\(R(t,t_0)?\)</span></p>
<p>Załóżmy, że macierz <span class="math inline">\(A: \mathbb{R}^n\to\mathbb{R}^n\)</span> ma <span class="math inline">\(n\)</span> różnych wartości własnych. <span class="math display">\[\begin{aligned}
        &amp;\lambda_1,&amp;\lambda_2, &amp;\lambda_3,\ldots\\
        &amp;v_1,&amp;v_2, &amp;v_3,\ldots
    .\end{aligned}\]</span></p>
<p>Jeśli <span class="math inline">\(v\in ker(A - \lambda \mathbb{I})\)</span>, to znaczy, że <span class="math display">\[\begin{aligned}
        &amp;A v = \lambda v\\
        &amp;A^2 v = \lambda^2 v\\
        &amp;A^nv = \lambda^n v\\
        &amp;e^{A}v = e^{\lambda t}v
    .\end{aligned}\]</span></p>
<p>Jeżeli zatem przdstawimy warunek początkowy jako sumę: <span class="math display">\[\begin{aligned}
    &amp;\overline{x_0} = x_0&#39; + x_0^2 + \ldots + x_0^n\\
    &amp;e^{A(t-t_0)} \overline{x_0} = sum_{i=1}^{n}e^{A(t-t_0)}x_0^i = sum_{i=1}^n e^{\lambda_i(t-t_0)}x_0^i\\
.\end{aligned}\]</span></p>
<p>najogólniesza postać <span class="math inline">\(\lambda_j\)</span>(pierwiastki równania <span class="math inline">\(w(\lambda) = 0\)</span>) to <span class="math display">\[\lambda_j = a_j + ib_j
    .\]</span></p>
<p>Zatem dowolne rozwiązanie problemu jednorodnego przy <span class="math inline">\(n\)</span> różnych wartościach własnych może być jedynie kombinacją funkcji typu <span class="math display">\[\cos(bt),\quad \sin(bt),\quad e^{at},\quad ch(at),\quad sh(at),\quad e^{at}\sin(bt),\quad e^{at}\cos(bt)
.\]</span> I niewiele więcej.</p>
<p><span class="math display">\[\begin{aligned}
    \ddot{x} + a\dot{x} + \omega^2 x = 0\\
    \dot{x} = p\\
    \dot{p} = \ddot{x} = -a \dot{x} - \omega^2 x\\
    \frac{d}{dt}\begin{bmatrix} x\\p \end{bmatrix} = \begin{bmatrix} 0&amp;1\\-\omega^2&amp;-a \end{bmatrix} \begin{bmatrix} x\\p \end{bmatrix}
.\end{aligned}\]</span></p>
<p>Załóżmy, że macierz <span class="math inline">\(A\in M^n_n\)</span> ma <span class="math inline">\(k\)</span> różnych wartości własnych i <span class="math inline">\(A\)</span> nie zależy od czasu <span class="math display">\[\begin{aligned}
    \lambda_1\rightarrow n_1\\
    \lambda_2\rightarrow n_2\\
    \vdots\\
    \lambda_k\rightarrow n_k - V_k = ker(A-\lambda_k \mathbb{I})^{n_k}
.\end{aligned}\]</span> (gdzie <span class="math inline">\(n_1+n_2+\ldots+n_k = n\)</span>) <span class="math display">\[\mathbb{R}^n = V_{\lambda_1} \bigoplus V_{\lambda_2} \bigoplus ..\bigoplus V_{\lambda_k}
.\]</span> i teraz rozkładamy warunek początkowy: <span class="math display">\[x_0 = \underset{V_{\lambda_1}}{x_0^1}  + \underset{V_{\lambda_2}}{x_0^{2}} +\ldots+x_0^{k}
.\]</span> Wówczas <span class="math display">\[\begin{aligned}
    x(t) &amp;= e^{A(t-t_0)}x_0 = \sum_{i=1}^{k} e^{A(t-t_0)}x_0^i = \sum_{i=1}^k e^{\lambda_i(t-t_0)\mathbb{I}+A(t-t_0) - \lambda\mathbb{I}(t-t_0)}x_0^i=\\
         &amp;=\sum_{i=1}^k e^{\lambda_i(t-t_0)\mathbb{I}}e^{(A-\lambda\mathbb{I})(t-t_0)}x_0^i=\\
         &amp;=\sum_{i=1}^k e^{\lambda_i(t-t_0)\mathbb{I}}\left( \sum_{j=0}^{\infty} \frac{(t-t_0)^j (A-\lambda_j \mathbb{I})^j}{j!} x_0^i \right)\\
         &amp;\text{ale $x_0^i\in ker(A-\lambda_i\mathbb{I}^{n_i}) = \lambda_\lambda$}=\\
         &amp;=\sum_{i=1}^k e^{\lambda_i(t-t_0)\mathbb{I}}\left( \sum_{j=0}^{n-1}\frac{(t-t_0)^j}{j!}(A-\lambda_i \mathbb{I})^j \right)x_0^i
.\end{aligned}\]</span></p>
<p>Rozwiązać równanie: <span class="math display">\[\begin{aligned}
        &amp;\frac{dx_1}{dt} = x_1+x_2+2x_3\\
        &amp;\frac{dx_2}{dt} = x_2+x_3\\
        &amp;\frac{dx_3}{dt} = 2x_3\\
        &amp;\begin{bmatrix} x_1\\x_2\\x_3 \end{bmatrix} = \begin{bmatrix} 1\\2\\1 \end{bmatrix}
    .\end{aligned}\]</span> <span class="math display">\[A = \begin{bmatrix} 1&amp;1&amp;2\\0&amp;1&amp;1\\0&amp;0&amp;2 \end{bmatrix}, w(\lambda) = det\left( \begin{bmatrix} 1-\lambda&amp;1&amp;2\\0&amp;1-\lambda&amp;1\\0&amp;0&amp;2-\lambda \end{bmatrix}  \right)
    .\]</span> <span class="math display">\[w(\lambda) = (2-\lambda)(1-\lambda)^2
    .\]</span> <span class="math display">\[\begin{aligned}
        \lambda_1 = 1, n_1 = 2\\
        \lambda_2 = 2, n_2 = 1
    .\end{aligned}\]</span> <span class="math display">\[ker(A-\lambda_2\mathbb{I})
    .\]</span> <span class="math display">\[\begin{aligned}
        &amp;\begin{bmatrix} 1-2&amp;1&amp;2\\0&amp;1-2&amp;1\\0&amp;0&amp;2-2 \end{bmatrix} \begin{bmatrix} a\\b\\c \end{bmatrix} = \begin{bmatrix} 0\\0\\0 \end{bmatrix}\\
        &amp;\begin{bmatrix} -1&amp;1&amp;2\\0&amp;-1&amp;1\\0&amp;0&amp;0 \end{bmatrix} \begin{bmatrix} a\\b\\c\end{bmatrix} = 0\\
        &amp;-a+b+2c = 0\\
        &amp;-b+c = 0\\
        &amp;c=b\\
        &amp;-a-b+2b=0\\
        &amp;a=3b\\
    &amp;v\in V_{\lambda_2}\iff v = \begin{bmatrix} 3b\\b\\b \end{bmatrix} = b\begin{bmatrix} 3\\1\\1 \end{bmatrix} , V_{\lambda_2} = \left&lt; \begin{bmatrix} 3\\1\\1 \end{bmatrix}  \right&gt;\\
        &amp;V_{\lambda_1} = ker(A-\lambda_1\mathbb{I})^2\\
        &amp;\begin{bmatrix} 1-1&amp;1&amp;2\\0&amp;1-1&amp;1\\0&amp;0&amp;2-1 \end{bmatrix} ^2 = \begin{bmatrix} 0&amp;1&amp;2\\0&amp;0&amp;1\\0&amp;0&amp;1 \end{bmatrix} \begin{bmatrix} 0&amp;1&amp;2\\0&amp;0&amp;1\\0&amp;0&amp;1 \end{bmatrix} = \\
        &amp;=\begin{bmatrix} 0&amp;0&amp;3\\0&amp;0&amp;1\\0&amp;0&amp;1 \end{bmatrix}\\
        &amp;\begin{bmatrix} 0&amp;0&amp;3\\0&amp;0&amp;1\\0&amp;0&amp;1 \end{bmatrix} \begin{bmatrix} a\\b\\c \end{bmatrix} = \begin{bmatrix} 0\\0\\0 \end{bmatrix}\\
    &amp;c = 0, v\in V_{\lambda_1}\iff v = \begin{bmatrix} a\\b\\c \end{bmatrix}  = a \begin{bmatrix} 1\\0\\0 \end{bmatrix} +b \begin{bmatrix} 0\\1\\0 \end{bmatrix}\\
    &amp;V_{\lambda_1} = \left&lt;\begin{bmatrix} 1\\0\\0 \end{bmatrix}, \begin{bmatrix} 0\\1\\0 \end{bmatrix}  \right&gt;\\
    &amp;\begin{bmatrix} 1\\2\\1 \end{bmatrix} = x_0^1\in V_{\lambda_1} + x_0^2\in V_{\lambda_2}
    &amp;\begin{bmatrix} 1\\2\\1 \end{bmatrix} = (-2)\begin{bmatrix} 1\\0\\0 \end{bmatrix} + 1\begin{bmatrix} 0\\1\\0 \end{bmatrix} + 1\begin{bmatrix} 3\\1\\1 \end{bmatrix} \\
    &amp;\begin{bmatrix} 1\\2\\1 \end{bmatrix} = \begin{bmatrix} -2\\0\\0 \end{bmatrix} + \begin{bmatrix} 0\\1\\0 \end{bmatrix} + \begin{bmatrix} 3\\1\\1 \end{bmatrix}\\
        &amp;\sum_{i=1}^k e^{\lambda_i (t-to)\mathbb{I}}\left( \sum_{j=0}^{n-1} \frac{(t-t_0)^j}{j!} (A-\lambda_i\mathbb{I})^j \right) x_0^i=  \\\\
        &amp;= e^{\lambda_1(t)\mathbb{I}}\left( \sum_{j=0}^{2-1}\frac{t^j}{j!}(A-\lambda_1)^j \right) x_0^1 + e^{\lambda_2 t \mathbb{I}}\left( \mathbb{I} \right)x_0^2=\\
        &amp;= \begin{bmatrix} x_1(t)\\x_2(t)\\x_3(t) \end{bmatrix} = e^{t}\begin{bmatrix} 1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1 \end{bmatrix} \left( \begin{bmatrix} 1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1 \end{bmatrix} + \frac{t^1}{1!} \begin{bmatrix} 1-1&amp;1&amp;2\\0&amp;1-1&amp;1\\0&amp;0&amp;2-1 \end{bmatrix} \right) \begin{bmatrix} -2\\1\\0 \end{bmatrix} + e^{2t}\mathbb{I}\begin{bmatrix} 3\\1\\1 \end{bmatrix}=  \\
        &amp;= e^{t}(a+bt) + e^{2t}+C
    .\end{aligned}\]</span></p>
<h2 id="baza-rozwiązań">Baza rozwiązań</h2>
<p>Jeżeli <span class="math inline">\(x(t) = R(t,t_0)x_0\)</span> i <span class="math inline">\(R(t,t_0)\in M_n^n\)</span>, to znaczy, że <span class="math display">\[x(t) = \begin{bmatrix} \Vert  \Vert \Vert  \Vert  \end{bmatrix} \begin{bmatrix} x_0^1\\ \vdots\\ x_0^n \end{bmatrix} = x_0^1 \begin{bmatrix} | \end{bmatrix} + x_0^2 \begin{bmatrix} | \end{bmatrix} + \ldots + x_0^n \begin{bmatrix} | \end{bmatrix}
    .\]</span></p>
<p>Czy <span class="math inline">\(\det (R(t,t_0)) \neq 0\)</span>?</p>
<p>Jeżeli tak, to kolumny <span class="math inline">\(R(t,t_0)\)</span> możemy potraktować jako wektory rozpinające przestrzeń rozwiązań i <span class="math inline">\(\det R(t,t_0)\neq 0 \underset{t\in[a,b]}{\forall}\)</span>.<br />
W bazie wektorów własnych macierz <span class="math inline">\(e^{At}\)</span> wygląda tak (zakładamy <span class="math inline">\(n\)</span> wartości własnych): <span class="math display">\[\det \begin{bmatrix} e^{\lambda_1 t}&amp;\ldots&amp;0\\ \vdots&amp;\ddots&amp;\vdots \\ 0 &amp;\ldots&amp;e^{\lambda_n t} \end{bmatrix} = e^{t(\lambda_1+\ldots+\lambda_n)} = e^{t* Tr A} \neq 0
.\]</span></p>
<h1 id="wykład-30.04.2019">Wykład (30.04.2019)</h1>
<p>Czy kolumny <span class="math inline">\(R(t,t_0)\)</span> są liniowo niezależne <span class="math inline">\(\underset{t,t_0\in [a,b]}{\forall}\)</span>?</p>
<p>Wiemy, że <span class="math inline">\(R(t,t_0) = \mathbb{I} = \begin{bmatrix} 1&amp;0&amp;\ldots&amp;0\\ \vdots&amp;1&amp;\ddots&amp;\vdots \\ \vdots&amp;\dots&amp;\ddots&amp;\vdots \\ 0&amp;\dots&amp;\dots&amp;1 \end{bmatrix}\)</span>.<br />
Chcielibyśmy, żeby <span class="math inline">\(\underset{t,t_0\in[a,b] }{\forall} \det R(t,t_0) \neq 0\)</span>.</p>
<p>Przypomnienie z algebry:<br />
Z macierzą <span class="math inline">\(\begin{bmatrix} a_{11}&amp;\ldots&amp;a_{1n}\\ \vdots &amp; &amp; \vdots \\a_{n1} &amp;\dots&amp; a_{n} \end{bmatrix}\)</span> możemy związać macierz <span class="math inline">\(D = \begin{bmatrix} D_{11}&amp;\ldots&amp;D_{1n}\\ \vdots &amp;&amp;\vdots \\ D_{n1} &amp; \ldots &amp; D_{nn} \end{bmatrix}\)</span>.<br />
Zatem <span class="math inline">\(\det A\)</span> uzyskamy mnożąc np. pierwszy wiersz <span class="math inline">\(A\)</span> z pierwszą kolumną <span class="math inline">\(D^T\)</span>.<br />
Pytanie: Co się stanie, jeśli przemnożymy pierwszy wiersz <span class="math inline">\(A\)</span> przez drugą kolumnę <span class="math inline">\(D^T\)</span>?</p>
<p><span class="math inline">\(A = \begin{bmatrix} 1&amp;2\\3&amp;4 \end{bmatrix},\quad D = \begin{bmatrix} 4&amp;-3\\ -2&amp;1 \end{bmatrix},\quad D^T = \begin{bmatrix} 4&amp;-2\\-3&amp;1 \end{bmatrix}\)</span> i wtedy <span class="math inline">\(\begin{bmatrix} 1&amp;2\\3&amp;4 \end{bmatrix} \begin{bmatrix} 4&amp;-2\\-3&amp;1 \end{bmatrix} = \begin{bmatrix} -2&amp;0\\0&amp;-2 \end{bmatrix}\)</span>, zatem <span class="math inline">\(AD^T = \sum_{i=1}^n D_{ik}a_{si} = \delta_{ks}\det A\)</span></p>
<p>(Liouville)<br />
Jeżeli <span class="math inline">\(R(t,t_0)\)</span> - rezolwenta dla problemu <span class="math display">\[\begin{aligned}
        &amp;\frac{dx}{dt} = A(x)x(t)\\
        &amp;x(t_0) = x_0
    .\end{aligned}\]</span> i <span class="math inline">\(x\in \mathbb{R}^n\)</span>, to <span class="math inline">\(w(t) = w(t_0)e^{\int_{t_0}^t tr(A(s))ds}\)</span>, gdzie <span class="math inline">\(w(t) = \det R(t,t_0)\)</span> i <span class="math inline">\(w(t)\)</span> nazywamy wrońskianem.<br />
</p>
<p>Uwaga:<br />
Zauważmy, że <span class="math inline">\(w(t)\)</span> nigdy nie będzie równa zero, bo <span class="math inline">\(w(t_0) = \det(R(t_0,t_0)) = \det \begin{bmatrix} 1&amp;\ldots&amp;0\\0&amp;\ldots&amp;1 \end{bmatrix} = 1\)</span> a <span class="math inline">\(\left| \int_{t_0}^t tr A(s)ds \right| &lt; +\infty\)</span> (bo <span class="math inline">\(A(t)\to\)</span> lipszycowalna).<br />
Oznacza to, że kolumny <span class="math inline">\(R(t,t_0)\)</span> są <span class="math inline">\(\underset{t,t_0\in [a,b]}{\forall}\)</span> liniowo niezależne, więc możemy badać bazę rozwiązań złożoną z kolumn <span class="math inline">\(R(t,t_0)\)</span></p>
<p>Rezolwenta jest postaci: <span class="math display">\[R(t,t_0) = \begin{bmatrix} u_{11}(t) &amp; u_{12}(t) &amp; \ldots &amp; u_{1n}(t)\\ \vdots \\ u_{n1}(t) &amp; \ldots &amp; \ldots &amp; u_{nn}(t)\end{bmatrix}
            ,\]</span> gdzie <span class="math inline">\(u_{ij}(t_0) = \delta_{ij}\)</span>.<br />
Wiemy, że <span class="math inline">\(\frac{d R(t,t_0)}{dt} = A(t) R(t,t_0)\)</span>.<br />
Obserwacja: policzmy <span class="math inline">\(\det R(t,t_0)\)</span> względem pierwszego wiersza: <span class="math display">\[w(t) = (-1)^{1+1}u_{11}(t) \begin{bmatrix} u_{22} &amp; \ldots &amp; u_{2n} \\ \vdots \\ u_{n2}(t) &amp; \ldots &amp; u_{nn}(t) \end{bmatrix} + (\text{ brak  }u_{11})
            .\]</span> Zatem <span class="math inline">\(\frac{\partial w(t)}{\partial u_{11}} = D_{11}\)</span> i ogólnie <span class="math inline">\(\frac{\partial w(t)}{\partial u_{ij}} = D_{ij}\)</span>.<br />
Zatem <span class="math inline">\(w(t)\)</span> możemy potraktować jako funkcję od <span class="math inline">\(n\times n\)</span> zmiennych. <span class="math inline">\(w(t) = w(u_{11}(t), u_{12}(t), \ldots, u_{nn}(t))\)</span>, zatem <span class="math display">\[\frac{\partial w(t)}{\partial t} = \frac{\partial w}{\partial u_{11}} \frac{\partial u_{11}}{\partial t} + \frac{\partial w}{\partial u_{12}} \frac{\partial u_{12}}{\partial t} +\ldots+ \frac{\partial w}{\partial u_{nn}} \frac{\partial u_{nn}}{\partial t}
            .\]</span> Skoro <span class="math inline">\(\frac{d R(t,t_0)}{dt} = A(t) R(t,t_0)\)</span> to znaczy, że <span class="math display">\[\frac{\partial u_{ki}}{\partial t} = \sum_{s=1}^n a_{ks}u_{si}
            .\]</span> Czyli <span class="math display">\[\begin{aligned}
                &amp;\frac{d w}{dt} = \sum_{k,i}D_{ki} \sum_s a_{ks}u_{si} = \sum_{k=1}^n \sum_{i=1}^n \sum_{s=1}^n a_{ks} D_{ki}u_{si} =\\
                &amp;= \sum_{k=1}^n\sum_{s=1}^n a_{ks}\delta_{ks}w(t) = \sum_{k=1}^n a_{kk}w(t)\\
            .\end{aligned}\]</span> Zatem <span class="math inline">\(\frac{\partial w}{\partial t} = tr(A(t))\cdot  w(t)\)</span>. Jak przyłożymy obustronnie całkę to otrzymamy: <span class="math display">\[\int_{t_0}^t \frac{dw}{w} = \int_{t_0}^t tr(A(s))ds \implies -\ln t_0 + \ln w = \int_{t_0}^t tr(A(s))ds \to w(t) = e^{\int_{t_0}^t tr(A(s))ds}e^{\ln \ln t_0}
            .\]</span> Czyli <span class="math display">\[w(t) = w(t_0) e^{\int_{t_0}^t tr(A(s))ds}\quad\Box\]</span></p>
<h2 id="równania-liniowe-wyższych-rzędów-na-skróty">Równania liniowe wyższych rzędów (na skróty)</h2>
<p>Rozważmy równanie: <span class="math display">\[\label{eq:linwrz}
            \frac{d^n x}{dt^n} = a_0x(t) + a_1x&#39;(t) + \ldots + a_{n-1}x^{n-1}(t)\]</span> (gdzie <span class="math inline">\(a_0,\ldots,a_{n-1}\in \mathbb{R}\)</span> ).<br />
Chcemy znaleźć bazę rozwiązań.<br />
Możemy zapisać (<a href="#eq:linwrz" data-reference-type="ref" data-reference="eq:linwrz">[eq:linwrz]</a>) jako <span class="math display">\[\frac{d}{dt} \begin{bmatrix} x(t)\\ x&#39;(t) \\ \vdots \\ x^{n-1}(t) \end{bmatrix} = \begin{bmatrix} 0&amp;1&amp;0&amp;\ldots&amp;0
         \\ 0&amp;0&amp;1&amp;\ldots&amp;0
         \\ \vdots &amp;&amp;&amp;&amp;1\\
     a_0 &amp; a_1 &amp; \ldots &amp;&amp; a_{n-1}\end{bmatrix}
     \begin{bmatrix} x(t) \\ x&#39;(t) \\ \vdots \\ x^{n-1}(t) \end{bmatrix}
        .\]</span> Zatem <span class="math display">\[\begin{bmatrix}  x(t) \\ x&#39;(t) \\ \vdots \\ x^n(t) \end{bmatrix} = \sum_i e^{\lambda_i(t-t_0)} \sum_j \frac{t-t_0}{j}(a-\lambda_i \mathbb{I})^{\ln_i - 1}\underbrace{x_0^i}_{(*)}
        .\]</span> Chcemy znaleźć pierwiastki <span class="math inline">\(w(\lambda) = \det(A - \lambda \mathbb{I})\)</span></p>
<p><span class="math display">\[\begin{bmatrix} 0&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ a_0&amp;a_1&amp;a_2&amp;a_3 \end{bmatrix}
            .\]</span> <span class="math display">\[\begin{aligned}
                &amp;\det(A - \lambda \mathbb{I}) = \det \begin{bmatrix} -\lambda &amp; 1 &amp; 0 &amp; 0\\ 0 &amp;-\lambda&amp;1&amp;0\\ 0&amp;0&amp;-\lambda&amp;1\\ a_0&amp;a_1&amp;a_2&amp;a_3-\lambda \end{bmatrix} = a_0(-1)^{1+4} \begin{vmatrix} 1&amp;0&amp;0\\-\lambda&amp;1&amp;0\\0&amp;-\lambda&amp;1 \end{vmatrix} + (-1)^{2+4} a_1 \begin{vmatrix} -\lambda&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;-\lambda&amp;1 \end{vmatrix} +\\
                &amp;(-1)^{3+4}a_2\begin{vmatrix} -\lambda&amp;1&amp;0\\0&amp;-\lambda&amp;0\\0&amp;0&amp;1 \end{vmatrix} + (-1)^{4+4}(a_3-\lambda)\begin{vmatrix} -\lambda&amp;1&amp;0\\0&amp;-\lambda&amp;1\\0&amp;0&amp;-\lambda \end{vmatrix} = -a_0 \cdot 1 - a_1\lambda - a_2\lambda^2 - a_3\lambda^3 + \lambda^4\\
                &amp;\frac{d^4x}{dt^4} = a_0x + a_1x&#39; + a_2x&#39;&#39; + a_3x&#39;&#39;&#39;,\quad \lambda^4 = a_0+a_1\lambda+a_2\lambda^2+a_3\lambda^3\\
                \\
                &amp;\ddot{x} + \omega^2 x = te^{t}\\
                &amp;\frac{d}{dt} \begin{bmatrix} x\\x&#39;\\ \vdots \\ x^{n+1} \end{bmatrix} = \begin{bmatrix} . \\ . \\ . \\ . \end{bmatrix}\begin{bmatrix} x\\x^1\\ \vdots \\ x^{n+1} \end{bmatrix} + \begin{bmatrix} 0\\ \vdots \\ b(t) \end{bmatrix}\\
                \\
                &amp;\frac{d^nx}{dt^n} = a_0x + a_1x&#39; + \ldots + a_{n-1}x^{n-1}\\
                &amp;\lambda^n = a_0+a_1\lambda+\ldots+a_{n-1}\lambda^{n-1}
            .\end{aligned}\]</span> Połóżmy <span class="math inline">\(x = e^{\lambda t}\to\)</span> skrót mnemotechniczny<br />
<span class="math display">\[e^{\lambda t}\lambda^n = e^{\lambda t}a_0 + a_1 \lambda e^{\lambda t} + \ldots + a_{n-1}\lambda^{n-1}e^{\lambda t}
            .\]</span></p>
<h2 id="warunek-początkowy">Warunek początkowy</h2>
<p>czy można znaleźć współczynniki <span class="math inline">\(x_0^i\)</span> we wzorze <span class="math inline">\((*)\)</span> bez konieczności rozkładu warunku brzegowego w bazie wektorów własnych macierzy <span class="math inline">\(A\)</span>?</p>
<p>Niech <span class="math inline">\(\ddot{x} + \omega^2 x = 0\)</span> i <span class="math inline">\(x(0) = 0, x&#39;(0) = 1\)</span> i wiemy, że <span class="math inline">\(\lambda^2 + \omega^2 = 0\)</span>.<br />
Oznacza to, że <span class="math inline">\(x(t) = A e^{i\omega t} + B e^{-i\omega t}(*)\)</span>, <span class="math display">\[\begin{aligned}
                &amp;\lambda_1 = i\omega, &amp;&amp;n_1 = 1\\
                &amp;\lambda_2 = -i\omega, &amp;&amp;n_2 = 1
            .\end{aligned}\]</span> gdzie <span class="math inline">\(A\)</span> i <span class="math inline">\(B\)</span> nieznane, ale wiemy, że <span class="math inline">\(x(0) = 0\)</span> i <span class="math inline">\(x&#39;(0) = 1\)</span> i <span class="math inline">\(x&#39;(t) = Ai\omega e^{i\omega t} - Bi\omega e^{-i\omega t}.\)</span><br />
Czyli <span class="math display">\[\begin{aligned}
            &amp;Ae^{0}+Be^{-0}= 0 \implies -A = +B\\
            &amp;Ai\omega e^{0}- Bi\omega e^{-0} = 1\\
            &amp;2Ai\omega = 1\\
            &amp;A = \frac{1}{2i\omega}\\
            &amp;B = -\frac{1}{2i\omega}
            .\end{aligned}\]</span> Czyli <span class="math display">\[x(t) = \frac{1}{2i\omega}\left( e^{i\omega t}- e^{-i\omega t} \right) = \frac{1}{\omega}\sin(\omega t)
            .\]</span></p>
<p>Czy możemy zmienić bazę w równaniu (*)?</p>
<p>Odp: Możemy. Na przykład przyjmując <span class="math inline">\(x(t) = A \cos(\omega t) + B \sin(\omega t)\)</span>. Wówczas <span class="math display">\[\begin{aligned}
            &amp;x&#39;(t) = -A\omega \sin(\omega t) + B \omega \cos(\omega t)\\
            &amp;x(0) = A = 0\\
            &amp;x&#39;(0) = B\omega = 1 \to B = \frac{1}{\omega} \implies x(t) = \frac{1}{\omega}\sin(\omega t)
        .\end{aligned}\]</span></p>
<p>Co robić z niejednorością? (Dla równań wyższych rzędów)</p>
<p><span class="math display">\[\frac{d}{dt}\vec{x} = A \vec{x} + b, \frac{d}{dt} \vec{x} = A \vec{x}, \vec{x} = R(t,t_0)x_0
        .\]</span></p>
<p><span class="math display">\[\ddot{x} + \omega^2 x = e^t (\Delta)
            .\]</span> Wiemy, że rozwiązaniem problemu <span class="math inline">\(\ddot{x} + \omega^2 x = 0\)</span> jest <span class="math inline">\(x(t) = A \cos(\omega t) + B \sin(\omega t)\)</span>.<br />
Może uzmiennimy stałe: <span class="math display">\[\begin{aligned}
                &amp;x(t) = A(t) \cos(\omega t) + B(t) \sin(\omega t)\\
                &amp;\dot{x}(t) = \dot{A}(t) \cos(\omega t) - A t \sin (\omega t) + \dot{B} \sin(\omega t) + B(t) t \cos(\omega t)
            .\end{aligned}\]</span> W efekcie dostaniemy równanie drugiego rzędu na <span class="math inline">\(A(t)\)</span> i <span class="math inline">\(B(t)\)</span> :(<br />
Zapiszmy więc równanie <span class="math inline">\((\Delta)\)</span> w postaci macierzowej. <span class="math display">\[\frac{d}{dt}\begin{bmatrix} x\\x&#39; \end{bmatrix} = \begin{bmatrix} 0&amp;1\\-\omega^2&amp;0 \end{bmatrix} \begin{bmatrix} x\\x&#39; \end{bmatrix} + \begin{bmatrix} 0\\e^t \end{bmatrix} (\Delta\nabla)
            .\]</span> Jak wygląda rezolwenta? <span class="math display">\[R(t,t_0) = \begin{bmatrix} u_{11}(t)&amp;u_{12}(t)\\ u_{21}(t) &amp; u_{22}(t) \end{bmatrix} \text{ i } \frac{d}{dt}R(t,t_0) = A R(t,t_0), R(t_0,t_0) = 1
            .\]</span> <span class="math inline">\(\left(\begin{bmatrix} x\\x&#39; \end{bmatrix} = R(t,t_0)x_0 \right)\)</span>.<br />
Zauważmy, że skoro <span class="math display">\[\begin{aligned}
                &amp;x(t) = A\cos(\omega t) + B \sin(\omega t)\\
                &amp;x&#39;(t) = A\left( \cos(\omega t) \right)&#39; + B\left( \sin(\omega t) \right)&#39;\\
                &amp;\text{to wtedy } \begin{bmatrix} x(t)\\x&#39;(t) \end{bmatrix} = \begin{bmatrix} \cos(\omega t) &amp; \sin(\omega t)\\ -\omega \sin(\omega t) &amp; \omega \cos(\omega t) \end{bmatrix} \begin{bmatrix} A\\B \end{bmatrix}
            .\end{aligned}\]</span> I możemy zbudować macierz <span class="math display">\[\begin{bmatrix} u_{11}&amp;u_{12}\\u&#39;_{11}&amp;u&#39;_{12} \end{bmatrix}
            ,\]</span> która od rezolwenty różni się tym, że w <span class="math inline">\(t = t_0\)</span> nie zmienia się w macierz jednostkową.<br />
Uzmienniamy stałe: <span class="math display">\[\begin{aligned}
                (_{\tilde \tilde} ) \begin{bmatrix} x(t)\\x&#39;(t) \end{bmatrix} = \begin{bmatrix} \cos\omega t &amp; \sin\omega t\\ -\omega \sin\omega t &amp; \omega \cos \omega t \end{bmatrix} \begin{bmatrix} A(t) \\B(t)\end{bmatrix}
            \end{aligned}\]</span> i wstawiamy do <span class="math inline">\((\Delta\nabla)\)</span> <span class="math display">\[\frac{d}{dt}\begin{bmatrix} \cos\omega t&amp;\sin\omega t\\ -\omega \sin\omega t &amp; \omega \cos\omega t \end{bmatrix} \begin{bmatrix} A(t)\\B(t) \end{bmatrix} = \begin{bmatrix} \ldots \end{bmatrix} + \begin{bmatrix} 0\\e^t \end{bmatrix}
            .\]</span> <span class="math display">\[\begin{aligned}
                &amp;\begin{bmatrix} \cos\omega t&amp;\sin\omega t\\ -\omega \sin\omega t&amp; \omega \cos\omega t \end{bmatrix} \begin{bmatrix} \dot{A}(t)\\ \dot{B}(t) \end{bmatrix} + \left( \begin{bmatrix} \cos\omega t &amp; \sin\omega t\\ -\omega \sin\omega t &amp; \omega \cos \omega t \end{bmatrix}  \right)&#39;\begin{bmatrix} A(t)\\B(t) \end{bmatrix} = \\
                &amp;\begin{bmatrix} 0&amp;1\\-\omega^2&amp;0 \end{bmatrix} \cdot (_{\tilde \tilde }) + \begin{bmatrix} 0\\e^t \end{bmatrix} \text{, ale }\\
                &amp;\left( \begin{bmatrix} \cos\omega t&amp;\sin\omega t\\ -\omega \sin\omega t &amp; \omega \cos \omega t \end{bmatrix}  \right)&#39; \begin{bmatrix} A(t)\\B(t) \end{bmatrix} = \begin{bmatrix} -\omega \sin\omega t &amp; \omega \cos\omega t\\ -\omega^2\cos\omega t&amp;-\omega^2\sin\omega t \end{bmatrix} \begin{bmatrix} A(t)\\B(t) \end{bmatrix}\\
                &amp;\begin{bmatrix} 0&amp;1\\-\omega^2&amp;0 \end{bmatrix} \begin{bmatrix} \cos\omega t&amp;\sin\omega t\\ -\omega \sin\omega t &amp; \omega \cos\omega t \end{bmatrix} \begin{bmatrix} A(t)\\B(t) \end{bmatrix} = \begin{bmatrix} -\omega \sin\omega t &amp; \omega \cos\omega t \\ -\omega^2\cos\omega t &amp; -\omega^2 \sin\omega t \end{bmatrix}\begin{bmatrix} A(t)\\B(t) \end{bmatrix}\\
                &amp;\begin{bmatrix} \cos\omega t&amp; \sin\omega t\\ \left( \cos\omega t \right) &#39; &amp; \left( \sin\omega t \right) &#39;  \end{bmatrix} \begin{bmatrix} A&#39;(t)\\B&#39;(t) \end{bmatrix} = \begin{bmatrix} 0\\e^t \end{bmatrix}
            .\end{aligned}\]</span> Czyli mamy: <span class="math display">\[\begin{aligned}
                &amp;A&#39;(t) \cos\omega t + B&#39;(t) \sin\omega t = 0\\
                &amp;A&#39;(t) \left( \cos\omega t \right) &#39; + B&#39;(t) \left( \sin\omega t \right) &#39; = e^t\\
                &amp;x(t) = A \cos\omega t+ B\sin\omega t\\
                &amp;x(t) = A(t) \cos\omega t + B(t) \sin\omega t\\
                &amp;x&#39;(t) = A&#39;(t) \cos\omega t+ B&#39;(t) \sin\omega t + A(t) \left( \cos\omega t \right) &#39; + B(t)\left( \sin\omega t \right) &#39;
            .\end{aligned}\]</span></p>
<h1 id="wykład-07.05.2019">Wykład (07.05.2019)</h1>
<p>Chcemy dojść do tw Lebesque.</p>
<p>(Lebesque) Niech <span class="math inline">\(P\)</span> - zbiór nieciągłości funkcji <span class="math inline">\(f: D\to\mathbb{R}\)</span>, <span class="math inline">\(f\)</span> - ograniczona na <span class="math inline">\(D\)</span>, <span class="math inline">\(D\)</span> - …jest zbiorem miary Lebesque’a zera <span class="math inline">\(\iff\)</span> <span class="math inline">\(f\)</span> - całkowalna na <span class="math inline">\(D\)</span>.</p>
<p>Wiemy, że <span class="math inline">\(f\)</span> - całkowalna <span class="math inline">\(\iff\)</span> <span class="math display">\[\underset{\varepsilon&gt;0}{\forall} .\underset{\Pi}{\exists}. |\overline{S}(f,\Pi) - \underline{S}(f,\Pi) | &lt; \varepsilon
    .\]</span></p>
<p><img src="img/fig_33.png" alt="image" style="width:50.0%" /></p>
<p>Ostatnio pokazaliśmy, że <span class="math display">\[A_{\varepsilon} = \left\{ x\in A, O(f,x) \ge \varepsilon \right\} \text{, to $A_\varepsilon$ jest zbiorem domkniętym}
    .\]</span> (PS funkcja <span class="math inline">\(f\)</span> na zbiorze <span class="math inline">\(A\)</span> powinna być ograniczona!!!)</p>
<p><img src="img/fig_34.png" alt="image" style="width:50.0%" /></p>
<p>Jeżeli weźmiemy stól o jakiejś długości to mogę wziąć ileś kartek (albo naleśników. Nie wiadomo czy działa dla czego innego) i go nimi przykryć. Co więcej, jeżeli będzie promocja, to mogę nawet rzucić ich przeliczalnie dużo. Pytanie: czy dla każdego zbioru mogę (niezależnie od kształtu kartek) przykryć go skończoną liczbą kartek?</p>
<p>Weźmy długi stół: <span class="math display">\[\begin{aligned}
        &amp;R = \bigcup_{n=0}^\infty ] n-2, n+2 [ \cup ]-n-2, -n+2[\\
        &amp;]0,1[ \subset [-2,2]\\
        &amp;]0,1[ \subset [-2019,2018]\cup[-2,2]\\
        &amp;]0,1[ = \bigcup_{n=2}^\infty ]\frac{1}{n},1-\frac{1}{n}[
    .\end{aligned}\]</span> Ostatnie jest słabe, bo nie mogę wybrać pokrycia ze skończonej ilości elementów.</p>
<p>Niech <span class="math inline">\(X\)</span> - zbiór a <span class="math inline">\(F = \left\{ A_\alpha, \alpha\in\mathbb{R}, A_i, i\in\mathbb{N} \right\}\)</span> - rodzina zbiorów. Mówimy, że <span class="math inline">\(F\)</span> jest pokryciem zbioru <span class="math inline">\(X\)</span>, jeżeli <span class="math inline">\(X\subset \bigcup_{i,\alpha}A_\alpha\)</span>. Jeżeli zbiory <span class="math inline">\(A_\alpha\)</span> są otwarte, to mówimy, że <span class="math inline">\(F\)</span> jest pokryciem otwartym, jeżeli ilość zbiorów <span class="math inline">\(A_\alpha\)</span> jest skończona, to mówimy, że pokrycie jest skończone. Dowolny podzbiór <span class="math inline">\(F\)</span> taki, że jest też pokryciem zbioru <span class="math inline">\(X\)</span> nazywamy podpokryciem.</p>
<p>Zbiór <span class="math inline">\(X\)</span> nazywamy zwartym, jeżeli z <strong>każdego</strong> pokrycia otwartego możemy wybrać skończone podpokrycie.</p>
<p>Jak sprawdzamy, czy zbiór jest zwarty, to nie szukamy skończonych pokryć, tylko takie które nie są skończone.</p>
<p>(<span class="math inline">\(X\)</span> - domknięty,ograniczony) <span class="math inline">\(\iff\)</span> (<span class="math inline">\(X\)</span>-zbiór zwarty)</p>
<p>niech <span class="math inline">\(X\in\mathbb{X}\)</span>, <span class="math inline">\(\mathbb{X}\)</span> - przestrzeń metryczna<br />
<span class="math inline">\(\impliedby_1\)</span> Pokażemy, że jeżeli <span class="math inline">\(X\)</span> - zwarty, to <span class="math inline">\(X\)</span> - ograniczony. (przypomnienie: zbiór <span class="math inline">\(A\subset\mathbb{X}\)</span> jest ograniczony jeżeli <span class="math inline">\(\underset{r}{\exists} . \underset{x_0\in A}{\exists}\)</span>, że <span class="math inline">\(A\subset K(x_0,r)\)</span></p>
<figure>
<img src="img/fig_35.png" alt="Nieważne, co A myśli o sobie, jeżeli otoczymy je kulą, to jest ograniczone i koniec" style="width:50.0%" /><figcaption>Nieważne, co <span class="math inline">\(A\)</span> myśli o sobie, jeżeli otoczymy je kulą, to jest ograniczone i koniec</figcaption>
</figure>
<p>Skoro <span class="math inline">\(X\)</span> - zwarty, to niech <span class="math inline">\(F\)</span> będzie pokryciem złożonym z <span class="math inline">\(K(x,1), x\i X\)</span>. <span class="math inline">\(F = \left\{ K(x,1), \underset{x\in X}{\forall}  \right\}\)</span>. <span class="math inline">\(F\)</span> jest pokryciem zbioru <span class="math inline">\(X\)</span>, ale ponieważ <span class="math inline">\(X\)</span> - zwarty, to znaczy, że z pokrycia <span class="math inline">\(F\)</span> możemy wybrać <strong>skończone</strong> podpokrycie, co oznacza, że zbiór <span class="math inline">\(X\)</span> możemy ułożyć w kulę o skończonym promieniu. Zatem <span class="math inline">\(X\)</span> - ograniczony.</p>
<figure>
<img src="img/fig_36.png" alt="Przykrywanie zbioru kulami" style="width:80.0%" /><figcaption>Przykrywanie zbioru kulami</figcaption>
</figure>
<p><span class="math inline">\(\impliedby_2\)</span> Pokażemy, że <span class="math inline">\(X\)</span> - zwarty, to <span class="math inline">\(X\)</span> - domknięty. Pokażemy, że <span class="math inline">\(X&#39;\)</span> - zbiór otwarty. Czyli, że dla dowolnego <span class="math inline">\(p\in X&#39; \underset{K(p,\tilde r)}{\exists}\)</span>, że <span class="math inline">\(K(p,\tilde r)\cap X = \phi\)</span> co będzie oznaczało, że <span class="math inline">\(X&#39;\)</span> składa się wyłącznie z punktów wewnętrznych.</p>
<p><img src="img/fig_37.png" alt="image" style="width:80.0%" /></p>
<p>Weźmy <span class="math inline">\(q\in X\)</span>, utwórzmy dwa otoczenia: <span class="math display">\[K(q,r), K(p,r); r=\frac{1}{2}d(p,q)
    .\]</span> Widać, że <span class="math inline">\(K(q,r) \cap K(p,r) = \phi\)</span>. Powtarzamy taką procedurę dla każdego <span class="math inline">\(q\subset X\)</span>, oznacza to, że dostaniemy pokrycie zbioru <span class="math inline">\(X\)</span> kulami <span class="math inline">\(K(q,r_q), q\in X\)</span>, ale <span class="math inline">\(X\)</span> jest zbiorem zwartym więc mogę wybrać <strong>skończoną</strong> ilość kul<br />
<span class="math inline">\(K(q_1,r_1), K(q_2,r_2),\ldots,K(q_k,r_k)\)</span> będącą pokryciem zbioru <span class="math inline">\(X\)</span>.</p>
<p><img src="img/fig_38.png" alt="image" style="width:80.0%" /></p>
<p>A to znaczy, że <span class="math display">\[\underbrace{\left( K(p,r_1)\cap K(p,r_2) \cap \ldots \cap K(p,r_k) \right)}_{\text{jest do zbiór niepusty i \textbf{otwarty}}} \cap \underbrace{\left( K(q_1,r_1) \cup K(q_2,r_2) \cup \ldots \cup K(q_k,r_k) \right)}_{\text{Pokrywa cały $X$}} = \phi
    .\]</span> czyli np. <span class="math display">\[\bigcap_{n=1}^\infty ]-\frac{1}{n}, \frac{1}{n}[ = [0]
    .\]</span> Znaleźliśmy otoczenie otwarte punktu <span class="math inline">\(P\)</span> : <span class="math inline">\(K(p,r_k) \cap \ldots K(p,r_k)\)</span>, takie, że nie ma punktów wspólnych z <span class="math inline">\(X\)</span>, więc <span class="math inline">\(p\)</span> jest punktem wewnętrznym, czyli <span class="math inline">\(X&#39;\)</span> - otwarty, czyli <span class="math inline">\(X\)</span> - domknięty.</p>
<p><span class="math inline">\(X\)</span> - domknięty i ograniczony <span class="math inline">\(\implies\)</span> <span class="math inline">\(X\)</span> - zwarty. Niech <span class="math inline">\(P\)</span> - kostka z <span class="math inline">\(\mathbb{R}^n\)</span>, metryka <span class="math inline">\(d_2\)</span>. Pokażemy, że <span class="math inline">\(P\)</span> jest zwarta. <span class="math display">\[P = [a_1,b_1]\times\ldots\times[a_n,b_n]
    .\]</span> <span class="math display">\[\lnot(p\implies q)\iff p\land \lnot q
    .\]</span> Dowód przez sprzeczność:<br />
Załóżmy, że <span class="math inline">\(P\)</span> - domknięty i ograniczony i <span class="math inline">\(P\)</span> nie jest zwarty. Co to znaczy, że <span class="math inline">\(P\)</span> nie jest zwarte? Oznacza to, że istnieje pokrycie zbioru <span class="math inline">\(P\)</span> takie, że nie da się wyciągnąć z niego skończonego podpokrycia.</p>
<p>Jeżeli <span class="math inline">\(P\)</span> nie da się pokryć skończoną ilością zbiorów, to znaczy, że jeżeli weźmiemy kostkę <span class="math inline">\([a_1,c_1]\times[a_2,c_2]\times\ldots\times[a_n,c_n]\)</span> gdzie <span class="math inline">\(c_1 = \frac{a_1+b_1}{2}, c_2 = \frac{a_2+b_2}{2}, \ldots, c_n = \frac{a_n+b_n}{2}\)</span>,<br />
to jej też nie możemy podzielić na skończoną ilość elementów. Czyli <span class="math inline">\(P_1\subset P\)</span>, kulę <span class="math inline">\(P_1\)</span> też możemy podzielić na cztery części itd…W efekcie dostaniemy ciąg kostek <span class="math inline">\(P P_1 P_2 P_3 \ldots P_n \ldots\)</span>. Weźmy ciąg elementów <span class="math display">\[\begin{aligned}
        &amp;x_0\in P\\
        &amp;x_1\in P_1\\
        &amp;\vdots\\
        &amp;x_n\in P_n\\
        &amp;\vdots
    .\end{aligned}\]</span> Znaczy, że ciąg <span class="math inline">\(\left\{ x_n \right\}\)</span> jest ciągiem Cauchy (bo każdy element ciągu asdasd). Ciąg <span class="math inline">\(\left\{ x_n \right\} \in\mathbb{R}^n\)</span> czyli <span class="math inline">\(X_n\)</span> jest zbieżny. (bo <span class="math inline">\(\mathbb{R}^n\)</span> - zupełna). Niech <span class="math inline">\(\tilde x\)</span> będzie granicą <span class="math inline">\(\left\{ x_n \right\}\)</span> a zbiór <span class="math inline">\(\left\{ P,P_1,P_2,\ldots,P_n,\ldots \right\}\)</span> jest pokryciem <span class="math inline">\(P\)</span> takim, z którego nie możemy wyciągnąć skończonego podpokrycia. Ale skoro <span class="math inline">\(\lim_{n\to\infty}x_n = \tilde x\)</span>, to znaczy, że <span class="math display">\[\underset{\varepsilon&gt;0}{\forall} . \underset{n}{\exists} . \underset{n&gt;N}{\forall} . x_n\in K(\tilde x,\varepsilon)
    .\]</span> Oznacza to, że mogę tak dobrać <span class="math inline">\(\varepsilon\)</span>, że w <span class="math inline">\(K(\tilde x,\varepsilon)\)</span> będą się zawierać wszystkie <span class="math inline">\(P_{i}, i&gt;n\)</span>. Mogę wtedy wybrać <strong>skończone</strong> podpokrycia kostki <span class="math inline">\(P\)</span>.<br />
<span class="math display">\[\left\{ P_1,P_2,P_3,\ldots,P_{n_i}, K(\tilde x,\varepsilon) \right\}
    .\]</span> i sprzeczność</p>
<figure>
<img src="img/fig_39.png" alt="mogę wybrać sobie takie kółko, że wszytkie następne kwadraty będą już leżały w tym kółku!" style="width:50.0%" /><figcaption>mogę wybrać sobie takie kółko, że wszytkie następne kwadraty będą już leżały w tym kółku!</figcaption>
</figure>
<p>Wracamy do tw. Lebesque’a. Obserwacja: Niech <span class="math inline">\(D\)</span> - zwarty, <span class="math inline">\(D\subset \mathbb{R}^n\)</span>, <span class="math inline">\(f: D\to \mathbb{R}\)</span> - ograniczona i niech <span class="math inline">\(A = \left\{ x\in D, o(f,x) &lt; \varepsilon \right\}\)</span>. Wówczas: <span class="math display">\[\underset{\Pi}{\exists} . |\overline{S}(f,\Pi) - \underline{S}(f,\Pi)|&lt;\varepsilon |D|
    .\]</span></p>
<p><img src="img/fig_40.png" alt="image" style="width:80.0%" /></p>
<p>Skoro <span class="math inline">\(\underset{x\in A}{\forall} \lim_{r\to 0} | \underset{K(x&#39;,r)}{sup} f(x&#39;) - \underset{x&#39;\in K(x&#39;,r)}{inf} f(x&#39;) | &lt; \varepsilon\)</span> To znaczy, że <span class="math inline">\(\underset{r_\varepsilon}{\exists}\)</span> takie, że <span class="math inline">\(|sup f(x&#39;) - inf f(x&#39;) | &lt; \varepsilon\)</span>. Jeżeli zbadamy wszystkie kule <span class="math inline">\(K(x,r_\varepsilon) \underset{x\in D}{\forall}\)</span> to otrzymamy pokrycie <span class="math inline">\(A\)</span>. Ale <span class="math inline">\(A\)</span> jest zbiorem zwartym, więc możemy wybrać skończone podpokrycie, czyli skończoną ilość kul takich, że <span class="math display">\[(*)
            A \subset K(x_1,r_\varepsilon^1)\cup K(x_2,r_\varepsilon^2)\cup\ldots\cup K(x_n,r_\varepsilon^n)
        .\]</span> Możemy zatem wybrać podział <span class="math inline">\(\Pi\)</span> zbioru <span class="math inline">\(D\)</span> zgodny z podziałem <span class="math inline">\((*)\)</span>, w wyniku czego, <span class="math display">\[|\overline{S}(f,\Pi) - \underline{S}(f,\Pi)|&lt;\varepsilon |D|
        .\]</span></p>
<h1 id="wykład-10.05.2019">Wykład (10.05.2019)</h1>
<p>Ostatnio było: <span class="math display">\[\begin{aligned}
        &amp;A\subset D : \underset{x\in A}{\forall} \mathcal{O}(f,x)&lt;\varepsilon; A - \text{kostka, to}\\
        &amp;\underset{\Pi}{\exists} | \overline{S}(f,\Pi) - \underline{S}(f,\Pi)| &lt; \varepsilon |A|
    .\end{aligned}\]</span></p>
<p>(Lebesgue’a) niech <span class="math inline">\(D\)</span> - kostka, <span class="math inline">\(D\subset \mathbb{R}^n\)</span>, <span class="math inline">\(f: D\to \mathbb{R}\)</span>, <span class="math inline">\(f\)</span> - ograniczona.<br />
Wówczas <span class="math inline">\(f\)</span> - (całkowalna na <span class="math inline">\(D\)</span> ) <span class="math inline">\(\iff\)</span> (zbiór nieciągłości funkcji <span class="math inline">\(f\)</span> jest miary Lebesgue’a zero)</p>
<p><span class="math inline">\(\impliedby\)</span><br />
Chcemy pokazać, że <span class="math display">\[\underset{\Pi}{\exists} |\overline{S}(f,\Pi) - \underline{S}(f,\Pi) | &lt; \varepsilon
        ,\]</span></p>
<p><img src="img/fig_41.png" alt="image" style="width:80.0%" /></p>
<p>przy założeniu, że zbiór nieciągłośći jest miary L. zero.<br />
Wprowadźmy zbiór <span class="math inline">\(A_n = \left\{ x\in D, \mathcal{O}(f,x) \ge \frac{1}{n} \right\}\)</span> <span class="math display">\[\text{np. } A_2 = \left\{ x\in D, \mathcal{O}(f,x) \ge \frac{1}{2} \right\}
        .\]</span></p>
<p><span class="math inline">\(A_1\subset A_2\subset A_3\subset \ldots\)</span><br />
a zbiór <span class="math inline">\(A = \bigcup_{n=1}^{\infty}A_n\)</span> będzie zbiorem wszystkich punktów nieciągłości funkcji <span class="math inline">\(f\)</span> na <span class="math inline">\(D\)</span>.<br />
</p>
<p>Tw. Lebesgue’a udowodnimy dla wybranego <span class="math inline">\(A_n\)</span>, bo przeliczalna suma zbiórów miary L. zero też jest zbiorem miary L. zero.</p>
<p>Zbiór <span class="math inline">\(A_n\)</span> jest zbiorem domkniętym (bo lemat).<br />
Wiemy, że <span class="math inline">\(A_n\)</span> jest zbiorem miary L. zero gdy itnieje <span class="math inline">\(P_i \subset D\)</span>, (<span class="math inline">\(P_i\)</span> - kostki), że <span class="math inline">\(A_n \subset \bigcup P_i\)</span>, <span class="math inline">\(\sum |P_i |\)</span> - dowolnie mała (skończona lub nieskończona suma).<br />
</p>
<p>Niech <span class="math inline">\(\varepsilon &gt; 0\)</span>. Wiemy, że <span class="math display">\[\underset{\varepsilon&gt;0}{\forall} . \underset{N}{\exists} . \underset{n &gt; N}{\forall} \frac{1}{n} &lt; \varepsilon
        .\]</span> Wybierzmy zatem taki indeks <span class="math inline">\(n\)</span> dla zbioru <span class="math inline">\(A_n\)</span>, że <span class="math inline">\(\frac{1}{n} &lt; \varepsilon\)</span>. Wiemy, że <span class="math inline">\(A_n\)</span> - domknięty i ograniczony (bo <span class="math inline">\(A_n \subset D\)</span>, a <span class="math inline">\(D\)</span> - kostka w <span class="math inline">\(\mathbb{R}^n\)</span>), to znaczy, że <span class="math inline">\(A_n\)</span> jest zbiorem zwartym, a <span class="math inline">\(\left\{ P_i \right\}\)</span> jest jego pokryciem. Możemy więc wybrać z niej skończone podpokrycie <span class="math inline">\(\left\{ P_1, P_2, \ldots, P_k \right\}\)</span> takie, że <span class="math display">\[\begin{aligned}
        &amp;A_n \subset \bigcup_{j=1}^k P_j\\
        &amp;\sum_{j=1}^{k} |P_j| &lt; \frac{1}{n}
        .\end{aligned}\]</span> (możemy tak zrobić, bo zawsze możemy wybrać taką rodzinę <span class="math inline">\(\left\{ P_i \right\}\)</span>, że <span class="math inline">\(\sum |P_i|\)</span> - dowolnie mała.</p>
<p><img src="img/fig_42.png" alt="image" style="width:80.0%" /></p>
<p>Wybierzmy podział <span class="math inline">\(\Pi\)</span> zbioru <span class="math inline">\(D\)</span> taki, że <span class="math inline">\(\Pi\)</span> jest na tyle drobny, że odtwarza pokrycie <span class="math inline">\(A_n\)</span> zbioru <span class="math inline">\(\bigcup P_j\)</span>. Oznacza to, że podział <span class="math inline">\(\Pi\)</span> możemy podzielić na dwa podziały <span class="math display">\[\begin{aligned}
            &amp;\Pi = \Pi_1 \cup \Pi_2 \text{, takie że}\\
            &amp;\Delta\\
            &amp;\Pi_1 \cap \left\{ \bigcup_{j} P_j \right\} \neq \phi\\
            &amp;\text{oraz } \Pi_2 \cap \left\{ \bigcup_{j} P_j \right\} = \phi
        .\end{aligned}\]</span> <span class="math inline">\(\Delta\)</span> : każda kostka z <span class="math inline">\(\left\{ P_j \right\}\)</span> składa się z kostek należących do <span class="math inline">\(\Pi_1\)</span></p>
<p><span class="math inline">\(|\overline{S}(f,\Pi) - \underline{S}(f,\Pi)| = |\overline{S}(f,\Pi_1) - \underline(f,\Pi_1) + \overline{S}(f,\Pi_2) - \underline{S}(f,\Pi_2)|\)</span>, ale <span class="math display">\[\label{eq:Q}
            \overline{S}(f,\Pi_1) - \underline{S}(f,\Pi_1) = \sum_{Q_i\in\Pi_i}(\underset{x\in Q_i}{sup} f - \underset{x\in Q_I}{inf} f) Q_i |\]</span> Gdzie wiemy, że <span class="math inline">\(\sum |Q_i| &lt; \frac{1}{n}\)</span>, a <span class="math inline">\(f\)</span> - ograniczona na <span class="math inline">\(D\)</span> czyli <span class="math display">\[\underset{M}{\exists} .\underset{x\in D}{\forall} |f(x)| &lt; \frac{M}{4}
        .\]</span> Czyli <span class="math display">\[|\underset{x\in D}{\sup} f - \underset{x\in D}{\inf} f | &lt; M
        .\]</span> Zatem <span class="math display">\[\text{(\ref{eq:Q})} \le M \cdot \sum |Q_i| \le M \cdot  \frac{1}{n}
        .\]</span> Ale <span class="math display">\[\begin{aligned}
            &amp;\overline{S}(f,\Pi_2) - \underline{S}(f,\Pi_2) = \sum_{R_j \in \Pi_2}(\underset{x\in R_j}{\sup} f - \underset{x\in R_j}{\inf} f) |R_j|\\
            &amp;\le \frac{1}{n} \sum |R_j| \le \frac{1}{n} |D|
        .\end{aligned}\]</span> Zatem <span class="math display">\[|\overline{S}(f,\Pi) - \underline{S}(f,\Pi) \le M \cdot \frac{1}{n} + \frac{1}{n} |D| = \frac{1}{n} \cdot  const
        .\]</span> czyli możemy tak zwiększyć <span class="math inline">\(n\)</span>, że <span class="math inline">\(\underset{\varepsilon&gt;0}{\forall} \frac{1}{n}\cdot const &lt; \varepsilon \quad\Box\)</span><br />
Dlaczego wynika z tego prawdziwość dowodu dla całego <span class="math inline">\(A\)</span>?<br />
np. dla <span class="math inline">\(A_{2019}\)</span> działa, ale co dalej. Bo <span class="math inline">\(A_k\)</span> dla <span class="math inline">\(k&gt;n\)</span> też spełniają warunek, że <span class="math inline">\(\frac{1}{k}\cdot const &lt; \varepsilon\)</span>, a <span class="math inline">\(A_j\)</span> dla <span class="math inline">\(j&lt;n\)</span> jest takie, że <span class="math inline">\(A_j \subset A_n\)</span></p>
<p><span class="math inline">\(\implies\)</span><br />
Wiemy, że <span class="math inline">\(f\)</span> - całkowalne, czyli <span class="math display">\[\underset{\varepsilon&gt;0}{\forall} . \underset{\Pi}{\exists} |\overset{S}(f,\Pi) - \underset{S}(f,\Pi) | &lt; \varepsilon / n
        .\]</span> (chcemy pokazać, że <span class="math inline">\(A_n\)</span> jest zbiorem miary L. zero)</p>
<p><span class="math display">\[\begin{aligned}
            &amp;\Pi = \left\{ T_i \right\}\\
            &amp;\frac{\varepsilon}{n} &gt; |\overline{S}(f,\Pi) - \underline{S}(f,\Pi)| = \\
            &amp;\sum | \underset{x\in T_i}{\sup} f - \underset{x\in T_i}{\inf} f| |T_i|
            (*)
        .\end{aligned}\]</span> z podziału <span class="math inline">\(T_i\)</span> wybieram takie kostki <span class="math inline">\(P_i\)</span>, że <span class="math inline">\(|\underset{x\in P_i}{\sup} f - \underset{x\in P_i}{\inf} f \ge \frac{1}{n}\)</span>.<br />
Wówczas <span class="math display">\[\begin{aligned}
            &amp;(*) \ge \sum_{P_i} \frac{1}{n} | P_i | = \frac{1}{n} \sum |P_i|\\
            &amp;\text{czyli } \underset{\varepsilon&gt;0}{\forall} \frac{\varepsilon}{n} &gt; \frac{1}{n} \sum |P_i|\text{, gdzie $P_i$ jest pokryciem $A_n$}
        .\end{aligned}\]</span> Czyli <span class="math inline">\(A_n\)</span> jest zbiorem miary L. zero <span class="math inline">\(\quad\Box\)</span></p>
<p><span class="math inline">\(f(x,y) = x \sin(xy), \quad A=[0,1]\times[0,1]\)</span><br />
<span class="math inline">\(\int_A f \overset{\text{?}}{=} \int_{0}^{1} \varphi_1(x)dx \overset{\text{?}}{=} \int_0^1 \varphi_2(y)dy\)</span>,<br />
gdzie <span class="math inline">\(\varphi_1(x) = \int_0^1 x\sin(xy)dy\)</span>, <span class="math inline">\(\varphi_2(y) = \int_0^1 x\sin(xy)dx\)</span></p>
<figure>
<img src="img/fig_43.png" alt="życie było by proste gdybyśmy mogli tak robić" style="width:50.0%" /><figcaption>życie było by proste gdybyśmy mogli tak robić</figcaption>
</figure>
<p><span class="math display">\[\int_{A}f = \int_0^1 dx \int_0^1 dy f(x,y) \overset{\text{?}}{=} \int_0^1dy \int_0^1dx f(x,y)
        .\]</span></p>
<p>(Fubiniego)<br />
Niech <span class="math inline">\(f: A\times B\to \mathbb{R}\)</span>. <span class="math inline">\(A\subset\mathbb{R}^l, B\subset\mathbb{R}^k, A\times B\subset\mathbb{R}^n\)</span>, <span class="math inline">\(f\)</span> - ograniczona i całkowalna na <span class="math inline">\(A\times B\)</span>. Oznaczmy <span class="math inline">\(x^l\in A, y^k\in B\)</span>, <span class="math inline">\(A,B\)</span> - kostki.<br />
Niech <span class="math display">\[\varphi(x) = \overline{\int_B}f(x^l,y^k)dy^k, \psi(x) = \underline{\int_B} f(x^l, y^k)dy^k
        .\]</span> Wówczas <span class="math display">\[\int_{A\times B} f = \int_A \varphi = \int_A \psi
        .\]</span></p>
<p>całkowalnośc na <span class="math inline">\(A\times B\)</span> nie oznacza całkowalności na np. <span class="math inline">\(B\)</span>.</p>
<p>Niech <span class="math inline">\(\left\{ Q_i \right\} = \Pi_1\)</span> - podział zbioru <span class="math inline">\(A\)</span>, <span class="math inline">\(\left\{ R_j \right\} = \Pi_2\)</span> - podział zbioru <span class="math inline">\(B\)</span>.<br />
Wówczas <span class="math inline">\(\Pi_1 \times \Pi_2\)</span> - podział <span class="math inline">\(A\times B\)</span>. <span class="math display">\[\begin{aligned}
            &amp;\underline{S}(f,\Pi_1\times \Pi_2) = \\
            &amp;= \sum_{\substack{Q_i\\ R_j}} \underset{\substack{x\in Q_i\\ y\in R_j}}{\inf} f(x,y) |Q_i| |R_j| \le\\
            &amp;\sum_{Q_i}\sum_{R_j} \underset{x\in Q_i}{\inf} \underset{y\in R_j}{\inf} f(x,y) |Q_i| |R_j|\le\\
            &amp;\le \sum_{Q_i}\underset{x\in Q_i}{\inf} \underset{\text{suma dolna dla $\psi(x)$}}{\sum_{R_j}\underset{y\in R_j}{\inf} f(x,y) |R_j| |Q_i|}\le\\
            &amp;\le\sum_{Q_i}\underset{\text{bo suma dolna $\le$ całki dolnej}}{\underset{x\in Q_i}{\inf} \psi(x) |Q_i|} = \underset{S}(\psi,\Pi_1)
        .\end{aligned}\]</span> Ale <span class="math inline">\(\underline{\int_A}\psi(x) = \underset{\Pi}{\sup} \left| \sum_{Q_i}\underset{x\in Q_i}{\inf} \psi(x) | Q_i | \right|\)</span>.<br />
Czyli <span class="math inline">\(\underline{S}(f,\Pi_1\times\Pi_2) \le \underline{S}(\psi,\Pi_1)\)</span>. Analogicznie możemy pokazać, że<br />
<span class="math display">\[\underline{S}(\psi,\Pi_1)\le\underline{S}(\varphi,\Pi_1)\le\overline{S}(\varphi,\Pi_1) \le\overline{S}(f,\Pi_1\times\Pi_2).\]</span> Zatem <span class="math display">\[\underline{S}(f,\Pi_1\times\Pi_2) \le \underline{\underline{S}(\psi,\Pi_1)} \le \overline{S}(\psi, \Pi_1) \le \underline{\overline{S}(\varphi,\Pi_1)} \le \overline{S}(f,\Pi_1\times\Pi_2)
        .\]</span> Skoro <span class="math inline">\(f\)</span> - całkowalna na <span class="math inline">\(A\times B\)</span>, to <span class="math display">\[\underset{\varepsilon&gt;0}{\forall} .\underset{\Pi}{\exists} |\overline{S}(f,\Pi) - \underline{S}(f,\Pi)|&lt;\varepsilon
    .\]</span> Co oznacza, że <span class="math inline">\(\int_A \psi\)</span> i <span class="math inline">\(\int_B \varphi\)</span> - istnieją i wynoszą <span class="math inline">\(\int_{A\times B} f \quad\Box\)</span></p>
<h1 id="wykład-14.05.2019">Wykład (14.05.2019)</h1>
<p>Chcemy wygenerować wzór na zamianę zmiennych. Dawno dawno temu mogliśmy zrobić tak: <span class="math display">\[\int_2^4 2xe^{x^2}dx =  \mid x^2 = t, 2xdx = dt  \mid=  \int_4^{16}e^{t}dt
    .\]</span> Czyli w ogólności <span class="math display">\[\int_{\varphi(a)}^{\varphi(b)}f(x)dx = \int_a^b f(\varphi(t))\varphi&#39;(t)dt
    .\]</span> Jak weźmiemy całkę <span class="math display">\[\int f(x,y) dxdy = \int dx \int f(x,y)dy = \mid r=\sqrt{x^2+y^2} , \varphi = arctg(\frac{y}{x})   \mid =  \int dr \int d  \varphi f(r,\varphi) ??
    .\]</span></p>
<figure>
<img src="img/fig_45.png" alt="zmieniamy zmienne pojedynczo a nie jednocześnie (x,y)\to (x,\varphi)\to (r,\varphi)" style="width:80.0%" /><figcaption>zmieniamy zmienne pojedynczo a nie jednocześnie <span class="math inline">\((x,y)\to (x,\varphi)\to (r,\varphi)\)</span></figcaption>
</figure>
<p><span class="math display">\[\begin{aligned}
    &amp;\int dx \int dy f(x,y) = \Vert y = x \tg \varphi, dy = \frac{x}{\cos^2 \varphi}d \varphi \Vert = \int dx \int \frac{x}{\cos^2 \varphi} \varphi f(x,y(x,\varphi))= \\
    &amp;=\Vert x=r \cos \varphi, dx =dr \cos \varphi \Vert = \int d \varphi \int \frac{dr \cos\varphi r \cos \varphi}{\cos^2 \varphi} f(x(r,\varphi), y(x(r,\varphi))) =\\
    &amp;= \int d \varphi \int dr f(r,\varphi) r \text{, czyli } &quot;??&quot; = r
    .\end{aligned}\]</span> To teraz w drugą stronę. (<span class="math inline">\(y\to r\)</span>), ( <span class="math inline">\(x\to \varphi\)</span> ) <span class="math display">\[\begin{aligned}
        &amp;\int \int f(x,y) dxdy = \Vert y = \sqrt{r^2 - x^2} , dy = \frac{2r dr}{2 \sqrt{r^2-x^2} } \Vert = \\
        &amp;= \int dx \int \frac{r dr}{\sqrt{r^2 - x^2} } f(x,y(x,r)) = \Vert x=r \cos \varphi, dx = -r \sin\varphi d\varphi \Vert = \\
        &amp;=-\int dr \int \frac{r \sin\varphi d \varphi r}{\sqrt{r^2 - x^2} } f(x(r,\varphi),y(x(r,\varphi),r)) = \\
        &amp;= -\int dr r^2 \int d\varphi \frac{\sin\varphi f(r,\varphi)}{\sqrt{r^2 - r^2 \cos^2 \varphi} } = -\int dr \int d\varphi f(r,\varphi) r
    .\end{aligned}\]</span> Dostaliśmy prawie to co trzeba (<span class="math inline">\(r\)</span> ). Tylko wpadł jakiś dziwny minus. Podobno minus zniknie gdy doprowadzimy do porządku granice zmiennej <span class="math inline">\(\varphi\)</span>, bo <span class="math inline">\(x = r \cos \varphi\)</span> a <span class="math inline">\(\cos\)</span> jest malejący w tym przedziale. (tablica dalej nie działa - minęły 3 miesiące - z marsa by już doszła więc wysyłają pewnie z Saturna - MK<span class="math inline">\(\texttrademark\)</span>)</p>
<p>Niech <span class="math inline">\(\psi \begin{bmatrix} r\\\varphi \end{bmatrix} \to \begin{bmatrix} r \cos \varphi\\ r \sin \varphi \end{bmatrix}\)</span>. <span class="math display">\[\begin{aligned}
        &amp;\psi&#39; = \begin{bmatrix} \cos \varphi &amp;- r \sin \varphi\\ \sin \varphi &amp; r \cos \varphi \end{bmatrix}\\
        &amp;\Vert \psi&#39; \Vert = r\cos^2 \varphi - (-r\sin^2 \varphi) = r
    .\end{aligned}\]</span> Chcemy pokazać, że jeżeli <span class="math inline">\(\varphi: A\to A\)</span>, <span class="math inline">\(A\subset\mathbb{R}^n\)</span>, <span class="math inline">\(\varphi\)</span> - klasy <span class="math inline">\(\mathcal{C}^1, \varphi^{-1}\)</span> - klasy <span class="math inline">\(\mathcal{C}^1\)</span>, to możemy przedstawić <span class="math inline">\(\varphi\)</span> jako złożenie dwóch transformacji, z których pierwsza nie zmienia <span class="math inline">\(n-1\)</span> zmiennych a druga nie zmienia <span class="math inline">\(1\)</span> zmiennej (transformacje pierwotne/prymitywne albo inne ubogacające nazwy).</p>
<p>(coś w rodzaju dowodu)<br />
<span class="math inline">\(\varphi\)</span> możemy przedstawić jako <span class="math display">\[\varphi \begin{bmatrix} t_1\\ \vdots \\ t_n \end{bmatrix} \to \begin{bmatrix} \varphi_1(t_1,\ldots,t_n)\\ \vdots \\ \varphi_n(t_1,\ldots,t_n) \end{bmatrix} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
        .\]</span></p>
<p>Czy istnieje odwzorowanie <span class="math inline">\(\Theta^{-1}: A\to A\)</span> takie, że <span class="math display">\[\Theta = \begin{bmatrix} t_1\\ \vdots \\ t_n \end{bmatrix} = \begin{bmatrix} t_1 \\ \vdots \\ t_{j-1} \\ \varphi_j(t_1,\ldots,t_n)\\ t_{j+1} \\ \vdots \\ t_n \end{bmatrix} = \begin{bmatrix} x_1 \\ \vdots \\ x_{j-1} \\ x_j \\ x_{j+1} \\ \vdots \\ x_n \end{bmatrix}
            .\]</span> (<span class="math inline">\(t_{i\neq j}\)</span> ) mogą zostać zamiast zamieniać je na <span class="math inline">\(x_i\)</span>. Dlaczego interesuje nas czy istnieje funkcja odwrotna? Bo jeżeli istnieje, to możemy zapisać <span class="math display">\[\varphi = \varphi \circ \Theta^{-1} \circ \Theta = \left(\varphi \circ \Theta^{-1}\right) \circ \Theta
            .\]</span></p>
<p>Wiemy, że <span class="math inline">\(\varphi\)</span> - klasy <span class="math inline">\(\mathcal{C}^1\)</span> i <span class="math inline">\(\varphi^{-1}\)</span> - klasy <span class="math inline">\(\mathcal{C}^1\)</span> i <span class="math inline">\(\varphi: A\to A\)</span>. Mamy twierdzenie o lokalnej odwracalności!<br />
<span class="math inline">\(det \varphi&#39; \neq 0\)</span> , czyli w macierzy <span class="math inline">\(\varphi&#39;\)</span> istnieje prznajmniej <span class="math inline">\(1\)</span> element niezerowy. (w rzeczywistości to zawsze będzie trochę więcej - nieśmiały warunek)</p>
<p>np. <span class="math inline">\(\frac{\partial \varphi^i}{\partial t^i} \neq 0\)</span>. Oznacza to, że odwzorowanie <span class="math display">\[\eta : \begin{bmatrix} t_1 \\ t_2 \\ \vdots \\ t_{i-1} \\t_i \\ t_{i+1} \\ \vdots \\ t_n \end{bmatrix} \to \begin{bmatrix} x_1\\ x_2 \\ \vdots \\ x_{j-1} \\ x_j = \varphi^i (t_1,\ldots,t_n) \\ \vdots \\ x_n \end{bmatrix}
        .\]</span> Wtedy <span class="math display">\[\eta&#39; = \begin{bmatrix} 1&amp;&amp;&amp;&amp;\\ &amp;1&amp;&amp;&amp; \\ \ldots&amp;\ldots&amp; \frac{\partial \varphi^i}{\partial x^i} &amp;\ldots&amp;\ldots \\ &amp;&amp;&amp;1&amp; \\ &amp;&amp;&amp;&amp;1\end{bmatrix}
        .\]</span> i <span class="math inline">\(\det \eta&#39; \neq 0\)</span>, więc istnieje <span class="math inline">\(\eta^{-1}\)</span>.<br />
Czyli <span class="math inline">\(\varphi = \varphi \circ \eta \circ \eta^{-1} = (\varphi \circ \eta ) \circ \eta^{-1} \quad\Box\)</span></p>
<p><span class="math display">\[\int_{-1}^{1}dx\int_{-\sqrt{1-x^2} }^{\sqrt{1-x^2} }dy f(x,y) = \int_0^1 r dr \int_0^{2\pi} d\varphi f(r,\varphi)\]</span></p>
<p>(O zamianie zmiennych)<br />
Niech <span class="math inline">\(\Theta, \Omega\)</span> - zbiory otwarte w <span class="math inline">\(\mathbb{R}^n\)</span> i <span class="math inline">\(\xi: \Omega\to \Theta\)</span>, <span class="math inline">\(f: \Theta\to \mathbb{R}\)</span>, <span class="math inline">\(f\)</span> - ograniczona i całkowalna. <span class="math inline">\(\xi\)</span> - klasy <span class="math inline">\(\mathcal{C}^1\)</span> na <span class="math inline">\(\Omega\)</span>, <span class="math inline">\(\xi^{-1}\)</span> klasy <span class="math inline">\(\mathcal{C}^{1}\)</span> na <span class="math inline">\(\Theta\)</span>. Wtedy <span class="math display">\[\int_{\Theta} f(x) dx =  \int_{\Omega} f(\xi(t)) | \det \xi&#39;(t) | dt.\]</span> <span class="math inline">\(x=(x^1,\ldots,x^n)\in \Theta, t=(t^1,\ldots,t^n)\in \Omega\)</span></p>
<figure>
<img src="img/fig_46.png" alt="\Omega\to\Theta - f - \mathbb{R}" style="width:80.0%" /><figcaption><span class="math inline">\(\Omega\to\Theta - f - \mathbb{R}\)</span></figcaption>
</figure>
<p>(przez indukcję względem wymiaru przestrzeni)<br />
</p>
<ul>
<li><p>dla <span class="math inline">\(n=1\)</span> - zrobione w I semetrze.</p></li>
<li><p>zakładamy, że prawdziwy jest napis <span class="math display">\[\int_{A&#39;\subset\mathbb{R}^{n-1}} f(x)dx = \int_{\Omega&#39; \subset\mathbb{R}^{n-1}} f(\xi(t)) | det(\xi&#39;(t)) |, (\xi: \mathbb{R}^{n-1}\to \mathbb{R}^{n-1})
                .\]</span></p></li>
</ul>
<p>Chcem pokazać, że prawdziwy jest napis <span class="math display">\[\int_{A\subset\mathbb{R}} f(x) dx = \int_{\Omega \subset \mathbb{R}^n} f(\xi(t)) | det(\xi&#39;(t)) |
        .\]</span> Uwaga: wartośc bezwzględna oznacza, że musimy uważać przy rozstawianiu granic:<br />
<span class="math inline">\(\left( \int_a^b f \right)\)</span> oznacza, że zakładamy, że <span class="math inline">\(a\le b\)</span>. Dowód przeprowadzamy dla <span class="math inline">\(\xi: \Theta\subset\mathbb{R}^n\to \Omega\subset\mathbb{R}^n\)</span> takiego, że <span class="math inline">\(\xi\)</span> nie zamienia jednej zmiennej.</p>
<p>Niech <span class="math inline">\(K = \left\{ (x,y), x^2+y^2 \le 1 \right\}\)</span>, niech <span class="math inline">\(K_a = \left\{ (x,a), x^2+a^2 \le 1 \right\}.\)</span><br />
Wówczas <span class="math inline">\(K = \bigcup_{a\in[-1,1]}K_a\)</span>, zatem <span class="math inline">\(\int_K f = \int_{-1}^1 da \int _{K_a} f\)</span></p>
<p>Ostatnio skończyliśmy na kroku <span class="math inline">\(n-1\to n\)</span> i wiemy, że dla <span class="math inline">\(n-1\)</span> wymiarów możemy napisać <span class="math display">\[\int_{A\in \mathbb{R}^{n-1}}f(x)dx = \int_{B\in \mathbb{R}^{n-1}}f(\xi(t)) |\det \xi&#39;|dt
        .\]</span></p>
<figure>
<img src="img/fig_48.png" alt="Kółko K składamy z kresek K_a i mamy \int_K f = \int da \int_{K_a} f" style="width:30.0%" /><figcaption>Kółko <span class="math inline">\(K\)</span> składamy z kresek <span class="math inline">\(K_a\)</span> i mamy <span class="math inline">\(\int_K f = \int da \int_{K_a} f\)</span></figcaption>
</figure>
<p><span id="fig:fig_48" label="fig:fig_48">[fig:fig_48]</span></p>
<p><span class="math display">\[\int_{\Theta} f dx = \int_{\Omega}f(\xi(t))|\det \xi&#39;|dt
    .\]</span> Mając zbiór <span class="math inline">\(\Theta\)</span>, zdefiniujmy zbiór <span class="math inline">\(\Theta_a\)</span>, który jest zbiorem takich <span class="math inline">\(x\in\Theta\)</span>, że na miejsca <span class="math inline">\(x_i\)</span> wstawimy wielkość <span class="math inline">\(a\)</span>. <span class="math display">\[\Theta_a = \left\{ x\in \mathbb{Q}, x = \left( x^1,x^2,\ldots,x^{i-1},a,x^{i+1},\ldots,x^n \right)  \right\}
    .\]</span> <span class="math display">\[K = \left\{ (x,y), x^2+y^2 = 1 \right\}
    .\]</span> <span class="math display">\[K_a = \left\{ (x,y)\in K, (x,y) = (x,a) \right\}, \left\{ (x,a), x^2+a^2 = 1 \right\}
    .\]</span> Oznacza to, że <span class="math display">\[\int_\Theta f dx = \int da \int_{\Theta_a}f(x^1,x^2,\ldots,x^{i-1},a,x^{i+1},\ldots,x^n) dx^1 dx^2 \ldots dx^{i-1} dx^{i+1}\ldots dx^n
    .\]</span> Rozważmy <span class="math inline">\(\xi: \Theta \to \Omega\)</span> taką, że <span class="math display">\[\begin{bmatrix} t_1\\ \vdots \\ t_n \end{bmatrix} \to \begin{bmatrix} \xi_1(t_1,\ldots,t_n)\\ \xi_2(t_1,\ldots,t_n) \\ \vdots \\ \xi_{i-1} \\ t_1 \\ \xi_{i+1} \\ \vdots \\ \xi_n(t_1,\ldots,t_n)
    \end{bmatrix} \begin{bmatrix} x^1\\x^1\\ \vdots \\ x^i \\ \vdots \\ x^n \end{bmatrix}
    .\]</span> Czyli <span class="math inline">\(\xi\)</span> nie zmienia jednej współrzędnej np. <span class="math inline">\(\begin{bmatrix}
        x\\y
    \end{bmatrix}
    \to
    \begin{bmatrix}
        r\\x
    \end{bmatrix}\)</span>.<br />
Możemy więc zapisać transformację <span class="math inline">\(\xi_a: \Theta_a\to\Omega_a\)</span></p>
<p><span class="math display">\[\begin{bmatrix} t_1\\ \vdots\\ t_{i-1}\\ t_{i+1} \\ \vdots \\ t_n \end{bmatrix} \to \begin{bmatrix} \xi_1(t_1,\ldots,t_n) \\ \xi_2(t_1,\ldots,t_n) \\ \vdots \\ \xi_{i-1}(\ldots) \\ \xi_{i+1}(\ldots) \\ \vdots \\ \xi_n(t_1,\ldots,t_n)\end{bmatrix}
    .\]</span> Wówczas na mocy założenia indukcyjnego wiemy, że <span class="math display">\[\begin{aligned}
        &amp;\int_{\Theta_a}f(x^1,\ldots,x^{i-1},a,x^{i+1},\ldots,x^n) dx^1 \ldots dx^{i-1} dx^{i+1} \ldots dx^n = \\
        &amp;\int_{\Omega_a} f(t_1,t_2,\ldots,t_{i-1},a,t_{i+1},\ldots,t_n) |\det \xi&#39;_a | dt^1 dt_2 \ldots dt^{i-1} dt^{i+1} \ldots dt^n
    .\end{aligned}\]</span> Wówczas <span class="math display">\[\begin{aligned}
        &amp;\int_{\Theta}f(x^1,\ldots,x^n)dx^n = \int_a da \int_{\Omega_a}f(t_1,\ldots,t_{i-1},a,t_{i+1},\ldots,t_n) |\det \xi&#39;_a| \cdot (\pm 1) dt^1 \ldots dt^{i-1} dt^{i+1} \ldots dt^n\\
        &amp;= \begin{bmatrix} a=t_i \end{bmatrix} =\\
        &amp;= \int_{\Omega} f(t^1,t^2,\ldots,t^n)|\det \xi&#39; | dt^1 \ldots dt^n
.\end{aligned}\]</span></p>
<p><span class="math display">\[\xi&#39; = \begin{bmatrix} \frac{\partial \xi_1}{\partial t_1} &amp; \frac{\partial \xi_2}{\partial t_2} &amp; \ldots &amp;&amp; \frac{\partial \xi_n}{\partial t_n}\\ 0 &amp; \ldots &amp; 1 &amp; \ldots &amp; 0 \\ \frac{\partial \xi_n}{\partial t_1} &amp; \ldots &amp;&amp;&amp; \frac{\partial \xi_n}{\partial t_n}  \end{bmatrix}
.\]</span></p>
<p><img src="img/fig_47.png" alt="image" style="width:80.0%" /></p>
<p>Policzmy całkę <span class="math inline">\(I = \int_0^{\infty}e^{-x^2}dx\)</span>. Nie umiemy. Ale skoro nie umiemy policzyć <span class="math inline">\(I\)</span>, to tym bardziej <span class="math inline">\(I^2?\)</span> <span class="math display">\[I^2 = \int_0^{\infty}e^{-x^2}dx \int_0^{\infty}e^{-y^2}dy = \int_{\Box} e^{-(x^2+y^2)}
        .\]</span> Zamieńmy sobie zmienne: <span class="math inline">\(x = r\cos \varphi,\quad y = r\sin \varphi\)</span>. <span class="math inline">\(\psi: \begin{bmatrix} r\\\varphi\end{bmatrix}\to \begin{bmatrix} x\\y \end{bmatrix}\)</span>, <span class="math inline">\(|\psi&#39;| = r\)</span> Mamy <span class="math display">\[\begin{aligned}
             &amp;I^2 = \int_0^{\infty}dr \int_0^{\frac{\pi}{2}}d\varphi e^{-r^2}r = \frac{\pi}{2} \lim\limits_{p\to +\infty} \int_0^p dr \cdot e^{-r^2} \cdot r = \frac{\pi}{2} \lim\limits_{p\to\infty} \left[ -\frac{1}{2}e^{-r^2} \right] _0^p\\
             &amp;\frac{\pi}{2}\left[ \lim\limits_{p\to\infty}\left[ -\frac{1}{2}e^{-p^2} \right] - \left[ -\frac{1}{2}e^{(0)^2} \right]  \right] = \frac{\pi}{2} \frac{1}{2} = \frac{\pi}{4}
         \end{aligned}\]</span> czyli <span class="math inline">\(I^2 = \frac{\pi}{4} \implies I = \frac{\sqrt{\pi} }{2}\)</span></p>
<h1 id="wykład-17.05.2019">Wykład (17.05.2019)</h1>
<h2 id="formy-różniczkowe">Formy różniczkowe</h2>
<p>(czyli o uprawianiu analizy na powierzchni balonika albo kartki)</p>
<p>Niech <span class="math inline">\(M\subset\mathbb{R}^n\)</span> - taki, że dla każdego punktu <span class="math inline">\(p\in M\)</span> istnieje otoczenie otwarte <span class="math inline">\(U\subset M\)</span></p>
<p>(sfera, obwarzanek, itd., okrąg), (stożek - nie ok!), (taka ósemka co się przecina - też nie)</p>
<p>Niech <span class="math inline">\(U\)</span> - zbiór otwarty <span class="math inline">\(\subset M\)</span> i niech odwzorowanie <span class="math inline">\(\varphi: U\to \mathbb{R}^n\)</span> takie, że <span class="math inline">\(\varphi\)</span> - klasy <span class="math inline">\(\mathcal{C}^1\)</span>, (czasami <span class="math inline">\(\mathcal{C}^\infty\)</span> ), <span class="math inline">\(\varphi^{-1}\)</span> - klasy <span class="math inline">\(\mathcal{C}^1\)</span>, (czasami <span class="math inline">\(\mathcal{C}^\infty\)</span> ) nazywamy mapą.<br />
Uwaga: mapa pokrywać całego zbioru <span class="math inline">\(M\)</span>.</p>
<p>Wyobraźmy sobie, że mamy jakiś zbiór <span class="math inline">\(M\)</span>. Połowa tego zbioru to niech będzie <span class="math inline">\(U_1\)</span>, i ono się przecina z <span class="math inline">\(U_2\)</span>. <span class="math inline">\(U_1\)</span> i <span class="math inline">\(U_2\)</span> możemy rozłożyć na prostokąty w <span class="math inline">\(\mathbb{R}^2\)</span>. Co się stanie z punktami mapowanymi do obu <span class="math inline">\(U\)</span>?</p>
<p><span class="math inline">\((U^1,\varphi^1), (U^2,\varphi^2)\)</span> - mapy na <span class="math inline">\(M\)</span>.<br />
<span class="math inline">\(U_1\)</span> i <span class="math inline">\(U_2\)</span> nazywamy zgodnymi jeżeli<br />
<span class="math inline">\(a)\)</span> <span class="math inline">\(U_1\cap U_2 = \phi\)</span><br />
albo odwzorowanie <span class="math inline">\(\varphi_2 \circ \varphi_1^{-1}: \varphi_1(U_1\cap U_2)\to \varphi_2(U_2\cap U_1)\)</span> jest bijekcją (klasy powiedzmy sobie <span class="math inline">\(\mathcal{C}^1, \mathcal{C}^{\infty}\)</span> )</p>
<figure>
<img src="img/fig_49.png" alt="M = \left\{ (x,y): x^2 + y^2 = 1^2 \right\}" style="width:50.0%" /><figcaption><span class="math inline">\(M = \left\{ (x,y): x^2 + y^2 = 1^2 \right\}\)</span></figcaption>
</figure>
<p><span class="math display">\[\begin{aligned}
    &amp;U_1 = \left\{ (x,y)\in M, y&gt;0 \right\},\quad \varphi_1: (x,y)\in U_1 \to x\\
    &amp;U_2 = \left\{ (x,y)\in M, x&gt;0 \right\},\quad \varphi_2: (x,y)\in U_2 \to y\\
    &amp;U_3 = \left\{ (x,y)\in M, y&lt;0 \right\},\quad \varphi_3: (x,y)\in U_3 \to x\\
    &amp;U_4 = \left\{ (x,y)\in M, x&lt;0 \right\},\quad \varphi_4: (x,y)\in U_4 \to y
    .\end{aligned}\]</span> <span class="math inline">\(U_1\)</span> i <span class="math inline">\(U_3\)</span> oraz <span class="math inline">\(U_2\)</span> i <span class="math inline">\(U_4\)</span> są zgodne. Czy zgodne są <span class="math inline">\(U_1\)</span> i <span class="math inline">\(U_2\)</span>? Czyli chcemy zbadać odwzorowanie <span class="math inline">\(\varphi_1(U_1\cap U_2)\to \varphi_2(U_1\cap U_2)\)</span>, ale <span class="math inline">\(\varphi_1(x,y)\in U_1\to x\)</span>.<br />
Czyli <span class="math inline">\(\varphi_1^{-1}(x)\to \left( x, \sqrt{1-x^2}  \right)\)</span>,<br />
czyli <span class="math inline">\(\varphi_2(\varphi_1^{-1}(x) = \varphi_2((x,\sqrt{1-x^2} )) = \sqrt{1-x^2}\)</span>.<br />
Zatem czy <span class="math inline">\(\varphi_2 \circ \varphi_{1}^{-1}(x) = \sqrt{1-x^2}\)</span> przerzuca <span class="math inline">\(]0,1[\to]0,1[\)</span> jest różniczkowalne? Odpowiedź: na zbiorze <span class="math inline">\(]0,1[\)</span> jest.</p>
<p>Kolekcję zgodnych map nazywamy atlasem. Zbiór <span class="math inline">\(M\)</span> wraz z atlasem, który pokrywa cały <span class="math inline">\(M\)</span> nazywamy <strong>rozmaitością</strong> (<em>ang. manifold</em>).</p>
<h1 id="wykład-21.05.2019">Wykład (21.05.2019)</h1>
<p>Chcemy powiedzieć co to są wektory w takim świecie? Zaczniemy rysować krzywą po powierzchni.</p>
<p>Niech <span class="math inline">\(M\)</span> - rozmaitość. Odwzorowanie <span class="math inline">\(\sigma: ]-\varepsilon,\varepsilon[ \subset\mathbb{R} \to \sigma(t)\in M\)</span> nazywamy krzywą na <span class="math inline">\(M\)</span>. <span class="math inline">\(\sigma\)</span> jest klasy <span class="math inline">\(\mathcal{C}^\infty\)</span></p>
<p>(spirala na walcu) <span class="math display">\[\sigma: ]-\varepsilon,\varepsilon[ \to \begin{bmatrix} \cos(t)\\ \sin(t) \\ t \end{bmatrix}
         .\]</span></p>
<p>Niech <span class="math inline">\(p\in M\)</span>, <span class="math inline">\(\sigma_1,\sigma_2\)</span> - krzywe na <span class="math inline">\(M\)</span> takie, że <span class="math inline">\(\sigma_1(0) = \sigma_2(0) = P\)</span>. Mówimy, że <span class="math inline">\(\sigma_1\)</span> i <span class="math inline">\(\sigma_2\)</span> są , jeżeli <span class="math display">\[\left.\frac{d(\varphi_0\cdot \sigma_1(t))}{d t}\right|_{t=0} = \left.\frac{d(\varphi_0 \cdot \sigma_2(t))}{dt}\right|_{t=0}
        .\]</span></p>
<p>Rozważmy wszystkie krzywe przechodzące przez punkt <span class="math inline">\(P\in M\)</span>. Na tym zbiorze wprowadzamy relację: <span class="math inline">\(\sigma_1 \sim \sigma_2\)</span> jeżeli <span class="math inline">\(\sigma_1\)</span> i <span class="math inline">\(\sigma_2\)</span> są styczne. Jeżeli <span class="math inline">\(\sigma\)</span> krzywa przechodząca przez punkt <span class="math inline">\(P\)</span>, to wektorem stycznym zaczepionym w punkcie <span class="math inline">\(P\)</span> nazwiemy <span class="math inline">\(v = \underset{\substack{\text{klasa}\\ \text{równoważności}}}{[\sigma]}\)</span></p>
<p>Weźmy krzywą <span class="math inline">\(\sigma(t) = \begin{bmatrix} \cos(t) \\ \sin(t) \\ t \end{bmatrix} , p = \begin{bmatrix} 1\\0\\0 \end{bmatrix}\)</span>.<br />
<span class="math display">\[\sigma&#39;(t) = \begin{bmatrix} -\sin(t)\\ \cos(t)\\ 1 \end{bmatrix}, \sigma&#39;(0) = \begin{bmatrix} 0\\1\\1 \end{bmatrix}
        .\]</span></p>
<p><span id="fig:fig_52" label="fig:fig_52">[fig:fig_52]</span></p>
<p>Niech <span class="math inline">\(f(p) = C \underset{p\in M}{\forall}\)</span>. Ile wynosi <span class="math inline">\(v(f)?\)</span> <span class="math display">\[\begin{aligned}
            &amp;v(f) = v(c) = v(c \cdot 1) = c \cdot v(1) = \\
            &amp;=  c \cdot v(1\cdot 1) = c \cdot (1 \cdot v(1) + 1v(1)) =\\
            &amp;= c \cdot 2v(1) = 2v(c) = 2v(f)
        .\end{aligned}\]</span> Czyli <span class="math inline">\(v(f) = 2v(f)\)</span>, czyli <span class="math inline">\(v(f) = 0\)</span> (pochodna stałej <span class="math inline">\(=0\)</span> )</p>
<p><strong>Każdy operator, który to umie to różniczkowanie.</strong></p>
<p>Jak można w praktyce zrealizować taki operator?<br />
Niech <span class="math inline">\(v\in T_pM, v=[\sigma]\)</span> <span class="math display">\[v(f) = \frac{d}{dt}f(\sigma(t))|_{t=0}
        .\]</span></p>
<p>Zbiór wszystkich wektorów stycznych zaczepionych w punkcie <span class="math inline">\(p\in M\)</span> oznaczamy przez <span class="math inline">\(T_pM\)</span> i nazywamy przestrzenią styczną.</p>
<p>Chcemy wyposażyć <span class="math inline">\(T_pM\)</span> w strukturę przestrzeni wektorowej. Potrzebujemy działań.<br />
Niech <span class="math inline">\(v_1,v_2\in T_pM\)</span> i <span class="math inline">\(v_1 = [\sigma_1], v_2 = [\sigma_2]\)</span>. Wówczas<br />
<span class="math display">\[\begin{aligned}
         &amp;v_1 \diamond v_2 \overset{\text{def}}{=} \left[ \varphi^{-1}(\varphi(\sigma_1))+\varphi(\sigma_2) \right]\\
         &amp;\underset{\alpha\in\mathbb{R}}{\forall} \alpha\cdot v_1 \overset{\text{def}}{=} \left[ \varphi^{-1}(\alpha\cdot \varphi(\sigma_1) \right]
    .\end{aligned}\]</span> <span class="math inline">\(T_pM\)</span> wraz z działaniami (<span class="math inline">\(\diamond, \cdot\)</span> ) ma strukturę przestrzeni wektorowej. Zbiór <span class="math display">\[TM \overset{\text{def}}{=} \left\{ p\in M, T_pM \right\}\]</span> nazywamy wiązką styczną.</p>
<p>Czy w <span class="math inline">\(TM\)</span> możemy zadać strukturę przestrzeni wektorowej?<br />
Odpowiedź: NIE DA SIĘ</p>
<p>Zbiór wszystkich różniczkowań w punkcie <span class="math inline">\(P\)</span> oznaczamy przez <span class="math inline">\(D_pM\)</span></p>
<p>Chcemy nadać <span class="math inline">\(D_pM\)</span> strukturę przestrzeni wektorowej. <span class="math display">\[\begin{aligned}
        &amp;v_1,v_2\in D_pM, f\in \mathcal{C}^{\infty}(M) \implies (v_1 \diamond v_2)f \overset{\text{def}}{=} v_1(f) + v_2(f)\\
        &amp;\underset{\alpha\in\mathbb{R}}{\forall} (\alpha \bowtie v_1)f = \alpha \cdot v_1(f) \\
    .\end{aligned}\]</span></p>
<p>Co to znaczy, że <span class="math inline">\(f\)</span> - klasy <span class="math inline">\(C^{\infty}(M)\)</span>?</p>
<p><span id="fig:fig_53" label="fig:fig_53">[fig:fig_53]</span></p>
<p>Jeżeli <span class="math inline">\(\psi \circ f \circ \varphi^{-1}\)</span> - jest klasy <span class="math inline">\(\mathcal{C}^{\infty}\)</span>.<br />
Związek między <span class="math inline">\(T_pM\)</span>, a <span class="math inline">\(D_pM\)</span> :<br />
Niech <span class="math inline">\(v = 5e_x + 6e_y \in T_pM\)</span>. Czy znajdziemy odwzorowanie z <span class="math inline">\(T_pM\)</span> do <span class="math inline">\(D_pM\)</span>, (które dokładnie jednemu <span class="math inline">\(v\)</span> przyporządkowałoby jeden element).<span class="math inline">\(\rightarrow\)</span> izomorfizm między <span class="math inline">\(T_pM\)</span> i <span class="math inline">\(D_pM\)</span>.</p>
<h2 id="przestrzeń-różniczkowa">Przestrzeń różniczkowa</h2>
<p>Niech <span class="math inline">\(f: M\to \mathbb{R}\)</span>, <span class="math inline">\(f\)</span> - klasy <span class="math inline">\(\mathcal{C}^{\infty}(M)\)</span><br />
niech <span class="math inline">\(v\left(  \right) : \mathcal{C}^{\infty}(M)\to\mathbb{R}\)</span>, takie, że <span class="math display">\[\begin{aligned}
    &amp;\underset{f,g\in\mathcal{C}^{\infty}\left( M \right) }{\forall}v(f\cdot g) = v(f) + v(g)\\
    &amp;\underset{\alpha\in\mathbb{R}}{\forall} \underset{f\in\mathcal{C}^\infty(M)}{\forall} v(\alpha f) = \alpha v(f)\\
    &amp; \underset{f,g\in\mathcal{C}^\infty (M)}{\forall} v(f\cdot g) = f(p) \cdot v(g) + g(p)v(f)
    .\end{aligned}\]</span> <span class="math inline">\(v\left(  \right)\)</span> spełniający te warunki nazywamy różniczkowaniem w punkcie <span class="math inline">\(p\)</span>.</p>
<h1 id="wykład-24.05.2019">Wykład (24.05.2019)</h1>
<p>Widzimy, że ten obrazek, który nam się jawi jest w miarę rozbudowany. Ostatnio wprowadziliśmy na baloniku układ współrzędnych.<br />
<span class="math display">\[\begin{aligned}
        &amp;v = [\sigma], v\in T_pM, v\left(  \right) \in D_pM\\
        &amp;v(f) = \frac{d}{dt}f(\sigma(t))|_{t=0} \\
        &amp;v\left(  \right) = ?\cdot ? + ?\cdot ? + ?\cdot ? = \frac{d}{dt}(f\circ \varphi^{-1} \circ \varphi \circ \sigma(t))|_{t=0}=\\
        &amp;= \frac{d}{dt}(\tilde f\circ \varphi(\sigma(t)))|_{t=0} = \frac{d}{dt}(\tilde f(\varphi^1(\sigma(t)),\varphi^2(\sigma(t)),\ldots,\varphi^n(\sigma(t))))|_{t=0}= \\
        &amp;= \frac{\partial \tilde f}{\partial x^1} \frac{\partial \varphi^1(\sigma(t))}{\partial t} +\ldots+\frac{\partial \tilde f}{\partial x^n} \frac{\partial \varphi^n(\sigma(t))}{\partial t}
    .\end{aligned}\]</span> Czyli jeżeli <span class="math inline">\(v\in T_pM\)</span> i <span class="math inline">\(v\in[\sigma]\)</span>, to wiemy, że <span class="math display">\[\begin{aligned}
        &amp;v = \frac{\partial \varphi^1(\sigma(t))}{\partial t}|_{t=0} e_1 + \frac{\partial \varphi^2(\sigma(t))}{\partial t}|_{t=0} e_2 + \ldots + \frac{\partial \varphi^n(\sigma(t))}{\partial t}|_{t=0} e_n= \\
        &amp;= \xi^1e_1 + \xi^2e_2 + \ldots + \xi^ne_n = \\
        &amp;= \xi_1 \frac{\partial \tilde f}{\partial x^1} + \xi_2 \frac{\partial \tilde f}{\partial x^2} +\ldots+ \xi_n \frac{\partial \tilde f}{\partial x^n} = v(f)
    .\end{aligned}\]</span> Zatem <span class="math display">\[v\left(  \right) = \xi_1 \frac{\partial }{\partial x^1} + \xi_2 \frac{\partial }{\partial x^2}  + \ldots + \xi_n \frac{\partial }{\partial x^n}
    .\]</span></p>
<p>Więc niech <span class="math inline">\(v = 2e_x + 3e_y \to v\left(  \right) = 2 \cdot \frac{\partial }{\partial x} + 3 \cdot \frac{\partial }{\partial y}\)</span>.</p>
<p>Wniosek: mając izomorfizm między <span class="math inline">\(T_pM\)</span> i <span class="math inline">\(D_pM\)</span> możemy zapisać bazy: <span class="math display">\[T_pM = \left&lt;e_1,e_2,\ldots,e_n \right&gt;
    .\]</span> To wtedy <span class="math display">\[D_pM = \left&lt;\frac{\partial }{\partial x^1} , \frac{\partial }{\partial x^2} ,\ldots,\frac{\partial }{\partial x^n}  \right&gt;
    .\]</span> np. <span class="math inline">\(v = 7 e_r + 8 e_\varphi \to v\left(  \right) = 7 \frac{\partial }{\partial r} + 8 \frac{\partial }{\partial \varphi}\)</span> (często użyjemy bazy z <span class="math inline">\(D_pM\)</span> jako bazy <span class="math inline">\(T_pM\)</span> ).</p>
<p>Wektorem kostycznym (albo jednoformą) nazywamy odwzorowanie liniowe <span class="math inline">\(\omega: T_pM\to\mathbb{R}\)</span>. Zbiór jednoform (<span class="math inline">\(p\in M\)</span>) oznaczamy przez <span class="math inline">\(T_p^* M\)</span> (lub <span class="math inline">\(\Lambda^1(M)\)</span>, <span class="math inline">\(\Lambda^1(\theta), \theta\in M\)</span>)</p>
<p>Skoro <span class="math inline">\(T_p^* M\)</span> jest dualna do <span class="math inline">\(T_pM\)</span>, to ma ten sam wymiar i możemy wprowadzić bazę dualną. <span class="math inline">\(T_p^*M = \left&lt; dx^1, dx^2, \ldots, dx^n \right&gt;\)</span>, gdzie <span class="math inline">\(\left&lt;dx^i, \frac{\partial }{\partial x^i}  \right&gt; = \delta^i_j\)</span></p>
<p>Niech <span class="math inline">\(\Lambda^1(M)\to\omega = 7dx + 3dy, v\in T_pM = 2 \frac{\partial }{\partial x} +4 \frac{\partial }{\partial y}\)</span>, wówczas <span class="math display">\[\begin{aligned}
            &amp;\left&lt;\omega,v \right&gt; = \left&lt;7dx+3dy,2 \frac{\partial }{\partial x} + 4 \frac{\partial }{\partial y}  \right&gt; =\\
            &amp;= \left&lt;7dx, 2 \frac{\partial }{\partial x} + 4 \frac{\partial }{\partial y}  \right&gt; + 3 \left&lt; dy, 2 \frac{\partial }{\partial x} + 4 \frac{\partial }{\partial y}  \right&gt; = \\
            &amp;= 7\cdot 2\left&lt;dx,\frac{\partial }{\partial x}  \right&gt; + 7\cdot 4 \left&lt;dx, \frac{\partial }{\partial y}  \right&gt; + 3\cdot 2 \left&lt;dy,\frac{\partial }{\partial x}\right&gt; + 3\cdot 4 \left&lt;dy , \frac{\partial }{\partial y} \right&gt; \\
            &amp;= 7\cdot 2+ 3\cdot 4
        .\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
            &amp;v = A^x \frac{\partial }{\partial x} + A^y \frac{\partial }{\partial y} + A^z \frac{\partial }{\partial z} , \omega = B_xdx + B_ydy + B_zdz=\\
            &amp;= \left&lt;\omega,v \right&gt; = A^xB_x + A^yB_y + A^zB_z
        .\end{aligned}\]</span></p>
<figure>
<img src="img/fig_50.png" alt="Strumień przez balonik" /><figcaption>Strumień przez balonik</figcaption>
</figure>
<p>Zbiór wszystkich odwzorowań <span class="math inline">\(T_pM \times \ldots \times T_pM \to \mathbb{R}\)</span> k - liniowych w każdej zmiennej i antysymetrycznych oznaczamy przez <span class="math inline">\(\Lambda^k(M)\)</span> i nazywamy k-formami.</p>
<p>Niech <span class="math inline">\(\alpha\in T_p^*M, \beta\in T_p^*M ( \alpha\in \Lambda_p^1 M, \beta \in \Lambda_p^1 M )\)</span>.<br />
Odwzorowanie <span class="math inline">\(\land : T_p^* M \times T_p^* M \to \Lambda^2(\theta), \theta\in M\)</span> nazywamy iloczynem zewnętrznym i definiujemy tak: <span class="math display">\[\left&lt;\alpha\land\beta;v,w \right&gt; \overset{\text{def}}{=} \begin{vmatrix}\alpha(v)&amp;\beta(v)\\ \alpha(w)&amp;\beta(w) \end{vmatrix}
        .\]</span></p>
<p>Niech <span class="math inline">\(\alpha = 7dx + 4dy, \beta = 2dx+3dy, v = 1 \frac{\partial }{\partial x} + 4 \frac{\partial }{\partial y} , w = 2 \frac{\partial }{\partial x} + \frac{\partial }{\partial y}\)</span><br />
Wtedy <span class="math display">\[\left&lt;\alpha\land\beta;v,w \right&gt; = \begin{vmatrix}\alpha(v)&amp;\beta(v)\\ \alpha(w)&amp;\beta(w)\end{vmatrix} = \begin{vmatrix}7\cdot 1+4\cdot 4&amp; 2\cdot 1+3\cdot 4 \\ 7\cdot 2+4 &amp; 2\cdot 2+3 \end{vmatrix}
        .\]</span></p>
<p>Obserwacja: <span class="math inline">\(\alpha\land\beta = -\beta\land\alpha\)</span>. Tzn. <span class="math inline">\(\alpha\land\alpha = 0\)</span>. Ważny przykład:</p>
<p><span class="math display">\[\begin{aligned}
            &amp;\alpha = A_xdx + A_ydy + A_zdz, \beta = B_xdx + B_ydy + B_zdz\\
            &amp; \alpha\land\beta = \left( A_xdx + A_ydy + A_zdz \right) \land \left( B_xdx + B_ydy + B_zdz \right) = \\
            &amp;= \left( A_xdx+A_ydy+A_zdz \right) \land B_xdx + \left( A_xdx+A_ydy+A_zdz \right) \land B_ydy + \\
            &amp;+ \left( A_xdx+A_ydy+A_zdz \right) \land B_zdz = A_yB_xdy\land dx + A_zB_x dz\land dx +   \\
            &amp;+ A_xB_y dx\land dy + A_zB_ydz\land dy + A_xB_zdx\land dz + A_yB_zdy\land dz = \\
            &amp;=  \left( A_xB_y - A_yB_x \right) dx\land dy + \left( A_yB_z - A_zB_y \right) dy\land dz + \left( A_zB_x - A_xB_z \right) dz\land dx\\
        .\end{aligned}\]</span></p>
<p>Potrzebujemy jeszcze jednego klocka, żeby zobaczyć gdzie tu siedzi fizyka.</p>
<p>Odwzorowanie <span class="math inline">\(d: \Lambda^k(M)\to\Lambda^{k+1}(M)\)</span> nazywamy różniczką zewnętrzną (ewentualnie pochodną zewnętrzną) i definiujemy następująco: <span class="math display">\[\begin{aligned}
            &amp;df = \frac{\partial f}{\partial x^1} dx^1 + \frac{\partial f}{\partial x^2} dx^2 + \ldots + \frac{\partial f}{\partial x^n} dx^n, f: \theta \to \mathbb{R}\\
            &amp;(\text{funkcje nazywamy zero-formami }f\in Lambda^0(\theta))\\
            &amp;\omega\in\Lambda^p(\theta), \eta\in \Lambda^L(\theta) \implies d(\omega\land\eta) = d\omega\land\eta + (-1)^p\omega\land d\eta \\
            &amp;dd\omega = 0, \omega\in\Lambda^k(\theta)
        .\end{aligned}\]</span></p>
<p><span class="math inline">\(f(r,\theta,\varphi)\)</span> - funkcja z <span class="math inline">\(\mathbb{R}^3\)</span> w <span class="math inline">\(\mathbb{R}^1\)</span>.<br />
<span class="math display">\[\begin{aligned}
            &amp;df = \frac{\partial f}{\partial r} dr + \frac{\partial f}{\partial \theta} d\theta + \frac{\partial f}{\partial \varphi} d\varphi\\
            &amp;\alpha = 7x^2y dx\\
            &amp;d\alpha = d(7x^2y)\land dx = \left( \frac{\partial }{\partial x} (7x^2y)dx + \frac{\partial }{\partial y} (7x^2y)dy \right) \land dx = 7x^2dy\land dx
        .\end{aligned}\]</span></p>
<p>Niech <span class="math inline">\(F = -E_xdt\land dx - E_xdt\land dy - E_z dt\land dz + B_xdy\land dz + B_y dz\land dx + B_zdx \land dy\)</span> <span class="math display">\[\begin{aligned}
            &amp;dF = \left( -\frac{\partial E_x}{\partial y} dy - \frac{\partial E_x}{\partial z} dz \right) \land dt\land dx +\\
            &amp;+ \left( - \frac{\partial E_y}{\partial x}dx - \frac{\partial E_y}{\partial z}dz  \right)\land dt \land dy + \\
            &amp;+ \left( -\frac{\partial E_z}{\partial x} dx - \frac{\partial E_z}{\partial y} dy \right)\land dt \land dz + \\
            &amp;+ \left( \frac{\partial B_x}{\partial t} dt + \frac{\partial B_x}{\partial x} dx \right) \land dy \land dz + \\
            &amp;+ \left( \frac{\partial B_y}{\partial t} dt + \frac{\partial B_y}{\partial y} dy \right) \land dz \land dx + \\
            &amp;+ \left( \frac{\partial B_z}{\partial t} dt + \frac{\partial B_z}{\partial z} dz \right) \land dx \land dy = \\
            &amp;= \underbrace{\left( -\frac{\partial E_x}{\partial y} + \frac{\partial E_y}{\partial x} + \frac{\partial B_z}{\partial t}  \right)} dt\land dx\land dy + \\
            &amp;+ \underbrace{\left( -\frac{\partial E_y}{\partial z} + \frac{\partial E_z}{\partial y} + \frac{\partial B_x}{\partial t}  \right)} dt\land dy\land dz + \\
            &amp;+ \underbrace{\left( -\frac{\partial E_x}{\partial z} + \frac{\partial E_z}{\partial x} - \frac{\partial B_y}{\partial t}  \right)}_{rot E - \frac{\partial B}{\partial t} } dt\land dx\land dz + \\
            &amp;+ \underbrace{\left( \frac{\partial B_x}{\partial x} + \frac{\partial B_y}{\partial y} + \frac{\partial B_z}{\partial z}  \right)}_{\nabla B} dx\land dy\land dz
        .\end{aligned}\]</span> I to są równania Maxwella (przynajmniej pierwsza ich część) <span class="math inline">\(dF = 0\)</span></p>
<h1 id="wykład-28.05.2019">Wykład (28.05.2019)</h1>
<p>Niech <span class="math inline">\(\alpha_1,\alpha_2,\ldots,\alpha_k\in T_p^*M\in\Lambda&#39;(M)\)</span>, wówczas <span class="math inline">\(\alpha_1\land\alpha_2\land\ldots\land\alpha_k\in \Lambda^k(M)\)</span> i dla <span class="math inline">\(v_1,v_2,\ldots,v_k\in T_p^*M\)</span>, <span class="math display">\[\left&lt;\alpha_1\land\alpha_2\land\ldots\land\alpha_k; v_1,v_2,\ldots,v_k \right&gt; \overset{\text{def}}{=}  \begin{bmatrix} \alpha_1(v_1)\alpha_2(v_1)\ldots\alpha_k(v_1)\\ \vdots \\ \alpha_1(v_k)\alpha_2(v_k)\ldots\alpha_k(v_k) \end{bmatrix}
    .\]</span></p>
<p>Uwagi do operatora <span class="math inline">\(d\)</span> (<span class="math inline">\(dd=0\)</span> ):<br />
Niech <span class="math inline">\(M = \mathbb{R}^3, f:\mathbb{R}^3\to\mathbb{R}^1\in\Lambda^0(M)\)</span></p>
<p><span class="math display">\[\begin{aligned}
&amp;df = \frac{\partial f}{\partial x} dx+ \frac{\partial f}{\partial y} dy + \frac{\partial f}{\partial z} dz\\
&amp;ddf = d\left( \frac{\partial f}{\partial x}  \right) \land dx + d\left( \frac{\partial f}{\partial y}  \right) \land dy + d\left( \frac{\partial f}{\partial z}  \right) \land dz = \\
&amp;= \left( \frac{\partial^2 f}{\partial x^2} dx + \frac{\partial ^2 f}{\partial y \partial x} dy + \frac{\partial ^2 f}{\partial z \partial x} dz \right) \land dx \left( \frac{\partial ^2 f}{\partial x \partial z} dx + \frac{\partial ^2 f}{\partial y \partial z} dy \right) \land dy \\
&amp;\left( \frac{\partial ^2 f}{\partial x \partial z} dx + \frac{\partial ^2 f}{\partial y \partial z} dz \right) \land dz \\
&amp;= \left( \frac{\partial ^2 f}{\partial y\partial x} - \frac{\partial ^2f}{\partial x\partial y}  \right) dy\land dx + \left( \frac{\partial ^2f}{\partial z\partial y} - \frac{\partial ^2f}{\partial y\partial z}  \right) dz\land dy +\\
&amp;+ \left( \frac{\partial ^2f}{\partial z\partial x} -\frac{\partial ^2f}{\partial x\partial z}  \right)dz\land dx = 0
.\end{aligned}\]</span> Niech <span class="math inline">\(\alpha = A_x dx + A_ydy + A_zdz\)</span><br />
<span class="math display">\[\begin{aligned}
     &amp;d\alpha = \left( \frac{\partial A_x}{\partial y} - \frac{\partial A_y}{\partial x}  \right) dy\land dx + \left( \frac{\partial A_z}{\partial y} - \frac{\partial y}{\partial z}  \right) dz\land dy +\\
     &amp;+ \left( \frac{\partial A_z}{\partial x} - \frac{\partial A_x}{\partial z}  \right)dz\land dx  \\
     &amp;dd\alpha = \left( \pm \left( \frac{\partial ^2A_x}{\partial z\partial y} - \frac{\partial ^2A_x}{\partial z \partial x}  \right) \pm \left( \frac{\partial ^2A_z}{\partial x\partial y}  - \frac{\partial ^2A_y}{\partial x\partial z}\right) \pm \left( \frac{\partial ^2A_z}{\partial y\partial x}  - \frac{\partial ^2A_x}{\partial y\partial z} \right)  \right)dx\land dy\land dz  \\
.\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
    &amp;\beta = A_xdy\land dz + A_ydx\land dz + A_zdy\land dz\\
    &amp;d\beta = \left(  \right) dx\land dy\land dz\\
    &amp;dd\beta = 0
.\end{aligned}\]</span> Niech <span class="math inline">\(M = \mathbb{R}^4\)</span>, <span class="math inline">\(A = \phi dt + A_x dx + A_y dy + A_z dz\)</span>. <span class="math display">\[\begin{aligned}
    &amp;dA = \left( \underbrace{\frac{\partial \phi}{\partial x} - \frac{\partial A_x}{\partial t}}_{E_x}  \right) dx\land dt + \left( \underbrace{\frac{\partial \phi}{\partial y} - \frac{\partial A_y}{\partial t}}_{E_y}  \right) dy\land dt + \left( \underbrace{\frac{\partial \phi}{\partial z}  - \frac{\partial A_z}{\partial t}}_{E_z} \right) dz\land dt+\\
    &amp;\left( \underbrace{\frac{\partial A_y}{\partial x} - \frac{\partial A_x}{\partial y} }_{B_z} \right) dx\land dy + \left( \underbrace{\frac{\partial A_z}{\partial y} - \frac{\partial A_y}{\partial z}}_{B_x}  \right) dy\land dz + \left( \underbrace{\frac{\partial A_x}{\partial z} - \frac{\partial A_z}{\partial x}}_{B_y}  \right) dz\land dx\\
    &amp;ddA = 0
.\end{aligned}\]</span> niech <span class="math inline">\(dA = F\)</span> <span class="math display">\[dF = 0
.\]</span></p>
<p>Pytanie: niech <span class="math inline">\(M\)</span> - rozmaitość wymiaru <span class="math inline">\(3\)</span> (bo mamy bijekcję między <span class="math inline">\(\theta\in M\)</span> i <span class="math inline">\(\mathbb{R}^3\)</span> ). Czy istnieje <span class="math inline">\(\Lambda^4(M)\)</span>?</p>
<p>niech <span class="math inline">\(M = \mathbb{R}^3\)</span> <span class="math display">\[\begin{aligned}
    &amp;\Lambda^0(M) &amp;&amp;f: \mathbb{R}^3\to M &amp;\dim \Lambda^0(M) = 1\\
    &amp;\Lambda^1(M) &amp;&amp;\alpha = A_xdx + A_ydy + A_zdz &amp;\Lambda^1(\eta) = \underbrace{\left&lt;dx,dy,dz \right&gt;}_{3}\\
    &amp;\Lambda^2(M) &amp;&amp;\beta = A_zdx\land dy + A_ydz\land dx + A_z dy \land dz &amp; \Lambda^2(M) = \underbrace{\left&lt;dx\land dy, dz\land dx, dy\land dz \right&gt;}_{3}\\
    &amp;\Lambda^3(\eta) &amp;&amp;\gamma = f dx\land dy \land dz &amp; \Lambda^3(M) = \underbrace{\left&lt;dx\land dy\land dz \right&gt;}_{1}\\\end{aligned}\]</span></p>
<p>Niech <span class="math inline">\(M = \mathbb{R}^4\)</span>. <span class="math display">\[\begin{aligned}
    &amp;\Lambda^0(M) &amp;&amp;f(t,x,y,z) \to \mathbb{R} &amp;&amp;\dim \Lambda^0(M) =1\\
    &amp;\Lambda^1(M) &amp;&amp;\alpha = A_t dt + A_xdx + A_ydy + A_zdz &amp;&amp;\dim \Lambda^1(M) = 4\\
    &amp;\Lambda^2(M) &amp;&amp;\beta = A_1 dt\land dx+A_2dt\land dy + A_3dt\land dz + B_1dy\land dx + B_2 dz\land dx + C_1dz\land dy &amp;&amp;\dim \Lambda^2(M) = 6\\
    &amp;\Lambda^3(M): &amp;&amp;\gamma = C_1dy\land dt\land dx + C_2dz\land dt\land dx + D_1dz\land dt\land dy + E_1dx\land dy\land dz &amp;&amp;\dim \Lambda^3(M) = 4\\
    &amp;\Lambda^4(M) &amp;&amp;\delta = gdt\land dx\land dy\land dz &amp;&amp; \dim\Lambda^4(M) = 1
.\end{aligned}\]</span></p>
<h2 id="pchnięcia-i-cofnięcia">Pchnięcia i cofnięcia</h2>
<p>Niech <span class="math inline">\(M,N\)</span> - rozmaitości <span class="math inline">\(\dim M = n, \dim N = k\)</span> i niech <span class="math inline">\(h: M\to N.\)</span> (<span class="math inline">\(h\)</span> nie musi być bijekcją !!!)</p>
<p>Niech <span class="math inline">\(p\in M\)</span>. Pchnięciem punktu <span class="math inline">\(p\)</span> w odwzorowaniu <span class="math inline">\(h\)</span> nazywamy punkt <span class="math inline">\(h_*(p) \overset{\text{def}}{=} h(p)\)</span></p>
<p>Niech <span class="math inline">\(M = \mathbb{R}^2\)</span>, <span class="math inline">\(N = \mathbb{R}\)</span>, <span class="math inline">\(h(x,y) = x+y, h: \mathbb{R}^2\to \mathbb{R}\)</span>.<br />
<span class="math inline">\(p = \begin{bmatrix} 1\\2 \end{bmatrix} , h_*(p) = 3\)</span><br />
<span class="math inline">\(M = \mathbb{R}^1\)</span>, <span class="math inline">\(N = \mathbb{R}^3\)</span>, <span class="math inline">\(h(t) = \begin{bmatrix} \cos t \\ \sin t \\ t \end{bmatrix}, p = \frac{\pi}{2}\)</span>.<br />
<span class="math inline">\(h_x(\frac{\pi}{2}) = \begin{bmatrix} \cos \frac{\pi}{2}\\ \sin \frac{\pi}{2} \\ \frac{\pi}{2} \end{bmatrix}\)</span></p>
<p><span id="fig:fig_54" label="fig:fig_54">[fig:fig_54]</span></p>
<p>Niech <span class="math inline">\(\sigma(t)\)</span> - krzywa na <span class="math inline">\(M\)</span>. Pchnięciem krzywej <span class="math inline">\(\sigma\)</span> w odwzorowaniu <span class="math inline">\(h\)</span> nazywamy krzywą <span class="math inline">\(h_*(\sigma(t)) \overset{\text{def}}{=} h(\sigma(t))\)</span></p>
<p><span id="fig:" label="fig:">[fig:]</span></p>
<p>Niech <span class="math inline">\(f: N\to \mathbb{R}^2\)</span>. Cofnięciem funkcji <span class="math inline">\(f\)</span> w odwzorowaniu <span class="math inline">\(h\)</span> nazywamy funkcję <span class="math display">\[h^*f(p) = f(h(p))
.\]</span></p>
<p><span class="math inline">\(M = \mathbb{R}^2, N = \mathbb{R}, f: N\to \mathbb{R}^2, f(t) = \begin{bmatrix} 2t\\t \end{bmatrix}, h(x,y) = x+y\)</span>. <span class="math display">\[h^*f(x,y) = f(h(x,y)) = \begin{bmatrix} 2(x+y)\\x+y \end{bmatrix}
    .\]</span></p>
<p>Pchnięciem wektora <span class="math inline">\(V\)</span> w odwzorowaniu <span class="math inline">\(h\)</span> nazywamy wektor <span class="math display">\[h_* V = \left[ h(\sigma) \right], h_*v\in T_{h(p)}N
    .\]</span></p>
<p>Niech <span class="math inline">\(M = \mathbb{R}^2, N = \mathbb{R}, h(x,y) = x+2y, v = 2 \frac{\partial }{\partial x} + 3 \frac{\partial }{\partial y}\)</span>. Co to jest <span class="math inline">\(h_*v\)</span>?</p>
<p><span class="math inline">\(p = (1,2) = (\varphi^1(p),\varphi^1(p))\)</span></p>
<p><span class="math display">\[\begin{aligned}
        &amp;\sigma(t): \frac{d}{dt}(\varphi(\sigma(t)))\vert_{t=0}\\
        &amp;\varphi(\sigma(t)) = \begin{bmatrix} 2t+1\\3t+2 \end{bmatrix}\\
        &amp;h[\sigma(t)] = 2t+1 + 2(3t+2)\\
        &amp;h[\sigma(t)] = 8t+5\\
        &amp;\left[ h[\sigma(t)] \right] = 8 \frac{\partial }{\partial t} \in t_sN
    .\end{aligned}\]</span> <span class="math inline">\(\dim M = n\)</span>, <span class="math inline">\(\varphi(\sigma(t)) = \left( \varphi^1(\sigma(t)), \varphi^2(\sigma(t)),\ldots,\varphi^n(\sigma(t)) \right), v\in T_pM\)</span>.<br />
<span class="math display">\[v = \frac{\partial \varphi^1(\sigma(t))}{\partial t} \vert_{t=0} \frac{\partial }{\partial x^1} + \frac{\partial \varphi^2(\sigma(t))}{\partial t} \vert_{t=0} \frac{\partial }{\partial x^2} \ldots \frac{\partial \varphi^n(\sigma(t))}{\partial t} \vert_{t=0} \frac{\partial }{\partial x^n}
    .\]</span> <span class="math display">\[\frac{d(\varphi\circ h(\sigma(t)))}{dt}\vert_{t=0} = \frac{d}{dt}\left( \psi \circ h \circ \varphi^{-1} \sigma \right)_{t=0} = \frac{d}{dt}\left( \tilde h \circ \tilde \sigma(t) \right)
.\]</span> <span class="math display">\[= \frac{d}{dt}\tilde h\left( \tilde \sigma_1(t), \tilde \sigma_2(t), \ldots, \tilde \sigma^n(t) \right)_{t=0} = \tilde h&#39;_{\tilde \sigma(0)} \frac{d\tilde \sigma}{dt}_{t=0} = \tilde h&#39;\cdot v
.\]</span></p>
<p>Czyli ostatecznie <span class="math inline">\(v = \begin{bmatrix} 2\\3 \end{bmatrix}_{\frac{\partial }{\partial x}, \frac{\partial }{\partial y} }, \tilde h(x,y) = x+2y \to \tilde h(x,y) = \left[ 1,2 \right]\)</span>.<br />
<span class="math display">\[h_* v = \left[ 1,2 \right] \begin{bmatrix} 2\\3 \end{bmatrix} = 2\cdot 1+ 6 = 8 \frac{\partial }{\partial t}
.\]</span></p>
<p>Niech <span class="math inline">\(\alpha\in \Lambda^1(?)\)</span> - pytanie: czy formy się pcha, czy cofa?</p>
<p><span id="fig:fig_56" label="fig:fig_56">[fig:fig_56]</span></p>
<h1 id="wykład-31.05.2019">Wykład (31.05.2019)</h1>
<p>(na pchnięcie wektora)<br />
Niech <span class="math inline">\(M = \mathbb{R}^1, N=\mathbb{R}^3, h(t) = \begin{bmatrix} f(t)\\g(t)\\r(t) \end{bmatrix}\)</span><br />
Niech <span class="math inline">\(p\in \mathbb{R}^1\)</span>, niech <span class="math inline">\(v\in T_pM, v = a \frac{\partial }{\partial t}\)</span>. <span class="math inline">\(v = [\sigma], \tilde \sigma(t) = at+p\)</span>, <span class="math inline">\(\sigma(c) = p, \frac{d \tilde \sigma(t)}{dt}\vert_{t=0} = a\)</span>.<br />
<span class="math display">\[h_x \sigma = \begin{bmatrix} f(at+p)\\g(at+p)\\r(at+p) \end{bmatrix}, h_x v = [h_x \sigma], \frac{d}{dt}(\tilde h_x \sigma)\vert_{t=0}
        .\]</span> <span class="math display">\[h_xv = \begin{bmatrix} af&#39;(p)\\ ag&#39;(p) \\ ar&#39;(p) \end{bmatrix} = af&#39;(p) \frac{\partial }{\partial x} + ag&#39;(p) \frac{\partial }{\partial y} + ar&#39;(p) \frac{\partial }{\partial z}
        .\]</span></p>
<p><span id="fig:fig_57" label="fig:fig_57">[fig:fig_57]</span></p>
<p>Niech <span class="math inline">\(M, N\)</span> - rozmaitości, <span class="math inline">\(h: M\to N\)</span> i niech <span class="math inline">\(p\in M, \alpha\in T^*_{h(p)}N\)</span>.<br />
Cofnięciem formy <span class="math inline">\(\alpha\)</span> w odwzorowaniu <span class="math inline">\(h\)</span> nazywamy formę <span class="math inline">\(h^*\alpha \in T_pM\)</span>, taką, że <span class="math inline">\(\left&lt;h^*\alpha,v \right&gt; = \left&lt;\alpha,hv \right&gt;\underset{v\in T_pM}{\forall}\)</span> i ciągła. Jeżeli <span class="math inline">\(\alpha_1,\alpha_2,\ldots,\alpha_k \in \Lambda^1(N)\)</span> i <span class="math inline">\(v_1,\ldots,v_k\in T_p(M)\)</span>, to <span class="math display">\[h^*(\alpha_1\land\ldots\land\alpha_k), v_1,\ldots,v_k \overset{\text{def}}{=} \begin{bmatrix} \left&lt;h^*\alpha_1,v_1 \right&gt;&amp;\left&lt;h^*\alpha_2,v_1 \right&gt;&amp;\ldots&amp;\left&lt;h^*\alpha_k,v_1 \right&gt; \\
            \vdots &amp;&amp;&amp;\\
            \left&lt;h^*\alpha_k,v_k \right&gt;&amp;\left&lt;h^*\alpha_k, v_k \right&gt;&amp;\ldots&amp;\left&lt;h^*\alpha_k,v_k \right&gt;
        \end{bmatrix}
    .\]</span> Czyli <span class="math display">\[h^*(\alpha_1\land\ldots\land\alpha_k) = (h^*\alpha_1)\land(h^*\alpha_2)\land\ldots\land h^*(\alpha_k)
    .\]</span></p>
<p><span id="fig:fig_58" label="fig:fig_58">[fig:fig_58]</span></p>
<p><span id="fig:fig_59" label="fig:fig_59">[fig:fig_59]</span></p>
<p>(wstępny)<br />
Niech <span class="math inline">\(\alpha = 3(x^2+y^2)dx - 2xdy + 2z^2dz, \alpha\in \Lambda^1(N)\)</span> (jednoformy nad <span class="math inline">\(N\)</span>, <span class="math inline">\(\dim N = 3\)</span>, chociaż można dać więcej jak się chce).<br />
<span class="math inline">\(h(t) = \begin{bmatrix} \sin(t)\\ \cos(t)\\ t \end{bmatrix}\)</span>. Czym jest <span class="math inline">\(h^*\alpha\)</span>?<br />
<span class="math display">\[\left&lt;h^*\alpha,v \right&gt; = \left&lt;\alpha,h_xv \right&gt;
        .\]</span> Niech <span class="math inline">\(v\in T_pM\)</span> i <span class="math inline">\(v = a \frac{\partial }{\partial t}\)</span>. Zatem <span class="math inline">\(h_x v = a \cos(p) \frac{\partial }{\partial x} - a \sin(p) \frac{\partial }{\partial y} + a\cdot 1 \frac{\partial }{\partial t}\)</span>. <span class="math display">\[\begin{aligned}
            &amp;\left&lt;\alpha,h_*v \right&gt; = \left&lt; 3 \left( \sin^2(t) + \cos^2(t)\right) dx - 2\left( \sin(t) \right) dy + 2\left( t^2 \right) dz, h_xv\right&gt; =\\
            &amp;= \left&lt;3dx - 2\sin(t)dy + 2t&#39;dz, a \cos(t) \frac{\partial }{\partial x} - a \sin(t) \frac{\partial }{\partial y} + a\cdot 1 \frac{\partial }{\partial z}  \right&gt;_{t=p} \\
            &amp;= 3a \cos(t) + 2a \sin^2(t) + at^2 \vert_{t=p} = \\
            &amp;= \left&lt; \left(3\cos(t)dt + 2a\sin^2(t) + at^2\right)\vert_{t=p}, a \frac{\partial }{\partial t}  \right&gt; = \\
            &amp; \text{czyli } h^*\alpha = \left( 3\cos(t) + 2 \sin^2(t) + t^2 \right) dt \\
        .\end{aligned}\]</span> Na skróty!<br />
<span class="math display">\[\begin{aligned}
            &amp;x = \sin(t) &amp;&amp;dx = \cos(t)dt\\
            &amp;y = \cos(t) &amp;&amp;dy = -\sin(t) dt\\
            &amp;z = t &amp;&amp;dz = dt
        .\end{aligned}\]</span> Zatem <span class="math display">\[\begin{aligned}
             h^*\alpha &amp;= 3\left( \sin^2(t) + \cos^2(t) \right) \cos(t)dt - 2\sin(t) \left( -\sin t dt \right) + 2t^2dt \\
                       &amp;= \left( 3\cos(t) + 2\sin^2(t) + 2t^2 \right) dt
        .\end{aligned}\]</span></p>
<p>Niech <span class="math inline">\(M = \mathbb{R}^4, N = \mathbb{R}^4\)</span>.<br />
<span class="math display">\[\begin{aligned}
            &amp;\gamma = \frac{1}{\sqrt{1-v^2} },\\
            &amp;c = 1\\
            h:\quad &amp;t = \gamma(t&#39;-vx&#39;)\\
               &amp;x = \gamma(x&#39;-vt&#39;)\\
               &amp;y = y&#39;\\
               &amp;z = z&#39;
        .\end{aligned}\]</span> Czyli <span class="math display">\[\begin{aligned}
            &amp;dt = \gamma(dt&#39; - v dx&#39;)\\
            &amp;dx = \gamma(dx&#39; - v dt&#39;)\\
            &amp;dy = dy&#39;\\
            &amp;dz = dz&#39;
        .\end{aligned}\]</span> Chcemy cofnąć naszą formę. Na fizyce nie używamy słowa <em>cofnięte</em>. <span class="math display">\[\begin{aligned}
            &amp;F&#39; = -E_x\left( \gamma\left( dt&#39; - vdx&#39; \right)  \right) \land \gamma \left( dx&#39;-vdt&#39; \right) - E_y \gamma\left( dt&#39; - vdx&#39; \right) \land dy&#39;=\\
            &amp;= -E_x \gamma^2 \left( 1 - v^2 \right) dt&#39; \land dx&#39; - E_y\gamma dt&#39;\land dy&#39; + E_y\gamma v dx&#39;\land dy&#39;= \\
            &amp;= -E_x \frac{1}{1-v^2}\left( 1-v^2 \right) dt&#39;\land dx&#39; - E_y \gamma dt&#39;\land dy&#39; + \gamma v E_x dx&#39;\land dy&#39; \\
            &amp;F&#39; = -E&#39;_x dt&#39;\land dx&#39; - E&#39;_y dt&#39; \land dy&#39; + B&#39;_z dx&#39;\land dy&#39; \\
        .\end{aligned}\]</span> Czyli <span class="math display">\[\begin{aligned}
            &amp;E&#39;_x = E_x\\
            &amp;E&#39;_y = \gamma E_y\\
            &amp;B&#39;_z = \gamma v E_y
        .\end{aligned}\]</span></p>
<p><span id="fig:fig_60" label="fig:fig_60">[fig:fig_60]</span></p>
<p>Obserwacja: Niech <span class="math inline">\(\alpha \in \Lambda^1(N)\)</span>, <span class="math inline">\(\dim N = k\)</span>, niech <span class="math inline">\(M\)</span> - rozmaitość, <span class="math inline">\(\dim M = n\)</span> i <span class="math inline">\(h: M\to N\)</span>. Wówczas <span class="math display">\[h^*f\in \Lambda^0(M)
    .\]</span> Oraz <span class="math display">\[d(h^*f) = h^*(df)
    .\]</span></p>
<p>Skoro <span class="math inline">\(f\in \Lambda^0(N)\)</span>, to <span class="math inline">\(f(x^1,x^2,\ldots,x^k)\)</span>,<br />
<span class="math inline">\(df = \frac{\partial f}{\partial x^1} dx^1 + \frac{\partial f}{\partial x^2} dx^2 + \ldots + \frac{\partial f}{\partial x^k} x^k\)</span>. <span class="math display">\[\left&lt;h^*(df),v \right&gt; = \left&lt;df,h_xv \right&gt;, v\in T^pM
        .\]</span> Niech <span class="math inline">\(V\in T_pM\)</span>.<br />
<span class="math display">\[\tilde h(t_1,\ldots,t_n) = \begin{bmatrix} h_1(t_1,\ldots,t_n)\\ \vdots \\ h_k(t_1,\ldots,t_n) \end{bmatrix}
        .\]</span> Jeżeli <span class="math inline">\(v = a_1 \frac{\partial }{\partial t^1} + a_2 \frac{\partial }{\partial t^2} + \ldots + a_n \frac{\partial }{\partial t_n}\)</span>, to <span class="math inline">\(h_*v = \left( \begin{bmatrix} h&#39; \end{bmatrix} \begin{bmatrix} a_1\\ \vdots \\ a_n \end{bmatrix}  \right)_{\frac{\partial }{\partial x^1} , \ldots, \frac{\partial }{\partial x^k} }\)</span>.<br />
<span class="math display">\[\begin{aligned}
            &amp;h_xv = \left( \begin{bmatrix} \frac{\partial h_1}{\partial t^1} &amp; \ldots &amp; \frac{\partial h_k}{\partial t^k} \\ \vdots \\ \frac{\partial h_k}{\partial t^1} &amp; \ldots &amp; \frac{\partial h_k}{\partial t^k}  \end{bmatrix} \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix}  \right)_{\frac{\partial }{\partial x^1} ,\ldots, \frac{\partial }{\partial x^k} } = \left( \frac{\partial h_1}{\partial t^1} a_1 + \ldots + \frac{\partial h_1}{\partial t^n} a_n \right) \frac{\partial }{\partial x^1} +\\
&amp;+ \ldots + \left( \frac{\partial h_k}{\partial t^1} a_1 + \ldots + \frac{\partial h_k}{\partial t^n} a_n \right) \frac{\partial }{\partial x^k}
        .\end{aligned}\]</span> Dalej <span class="math display">\[\begin{aligned}
            &amp;\left&lt;df,h_*v \right&gt; = \frac{\partial f_1}{\partial x^1} \left( \frac{\partial h_1}{\partial t^1} a_1 + \ldots + \frac{\partial h_1}{\partial t^n} a_n \right)  + \ldots + \frac{\partial f}{\partial x^k} \left( \frac{\partial h_k}{\partial t^1} a_n + \ldots + \frac{\partial h_k}{\partial t^n} a_n \right) =\\
            &amp;= \left&lt;df(h_1(t_1,\ldots,t_n),h_2(t_1,\ldots,t_n),\ldots,h_k(t_1,\ldots,t_n)), a_1 \frac{\partial }{\partial t^1} + \ldots + a_n \frac{\partial }{\partial t^n}  \right&gt; \\
        .\end{aligned}\]</span></p>
<h1 id="wykład-04.06.2019">Wykład (04.06.2019)</h1>
<p>W ostatnim odcinku:<br />
<span class="math inline">\(M, N\)</span> - rozmaitości, <span class="math inline">\(\dim M = n, \dim N = k, h: M\to N\)</span>, <span class="math inline">\(\left&lt;h^*\alpha,v \right&gt; = \left&lt;\alpha,h_xv \right&gt;\)</span> i ogólnie, jeżeli <span class="math inline">\(\alpha_1,\ldots,\alpha_k\in \Lambda^1(N)\)</span> to <span class="math inline">\(\left&lt; h^*(\alpha_1\land \alpha_2\land\ldots\land\alpha_k), v_1,\ldots,v_n\right&gt; = \left&lt;\alpha_1\land\ldots\land\alpha_k,h_xv_1,\ldots,h_xv_n \right&gt;\)</span>.</p>
<p>Niech <span class="math inline">\(N = \mathbb{R}^2\)</span> i <span class="math inline">\(M = \mathbb{R}^1\)</span>, <span class="math inline">\(\alpha = 7dx\land dy\in \Lambda^2(N)\)</span>,<br />
<span class="math display">\[h(t) = \begin{bmatrix} 2t\\3t \end{bmatrix} \to (x = 2t, y = 3t \implies dx = 2dt, dy = 3dt)
        .\]</span> <span class="math display">\[h^*\alpha = 7\cdot 2dt\land 3dt = h^*\alpha = 0
        .\]</span></p>
<p>Ostatino chcieliśmy pokazać, że <span class="math inline">\(d(h^*f) = h^*(df)\)</span>. To jest istotne w kontekście tej dwuformy przekształcenia transormacji Lorentza co była ostatnio. (<span class="math inline">\(d(h^*F) = 0 \implies dF = 0, h^*F \overset{h}{\to} F\)</span>).<br />
Wzięliśmy sobie <span class="math inline">\(f: N\to \mathbb{R}: f(x_1,\ldots,x_k)\)</span>. Potem mieliśmy <span class="math inline">\(h: M\to N: h(t_1,\ldots,t_n) = \begin{bmatrix} h^1(t_1,\ldots,t_n)\\ \vdots \\ h^k(t_1,\ldots,t_n) \end{bmatrix}\)</span> i chcieliśmy pokazać, że <span class="math inline">\(h^*(df) = d(h^*f)\)</span>.<br />
Wiemy, że <span class="math inline">\(\left&lt;h^*(df),v \right&gt; = \left&lt;df,h_*v \right&gt; (v\in T_pM: v = a_1 \frac{\partial }{\partial t^1} + \ldots + a_n \frac{\partial }{\partial t^n} )\)</span>. Przepchnięcie wektorka <span class="math inline">\(h_*v = \left(\left[ h&#39; \right] \begin{bmatrix} a_1\\ \vdots \\ a_n \end{bmatrix}\right)_{\frac{\partial }{\partial x^1} ,\ldots,\frac{\partial }{\partial x^k} } = \begin{bmatrix} \frac{\partial h^1}{\partial t^1} &amp; \ldots &amp; \frac{\partial h^1}{\partial t^n} \\ \vdots &amp; \ddots &amp; \\ \frac{\partial h^k}{\partial t^1} &amp; \ldots &amp; \frac{\partial h^k}{\partial t^n}  \end{bmatrix} \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix} = \left( a_1 \frac{\partial h^1}{\partial t^1} + \ldots + a_n \frac{\partial h^1}{\partial t^n}  \right) \frac{\partial }{\partial x^1} + \ldots + \left( a_1 \frac{\partial h^k}{\partial t^1} + \ldots + a_n \frac{\partial h^k}{\partial t^n} \right)\frac{\partial }{\partial x^k}\)</span>.<br />
<span class="math display">\[df = \frac{\partial f}{\partial x^1} dx^1 + \ldots + \frac{\partial f}{\partial x^k} dx^k
.\]</span></p>
<p><span class="math display">\[\begin{aligned}
&amp;\left&lt;df, h_xv \right&gt; = \frac{\partial f}{\partial x^1} \frac{\partial h^1}{\partial t^1} a_1 + \frac{\partial f}{\partial x^1} \frac{\partial h^1}{\partial t^2} a_2 + \ldots + \frac{\partial f}{\partial x^1} \frac{\partial h^1}{\partial t^n} a_n + \ldots + \frac{\partial f}{\partial x^1} \frac{\partial h^k}{\partial t^1} a_1 + \ldots + \frac{\partial f}{\partial x^k} \frac{\partial h^k}{\partial t^n} a_n = \\
&amp;= a_1 \left( \frac{\partial f}{\partial x^1} \frac{\partial h^1}{\partial t^1} + \frac{\partial f}{\partial x^2} \frac{\partial h^2}{\partial t^1} + \ldots + \frac{\partial f}{\partial x^k} \frac{\partial h^k}{\partial t^1} \right) + \ldots + a_n \left( \frac{\partial f}{\partial x^1} \frac{\partial h^1}{\partial t^n} + \frac{\partial f}{\partial x^2} \frac{\partial h^2}{\partial x^n} + \ldots + \frac{\partial f}{\partial x^k} \frac{\partial h^k}{\partial t^n}  \right) = \\
&amp;= \left&lt; ?, a_1 \frac{\partial }{\partial t^1} + a_2 \frac{\partial }{\partial t^2} + \ldots + a_n \frac{\partial }{\partial t^n}  \right&gt; \\
&amp;= \left&lt; \left( \frac{\partial f}{\partial x^1} \frac{\partial h^1}{\partial t^1} + \frac{\partial f}{\partial x^2} \frac{\partial h^2}{\partial t^1} + \ldots + \frac{\partial f}{\partial x^k} \frac{\partial h^k}{\partial t^1}\right)dt^1 + \ldots + \left( \frac{\partial f}{\partial x^1} \frac{\partial h^1}{\partial t^n} + \ldots + \frac{\partial f}{\partial x^k} \frac{\partial h^k}{\partial t^n}  \right) dt^n , a_1 \frac{\partial }{\partial t^1} , \ldots, a_n \frac{\partial }{\partial t^n}  \right&gt; = \\
&amp;= \left&lt; \underbrace{f\left(h^1(t^1,\ldots, t^n), h^2(t^1,\ldots,t^n), \ldots, h^k(t^1,\ldots,t^n)\right)}_{h^*f} \right.,  \\
&amp;\left. \frac{\partial }{\partial t^1} ,\ldots,a_n \frac{\partial }{\partial t_n}\right&gt; = \left&lt;d\left( h^*f \right) , v \right&gt; \\\end{aligned}\]</span> co daje <span class="math display">\[d\left( h^*(\alpha_1\land\ldots\land \alpha_k) \right) = h^* \left( d\left( \alpha_1\land\ldots\land \alpha_k \right)  \right) \quad\Box
.\]</span></p>
<h2 id="bazy-w-t_pm">Bazy w <span class="math inline">\(T_pM\)</span> </h2>
<p>Obserwacja: Niech <span class="math inline">\(M\)</span> - rozmaitość i <span class="math inline">\(\left&lt; | \right&gt;\)</span> - iloczyn skalarny. Niech <span class="math inline">\(e_1,\ldots,e_n\)</span> - baza <span class="math inline">\(T_pM\)</span>. Wówczas, jeżeli <span class="math inline">\(v = a_1e_1+\ldots+a_ne_n\)</span> i <span class="math inline">\(w = b_1e_1+\ldots+b_ne_n\)</span> (<span class="math inline">\(a_i,b_i\in \mathbb{R}, i= 1,\ldots,n\)</span>). <span class="math display">\[\begin{aligned}
    &amp;\left&lt;v|w \right&gt; = \left&lt;a_1e_1+\ldots+a_ne_n, b_1e_1+\ldots+b_ne_n \right&gt; = \\
    &amp;= a_1b_1\left&lt;e_1|e_1 \right&gt; + a_1b_2\left&lt;e_1|e_2 \right&gt;+ \ldots + a_1b_n \left&lt;e_1|e_n \right&gt; + \ldots + a_nb_n\left&lt;e_n|e_n \right&gt; = \\
    &amp; = \begin{bmatrix} a_1\\ \vdots \\ a_n\end{bmatrix}^T \begin{bmatrix} \left&lt;e_1|e_1 \right&gt; &amp; \left&lt;e_1|e_2 \right&gt;&amp;\ldots&amp;\left&lt;e_1|e_n \right&gt;\\ \vdots &amp; \ddots &amp; \\ \left&lt;e_n|e_1 \right&gt; &amp; \ldots &amp; &amp; \left&lt;e_n|e_n \right&gt; \end{bmatrix} \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix}
.\end{aligned}\]</span> Macierz <span class="math inline">\(\left[ g_{ij} \right]\)</span> nazywamy tensorem metrycznym <span class="math inline">\(\det \left[ g_{ij} \right] \overset{\text{ozn}}{=} g\)</span>. <span class="math inline">\(\left[ g_{ij} \right] ^{-1} \overset{\text{ozn}}{=} \left[ g^{ij} \right]\)</span> - macierz odwrotna.<br />
W zwykłym <span class="math inline">\(\mathbb{R}^4: \left[ g_{ij} \right] = \begin{bmatrix} 1&amp;&amp;\\ &amp;1&amp;\\ &amp;&amp;1 \end{bmatrix}\)</span>, p. Minkowskiego: <span class="math inline">\(g_{\mu v} = \begin{bmatrix} -1&amp;&amp;&amp;\\ &amp;1&amp;&amp; \\ &amp;&amp;1&amp; \\ &amp;&amp;&amp;1 \end{bmatrix}, \mu,v = 0,\ldots,3\)</span></p>
<p>Bazy w <span class="math inline">\(\mathbb{R}\)</span> <span class="math display">\[\begin{aligned}
 &amp;M = \mathbb{R}^2, &amp;&amp; &amp;N = \mathbb{R}^2\\
 &amp;\begin{bmatrix} x,y\\ e_x,e_y\\ \frac{\partial }{\partial x} , \frac{\partial }{\partial y}  \end{bmatrix} &amp;&amp; \overset{x=r\cos\varphi, y = r\sin\varphi}{\to}&amp; \begin{bmatrix} r,\varphi\\ e_r, e_\varphi \\ \frac{\partial }{\partial r}, \frac{\partial }{\partial \varphi}   \end{bmatrix}\\
 &amp;g_{ij} = \begin{bmatrix} 1&amp;0\\0&amp;1 \end{bmatrix} &amp;&amp;  &amp;\begin{bmatrix} ? \end{bmatrix}
.\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
    &amp;h^*(e_r) = \left( \begin{bmatrix} h&#39; \end{bmatrix} \begin{bmatrix} 1\\0 \end{bmatrix}  \right) _{\frac{\partial }{\partial x} , \frac{\partial }{\partial y} }, h^*(e_\varphi)\\
    &amp;h(r,\varphi) = \begin{bmatrix} r \cos\varphi\\ r \sin \varphi \end{bmatrix} , h&#39; = \begin{bmatrix} \cos\varphi&amp; - r\sin\varphi \\ \sin \varphi &amp; r \cos \varphi \end{bmatrix}\\
    &amp; h^*(e_r) = \begin{bmatrix} \cos\varphi \\ \sin \varphi \end{bmatrix} _{e_x,e_y}, e_r = \cos \varphi e_x + \sin \varphi e_y \\
    &amp;z = \cos \varphi e_x + \sin \varphi e_y \\
    &amp;h^*(e_\varphi) = \left[ h&#39; \right] \begin{bmatrix} 0\\1 \end{bmatrix} = \begin{bmatrix} -r\sin\varphi\\ r\cos\varphi \end{bmatrix}, e_\varphi = -r\sin\varphi e_x + r \cos \varphi e_y   \\
    &amp;\frac{\partial }{\partial \varphi} = -r\sin\varphi \frac{\partial }{\partial x} + r\cos\varphi \frac{\partial }{\partial y}  \\
    &amp;g_{ij} = \begin{bmatrix} \left&lt;e_1|e_1 \right&gt;&amp; \left&lt;e_1|e_2 \right&gt;\\ \left&lt;e_2|e_1 \right&gt; &amp; \left&lt;e_2|e_2 \right&gt; \end{bmatrix}, \left[ g_{ij} \right]_{x,y} = \begin{bmatrix} 1&amp;0\\0&amp;1 \end{bmatrix}, \left&lt;e_x|e_x \right&gt; =1, \left&lt; e_x|e_y\right&gt; =0\\
    &amp;\left&lt;e_r|e_r \right&gt; = \left&lt;\cos\varphi e_x + \cos\varphi e_y | \cos\varphi e_x + \sin \varphi e_y \right&gt; = \cos^2\varphi \left&lt;e_x|e_x \right&gt; + \sin^2\varphi\left&lt;e_y|e_y \right&gt;\\
    &amp;\left&lt;e_r|e_\varphi \right&gt; = \left&lt;\cos\varphi e_x + \sin \varphi e_y| -r\sin\varphi e_x + r\cos\varphi e_y \right&gt; =0\\
    &amp;\Vert \frac{\partial }{\partial \varphi}  \Vert ^2 = \left&lt;e_\varphi|e_\varphi \right&gt; = \left&lt;-r\sin\varphi e_x + r\cos\varphi e_y| -r\sin\varphi e_x + r\cos\varphi e_y \right&gt; = r^2
.\end{aligned}\]</span> <span class="math inline">\(\Vert \frac{\partial }{\partial \varphi}  \Vert = r,  \left[ g_{ij} \right]_{r,\varphi} = \begin{bmatrix} 1&amp;0\\0&amp;r^2 \end{bmatrix}\)</span>.<br />
baza <span class="math inline">\(\left&lt;\frac{\partial }{\partial r} , \frac{\partial }{\partial \varphi}  \right&gt;\)</span> nie jest bazą ortonormalną!!!</p>
<p><span class="math inline">\(e_x,e_y,e_z \to g_{ij} = \begin{bmatrix} 1&amp;&amp;\\&amp;1&amp;\\&amp;&amp;1 \end{bmatrix}\)</span> - jest fajnie.<br />
<span class="math inline">\(e_r,e_\theta,e_\varphi \to \begin{bmatrix} 1&amp;&amp;\\&amp;r^2&amp;\\&amp;r^2&amp;\sin^2\theta \end{bmatrix}, \Vert e_\theta \Vert = r, \Vert e_\varphi \Vert = r\sin\theta\)</span></p>
<p>Dostałem wektorek <span class="math inline">\(\begin{bmatrix} 2\\3\\4 \end{bmatrix}\)</span> w sferycznych. Ale w jakiej konkretnie bazie?</p>
<p>W fizyce mierzone wielkości np. wektorowe podajemy zawsze we współrzędnych .<br />
We współrzędnych sferycznych mamy dwie bazy: - ortogonalną: <span class="math inline">\(e_r, e_\theta, e_\varphi: \left( \frac{\partial }{\partial r} , \frac{\partial }{\partial \theta} , \frac{\partial }{\partial \varphi}  \right)\)</span><br />
- ortonormalną: <span class="math inline">\(i_r, i_\theta, i_\varphi: \left( \frac{\partial }{\partial r} , \frac{1}{r}\frac{\partial }{\partial \theta} , \frac{1}{r\sin\theta}\frac{\partial }{\partial \varphi}  \right)\)</span>. Więc jeżeli ktoś powiedział, że dostał <span class="math inline">\(\begin{bmatrix} 2\\3\\4 \end{bmatrix}\)</span> to znaczy, że ma <span class="math inline">\(2 \frac{\partial }{\partial r} + 3 \frac{1}{r} \frac{\partial }{\partial \theta} + 4 \frac{1}{r\sin\theta} \frac{\partial }{\partial \varphi}\)</span>.</p>
<p>Obserwacja: niech <span class="math inline">\(v = a_1e_1 + a_2e_2 + a_3e_3\)</span> i niech <span class="math inline">\(w = b_1e_1 + b_2e_2 + b_3e_3\)</span> i niech <span class="math inline">\(g_{ij} = \begin{bmatrix} g_{11}&amp;g_{12}&amp;g_{13}\\ g_{21}&amp; g_{22}&amp;g_{23}\\ g_{31}&amp;g_{32}&amp;g_{33} \end{bmatrix}\)</span> - tensor metryczny. Wówczas wiemy, że <span class="math inline">\(\left&lt;v|w \right&gt; = \left[v\right]^T\left[ g_{ij} \right] \left[ w \right] = \underbrace{\left[ a_1g_{11}+a_2g_{21}+a_3g_{31},\sum_{i=1}^{3}a_ig_{i2},\sum_{i=1}^3a_ig_{i3} \right]}_{\left&lt; v\right|}\left[ w \right]\)</span>.<br />
Ale w sumie to mogę wziąć coś takiego <span class="math inline">\(\left&lt; v \right|\)</span>.<br />
<span class="math display">\[\left( \sum_{i=1}^3 a^ig_{i1} \right) dx^1 + \left( \sum_{i=1}^3a^ig_{i2} \right) dx^2 + \left( \sum_{i=1}^3a^ig_{i3} \right)dx^3=
.\]</span> <span class="math display">\[= \sum_{i=1}^3\sum_{j=1}^3 a^ig_{ij}dx^j = a^ig_{ij}dx^j
.\]</span> Zapomniałem o sumach, bo <span class="math inline">\(a^ib_i \overset{\text{ozn}}{=}  a^1b_1 + a^2b_2 + a^3b_3\)</span>, w odróżnieniu od <span class="math inline">\(a^{\mu}b_\mu = a^0b_0+a^1b_1+\ldots\)</span> (Konwencja sumacyjna Einsteina).<br />
Ozn. <span class="math inline">\(\sum_{i=1}^3 a^ig_{ik} \overset{\text{ozn}}{=} a^ig_{ik} = a_k\)</span></p>
<p>niech <span class="math inline">\(M\)</span> - rozmaitość wymiaru <span class="math inline">\(n\)</span>, <span class="math inline">\(g_{ij}\)</span> - tensor metryczny na <span class="math inline">\(M\)</span>, operacją <span class="math inline">\(\sharp: T_pM \to T_p^*M\)</span> taką, że dla <span class="math inline">\(v = a^1 \frac{\partial }{\partial x^1} + \ldots + a^n \frac{\partial }{\partial x^n}\)</span>,<br />
<span class="math display">\[v^{\sharp}=a^ig_{i1}dx^1 + a^ig_{i2}dx^2 + \ldots + a^ig_{in}dx^n, i=1,\ldots,n
    .\]</span> zadaje izomorfizm między <span class="math inline">\(T_pM\)</span> a <span class="math inline">\(T_p^*M\)</span>.</p>
<p><span class="math inline">\(v = 7 \frac{\partial }{\partial r} + 8 \frac{\partial }{\partial \theta} + 9 \frac{\partial }{\partial \varphi}\)</span>. <span class="math display">\[\alpha\in T_p^*M  = v^{\sharp} = 7q_{11}dr+8q_{22}d\theta + 9q_{33}d\varphi = 7dr + 8r^2d\theta + 9r^2\sin^2\theta d\varphi
    .\]</span></p>
<h1 id="wykład-07.06.2019">Wykład (07.06.2019)</h1>
<p>Mając <span class="math inline">\(\sharp\)</span> możemy zdefiniować operację odwrotną:</p>
<p><span class="math display">\[\flat: T_p^*M\to T_pM\text{, taką, że } \alpha\in T_p^*M, \alpha = v_idx^i\]</span> to wtedy <span class="math display">\[T_pM \ni v \overset{\text{def}}{=} \alpha^\flat = \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n g^{ij}v_j \frac{\partial }{\partial x^i}
        .\]</span> Jeżeli wprowadzimy oznaczenie: <span class="math inline">\(v^i = \sum_{j=1}^n g^{ij}v_j\)</span>, to mamy <span class="math display">\[\alpha^\flat = \sum_{i=1}^n v^i \frac{\partial }{\partial x^i}
    .\]</span></p>
<p><span class="math display">\[\begin{aligned}
            &amp;\left[ g_{ij} \right] = \begin{bmatrix} 1&amp;&amp;\\ &amp;r^2&amp; \\ &amp;&amp;r^2\sin^2\theta \end{bmatrix}\\
            &amp;v = a \frac{\partial }{\partial r} + b \frac{\partial }{\partial \theta}  + c \frac{\partial }{\partial \varphi}, \alpha = v^{\sharp} = \frac{1}{2}\sum_{i=1}^3\sum_{j=1}^3 g_{ij}v^j dx^i = \\
            &amp;= \frac{1}{2}\left( g_{11}v^1dx^1 + g_{12}v^2dx^1+g_{13}v^3dx^1 \right) + \left( g_{21}v^1dx^2 + g_{22}v^2dx^2 + g_{23}v^3dx^2 \right) +\\
            &amp;+ \left(  g_{31}v^1dx^3 + g_{32}v^2dx^3 + g_{33}v^3dx^3 \right)
        .\end{aligned}\]</span> czyli mamy <span class="math display">\[\begin{aligned}
            \alpha = v^{\sharp} = 1\cdot a dr + r^2 b d\theta + r^2\sin^2\theta c d\varphi
        .\end{aligned}\]</span> Dostaliśmy z laboratorium wektor: <span class="math inline">\(v = \begin{bmatrix} a\\b\\c \end{bmatrix} = a i_r + b i_\theta + c i_\varphi = a \frac{\partial }{\partial r} + b \frac{1}{r} \frac{\partial }{\partial \theta} + c \frac{1}{r\sin\theta} \frac{\partial }{\partial \varphi}\)</span>. Chcemy ten wektorek podnieść.<br />
<span class="math display">\[\begin{aligned}
    &amp;\alpha = v^{\sharp} = \left( g \right) dr + \left( r^2 \frac{b}{r} \right) d\theta + \left( r^2 \sin^2\theta \frac{1}{r\sin\theta}c \right) d\varphi=\\
    &amp;= a dr + r b d\theta + r \sin\theta c d\varphi \\
.\end{aligned}\]</span></p>
<p>Niech <span class="math inline">\(\alpha = a dr + b d\theta + c d\varphi\)</span>. Chcemy zrobić wektorek <span class="math inline">\(v\)</span>, który jest dokładnie tyle: <span class="math display">\[\begin{aligned}
            v = \alpha^\flat = \left( 1\cdot a \right) \frac{\partial }{\partial r} + \left( \frac{1}{r^2}b \right) \frac{\partial }{\partial \theta} + \left( \frac{1}{r^2\sin^2\theta} c \right) \frac{\partial }{\partial \varphi}
        .\end{aligned}\]</span> Czyli ta nasza <span class="math inline">\(\alpha^\flat = \begin{bmatrix} a \\ \frac{b}{r^2} \\ \frac{c}{r^2 \sin^2\theta} \end{bmatrix}_{\frac{\partial }{\partial r} , \frac{\partial }{\partial \theta} , \frac{\partial }{\partial \varphi} } = a \frac{\partial }{\partial r} + \frac{b}{r} \cdot \frac{1}{r} \frac{\partial }{\partial \theta} + \frac{c}{r\sin\theta} \cdot \frac{1}{r\sin\theta} \frac{\partial }{\partial \varphi}\)</span>.<br />
Okazuje się, że <span class="math inline">\(\alpha^\flat = \begin{bmatrix} b\\ \frac{b}{r} \\ \frac{c}{r\sin\theta} \end{bmatrix}_{i_r, i_\theta, i_\varphi}\)</span></p>
<p>niech <span class="math inline">\(M = \mathbb{R}^3\)</span>, <span class="math display">\[\Lambda^0(M)\ni f \overset{\text{d}}{\to} df\in \Lambda^1(M) \overset{\flat}{\to} \left( df \right) ^\flat \in T_pM\]</span> nazywamy gradientem funkcji <span class="math inline">\(f\)</span> : <span class="math inline">\(\nabla f \overset{\text{def}}{=} \left( df \right) ^\flat\)</span>, gdzie <span class="math inline">\(f: M\to \mathbb{R}^1\)</span>, <span class="math inline">\(f\)</span> - klasy <span class="math inline">\(\mathcal{C}^k(M)\)</span></p>
<p><span class="math inline">\(f(r,\theta,\varphi): \mathbb{R}^3\to \mathbb{R}^1\)</span>,<br />
<span class="math inline">\(df = \frac{\partial f}{\partial r}dr + \frac{\partial f}{\partial \theta}d\theta + \frac{\partial f}{\partial \varphi}d\varphi\)</span> <span class="math display">\[\begin{aligned}
            &amp;\left( df \right) ^\flat = 1 \frac{\partial f}{\partial r} \frac{\partial }{\partial r} + \frac{1}{r^2} \frac{\partial f}{\partial \theta} \frac{\partial }{\partial \theta} + \frac{1}{r^2\sin^2\theta}\frac{\partial f}{\partial \varphi} \frac{\partial }{\partial \varphi}=\\
            &amp;= \frac{\partial f}{\partial r} \frac{\partial }{\partial r} + \frac{1}{r} \frac{\partial f}{\partial \theta} \frac{1}{r} \frac{\partial }{\partial \theta} + \frac{1}{r\sin\theta} \frac{\partial f}{\partial \varphi} \frac{1}{r\sin\theta} \frac{\partial }{\partial \varphi}
        .\end{aligned}\]</span> Siła tego polega na tym, że jak dostaniemy na ulicy tensor metryczny, to przez 3 minuty w cieniu możemy obliczyć np. gradient funkcji: <span class="math display">\[\nabla f = \begin{bmatrix} \frac{\partial f}{\partial r} \\ \frac{1}{r} \frac{\partial f}{\partial \theta} \\ \frac{1}{r\sin\theta} \frac{\partial f}{\partial \varphi}  \end{bmatrix}
        .\]</span></p>
<p>Dostaliśmy tensor metryczny i chcemy obliczyć <span class="math inline">\(\nabla f(\xi, \eta, \delta)\)</span>, <span class="math inline">\(\begin{bmatrix} \heartsuit &amp;&amp;\\ &amp;\triangle&amp; \\ &amp;&amp;\square \end{bmatrix}\)</span>. <span class="math display">\[\nabla f = \begin{bmatrix} \frac{1}{\sqrt{\heartsuit} } \frac{\partial f}{\partial \xi} \\ \frac{1}{\sqrt{\triangle} } \frac{\partial f}{\partial \eta} \\ \frac{1}{\sqrt{\square} } \frac{\partial f}{\partial \delta} \end{bmatrix}
        .\]</span></p>
<p><span class="math display">\[\begin{aligned}
        &amp;M = \mathbb{R}^3\\
        f\to &amp;\Lambda^0(M) &amp;&amp;\dim \Lambda^0(M) = 1 \downarrow d\\
        T_pM \overset{\overset{\flat}{\leftarrow}}{\underset{\underset{\sharp}{\rightarrow}}{\longleftrightarrow}} &amp;\Lambda^1(M) &amp;&amp;\dim \Lambda^1(M) = 3 \downarrow d\\
        &amp;\Lambda^2(M) &amp;&amp;\dim \Lambda^2(M) = 3 \downarrow d\\
        &amp;\Lambda^3(M) &amp;&amp;\dim \Lambda^3(M) = 1
    .\end{aligned}\]</span></p>
<p>Niech <span class="math inline">\(M\)</span> - rozmaitość, <span class="math inline">\(\dim M = n\)</span>, <span class="math inline">\(\left[ g_{ij} \right]\)</span> - tensor metryczny. Operację <span class="math inline">\(\Lambda^L(M)\to \Lambda^{n-L}(M)\)</span> nazywamy gwiazdką &quot;<span class="math inline">\(\ast\)</span>&quot; Hodge’a i definiujemy następująco: <span class="math display">\[\begin{aligned}
            \ast\left( dx^{i_1}\land dx^{i_2}\land \ldots \land dx^{i_L} \right) = \frac{\sqrt{g} }{(n-L)!} g^{i_1j_1}g^{i_2j_2}g^{i_Lj_L}\in _{j_1j_2\ldots j_L k_1k_2\ldots k_{n-L}}dx^{k_1}\land dx^{k_2}\land \ldots \land dx^{k_{n-1}}
        ,\end{aligned}\]</span> gdzie <span class="math inline">\(\in _{i_1,\ldots,i_n} = \left\{ sgn(i_1,\ldots,i_n) \text{ jeżeli } i_m \neq i_p,\quad 0 \text{ w.p.p} \right\}\)</span></p>
<p><span class="math inline">\(M = \mathbb{R}^3\)</span>, <span class="math inline">\(\left[ g_{ij} \right] = \begin{bmatrix} 1&amp;&amp;\\&amp;1&amp;\\&amp;&amp;1 \end{bmatrix}\)</span><br />
<span class="math display">\[\begin{aligned}
           \ast(dx) &amp;= \frac{1}{(3-1)!}g^{1 j_1}\in _{j_1 k_1k_2}dx^{k_1}\land dx^{k_2} = \frac{1}{(3-1)!}g^{11}\in _{1k_1k_2}dx^{k_1}\land dx^{k_2} =\\
                    &amp;= \frac{1}{(3-1)!}g^{11} \left[ \in_{1 2 3}dx^2\land dx^3 + \in_{1 3 2}dx^3\land dx^2 \right] = \frac{1}{2} \left[ 1\cdot dx^2 \land dx^3 - dx^3 \land dx^2 \right]  \\
                    &amp;= dx^2\land dx^3
        .\end{aligned}\]</span> Czyli <span class="math inline">\(\ast(dx) = dy \land dz\)</span>.<br />
<span class="math display">\[\begin{aligned}
            &amp;\ast(dy) = \ast(dx^2) = \frac{1}{(3-1)!}g^{22}\in_{2k_1k_2} dx^{k_1}\land dx^{k_2} = \frac{1}{(3-1)!} \cdot \\
            &amp;g^{22} \left[ \in_{2 1 3} dx^1\land dx^3 + \in _{2 3 1}dx^3\land dx^1 \right] = \frac{1}{(3-1)!} 1 \left[ -dx^1\land dx^2 + 1 dx^3 \land dx^1 \right] =\\
            &amp;= dx^3 \land dx^1
        .\end{aligned}\]</span> Więc <span class="math inline">\(\ast (dy) = dz\land dx\)</span>.<br />
<span class="math display">\[\begin{aligned}
             &amp;\ast(dz) = \frac{1}{(3-1)!}g^{33}\in_{3k_1k_2}dx^{k_1}\land dx^{k_2} = \frac{1}{2}g^{33}\left[ \in_{321}dx^2\land dx^1 + \in_{312} dx^1 \land dx^2 \right]=\\
             &amp;= \frac{1}{2} 1 \left[ -dx^2\land dx^1 + dx^1\land dx^2 \right]
        .\end{aligned}\]</span> Więc <span class="math inline">\(\ast(dz) = dx\land dy\)</span></p>
<p><span class="math inline">\(M = \mathbb{R}^3, (r,\theta,\varphi), \left[ g_{ij} \right] = \begin{bmatrix} 1&amp;&amp;\\&amp;r^2&amp;\\&amp;&amp;r^2\sin^2\theta \end{bmatrix}\)</span>.<br />
<span class="math display">\[\begin{aligned}
            &amp;\ast(dr) = \overset{\sqrt{g}}{r^2\sin\varphi} d\theta \land d\varphi\\
            &amp;\ast(d\theta) = r^2 \sin\theta \frac{1}{r^2} d\varphi \land dr\\
            &amp;\ast(d\varphi) = \frac{r^2\sin\theta}{r^2\sin^2\theta} dr \land d\theta  \\
        .\end{aligned}\]</span></p>
<p>Pytanko jest takie: Chcemy zapytać co to jest <span class="math inline">\(\ast(dx\land dy)\)</span>?<br />
<span class="math display">\[\begin{aligned}
         &amp;\ast(dx^1\land dx^2) = \frac{\sqrt{g} }{(3-2)!} g^{1 j_1}g^{2 j_2} \in_{j_1j_2k_1} dx^{k_1} = \\
         &amp;= \frac{1}{(3-2)!}g^{11}g^{22}\in_{123}dx^3
    .\end{aligned}\]</span> Więc <span class="math inline">\(\ast(dx\land dy) = dz\)</span>.<br />
A np. <span class="math inline">\(\ast(dx\land dz)\)</span> : <span class="math display">\[\begin{aligned}
        &amp;\ast(dx\land dz) = \frac{1}{(3-2)!}\in_{132}dx^2 = -dy\\
        &amp;\ast(dr\land d\theta) = r^2\sin\theta \cdot \frac{1}{1}\cdot \frac{1}{r^2}d\varphi \\
        &amp;\ast(dr\land d\varphi) = -r^2\sin\theta \frac{1}{1} \frac{1}{r^2\sin^2\theta} d\theta \\
        &amp;\ast(dx\land dy\land dz) = \frac{\sqrt{g} }{(3-3)!}g^{1 j_1}g^{2 j_2}g^{3 j_3} \in_{j_1 j_2 j_3} = \sqrt{g} g^{11}g^{22}g^{33}\in_{123} = 1\\
        &amp;\ast(dr\land d\theta \land d\varphi) = r^2\sin\theta \cdot \frac{1}{r^2}\cdot \frac{1}{r^2\sin^2\theta} = \frac{1}{r^2\sin\theta}
    .\end{aligned}\]</span></p>
<p><span class="math inline">\(M = \mathbb{R}^3\)</span><br />
niech <span class="math inline">\(v\in T_pM\)</span>, operację <span class="math display">\[rot(v) \overset{\text{def}}{=} \left( \ast\left( dv^\sharp \right)  \right) ^\flat\]</span> nazywamy rotacją wektora <span class="math inline">\(v\)</span> i oznaczamy <span class="math inline">\(\text{rot } v \overset{\text{ozn}}{=} \nabla\times v\)</span>.<br />
Operację <span class="math display">\[\text{div }v \overset{\text{def}}{=} d\left( \ast v^\sharp \right)\]</span> nazywamy dywergencją i oznaczamy <span class="math inline">\(\text{div }v \overset{\text{ozn}}{=} \nabla \cdot v\)</span>.<br />
Uwaga: rotacji nie możemy wprowadzić np. na <span class="math inline">\(M\)</span> takim, że <span class="math inline">\(\dim M = 4\)</span>, bo <span class="math inline">\(\ast(\Lambda^2(M))\to \Lambda^2(M)\)</span></p>
<p>Pozakonkursowo: chcemy zrobić z funkcji funkcję: <span class="math display">\[\begin{aligned}
        f \overset{d}{\longrightarrow} df \in \Lambda^1(M) \longrightarrow \underset{\text{operator Laplace}}{\ast d \ast df}
    .\end{aligned}\]</span></p>
<h1 id="wykład-11.06.2019">Wykład (11.06.2019)</h1>
<p>Zastanówmy się jak wygląda rotacja wektora w układzie sferycznym. <span class="math inline">\(M = \mathbb{R}^3\)</span>. <span class="math display">\[\begin{aligned}
        &amp;v \overset{\sharp}{\to} \Lambda^1(M) \overset{d}{\to} \Lambda^2(M) \overset{\ast}{\to} \Lambda^1(M) \to \overset{\flat}{T_pM} \to \begin{bmatrix} \\ \\ \end{bmatrix}_{i} \\
        &amp;rot v = \left( \ast (d v^\sharp) \right)^{\flat}\\
        &amp;\text{na początek dostajemy w smsie } \begin{bmatrix} A^r \\ A^\theta \\ A^\varphi \end{bmatrix}_{i_r, i_\theta, i_\varphi} = v = A^r \frac{\partial }{\partial r} + A^\theta \frac{1}{r} \frac{\partial }{\partial \theta} + A^\varphi \frac{1}{r\sin\theta} \frac{\partial }{\partial \varphi}\\
        &amp;\text{chcemy sobie zrobić jednoformę, która jest podniesionym wektorkiem: } \alpha = v^\sharp = \\
        &amp;= g_{rr} A^r dr + g_{\theta\theta} \frac{1}{r} A^\theta d\theta + g_{\varphi\varphi} \frac{1}{r\sin\theta}A^\varphi d\varphi = A^r dr + r A^\theta d\theta + r\sin\theta A^\varphi d\varphi \\
        &amp;d\alpha = \left( A^r_{,\theta} - (rA^\theta)_{,r}  \right) d\theta \land dr + \left( A^r_{,\varphi} - (r\sin\theta A^\varphi)_{,r} \right) d\varphi \land dr + \left( (rA^\theta)_{,\varphi} - (r\sin\theta A^\varphi)_{,\theta} \right) d\varphi \land d\theta\\
        &amp;\ast(dr\land d\theta) = \sin\theta d\varphi,\quad \ast(d\theta\land d\varphi) = \frac{1}{r^2}dr,\quad \ast(d\varphi\land dr) = \frac{1}{\sin\theta}d\theta\\
        &amp;\ast d\alpha = \left( (r \sin\theta A^\varphi)_{,\theta} - (rA^\theta)_{,\varphi} \right)\frac{1}{r^2\sin\theta} dr + \left( A^r_{,\varphi} - (r\sin\theta A^\varphi)_{,r} \right) \frac{1}{\sin\theta} d\theta + \\
        &amp; + \left( (rA^\theta)_{,r} - A^r_{,\theta} \right) \sin\theta d\varphi
    .\end{aligned}\]</span> notacja: <span class="math inline">\(\Box_{,\heartsuit} = \frac{\partial \Box}{\partial \heartsuit}\)</span>. Zostały nam jeszcze tylko dwie operacje. <span class="math display">\[\begin{aligned}
        &amp;\left( \ast d\alpha \right) ^\flat = \left( (r\sin\theta A^\varphi)_{,\theta} - (r A^\theta)_{,\varphi}\right) \cdot 1\cdot \frac{1}{r^2\sin\theta} \frac{\partial }{\partial r} + \left( A^r_{,\varphi} - (r \sin\theta A^\varphi)_{,r}\right) \frac{1}{\sin\theta} \frac{1}{r^2} \frac{\partial }{\partial \theta} +\\
        &amp;+ \left( (r A^\theta)_{,r} - A^r_{,\theta})\sin\theta \frac{1}{r^2\sin^2\theta} \right) \frac{\partial }{\partial \varphi}
    .\end{aligned}\]</span> Czyli <span class="math display">\[rot \begin{bmatrix} A^r \\ A^\theta \\ A^\varphi \end{bmatrix} =
    \begin{bmatrix} \frac{1}{r^2\sin\theta} \left( (r\sin\theta A^\varphi)_{,\theta} - (r A^\theta)_{,\varphi}) \right) \\
    \frac{1}{r\sin\theta} \left( A^r_{,\varphi} - (r\sin\theta A^\varphi)_{,r} \right) \\
\frac{1}{r} \left( (r A^\theta)_{,r} - A^r_{,\theta} \right) \end{bmatrix}
    .\]</span></p>
<p>To może teraz dywergencja rzutem na taśmę. <span class="math display">\[\begin{aligned}
        &amp;\begin{bmatrix} \\ \\  \end{bmatrix} = v \overset{\sharp}{\to} \Lambda^1(M) \overset{\ast}{\to} \Lambda^2(M) \overset{d}{\to} \Lambda^3(M) \overset{\flat}{\to} \Lambda^0 (M)  \\
        &amp;div(v) = \ast\left( d(\ast v^\sharp) \right)\\
        &amp;\begin{bmatrix} A^r\\ A^\theta\\ A^\varphi \end{bmatrix} = v, \alpha = v^\sharp\\
        &amp;\alpha = A^r dr + r A^\theta d\theta + A^\varphi r \sin\theta d\varphi\\
        &amp;\ast dr = r^2\sin\theta d\theta\land d\varphi\\
        &amp;\ast d\theta = \sin\theta d\varphi\land dr\\
        &amp;\ast d\varphi = \frac{1}{\sin\theta} dr\land d\theta\\
        &amp;\ast \alpha = \left( A^r r^2 \sin\theta \right) d\theta \land d\varphi + \left( r \sin\theta A^\theta \right) d\varphi\land dr + \left( r A^\varphi \right) dr\land d\theta\\
        &amp;d(\ast \alpha) = \left( (A^r r^2 \sin\theta)_{,r} + (r\sin\theta A^\theta)_{,\theta} + (r A^\varphi)_{,\varphi} \right) dr\land d\theta \land d\varphi\\
    .\end{aligned}\]</span> <span class="math display">\[div \begin{bmatrix} A^r\\ A^\theta \\ A^\varphi \end{bmatrix} = \frac{1}{r^2\sin\theta} \left( (A^r r^2\sin\theta)_{,r} + (r\sin\theta A^\theta)_{,\theta} + (rA^\varphi)_{,\varphi} \right)
    .\]</span> <span class="math display">\[\begin{aligned}
        &amp;f(r,\theta,\varphi) \overset{d}{\to} \Lambda^1(M) \overset{\ast}{\to} \Lambda^2(M) \overset{d}{\to} \Lambda^3(M) \overset{\ast}{\to} \Lambda^0(M)\\
        &amp;\alpha = df = \frac{\partial f}{\partial r} dr + \frac{\partial f}{\partial \theta} d\theta + \frac{\partial f}{\partial \varphi} d\varphi\\
        &amp;\ast \alpha = \left( \frac{\partial f}{\partial r} r^2\sin\theta \right) d\theta\land d\varphi + \left( \frac{\partial f}{\partial \theta} \sin\theta \right) d\varphi\land dr + \left( \frac{\partial f}{\partial \varphi} \frac{1}{\sin\theta} \right) dr\land d\theta\\
        &amp;d(\ast \alpha) = \left( \left(r^2\sin\theta \frac{\partial f}{\partial r} \right)_{,r} +\sin\theta \frac{\partial f}{\partial \theta} _{,\theta}  + \left( \frac{1}{\sin\theta} \frac{\partial f}{\partial \varphi} _{,\varphi} \right)  \right) dr\land d\theta\land d\varphi\\
        &amp; \ast(d(\ast \alpha)) = \frac{1}{r^2\sin\theta}\left( \left(r^2\sin\theta \frac{\partial f}{\partial r} \right)_{,r} + \left(\sin\theta \frac{\partial f}{\partial \theta} \right)_{,\theta} + \left(\frac{1}{\sin\theta}\frac{\partial f}{\partial \varphi} \right)_{,\varphi} \right)
    .\end{aligned}\]</span></p>
<p><span class="math inline">\(M = \mathbb{R}^3, f\in \Lambda^0(M)\)</span>. <span class="math display">\[\begin{aligned}
     &amp;dd f = 0\\
     &amp;ddf = d\left( \left( (df)^\flat \right)^\sharp \right) \implies rot( grad (f) ) = 0
    .\end{aligned}\]</span> Niech teraz <span class="math inline">\(v\in \Lambda^1(M)\)</span>. <span class="math display">\[\begin{aligned}
         &amp;d\left(\ast\left(\left(\ast(d V^\sharp)\right)^\flat\right)^\sharp\right) = \overset{\ast\ast=\mathbb{I}?}{d(\ast(\ast(d(v^\sharp))))} = dd(v^\sharp) = 0\\
         &amp;div(rot (V)) = 0
    .\end{aligned}\]</span></p>
<p>Weźmy sobie jakąś funkcję: <span class="math inline">\(f: (t,x,y,z)\to \mathbb{R}\)</span>.<br />
Zobaczmy jak <span class="math inline">\(\ast d(\ast d f)\)</span> wygląda w <span class="math inline">\(\begin{bmatrix} -1&amp;&amp;&amp;\\&amp;1&amp;&amp;\\&amp;&amp;1&amp;\\&amp;&amp;&amp;1 \end{bmatrix}\)</span>. <span class="math display">\[\begin{aligned}
            &amp;df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy + \frac{\partial f}{\partial z} dz.\\
            &amp;\ast\left( dx^{i_1}\land \ldots \land dx^{i_L} \right) = \frac{\sqrt{g} }{(n-L)!} g^{i_1j_1}\ldots g^{i_Lj_L}\in_{j_1\ldots j_kk_1\ldots k_{n-L}}dx^{k_1}\land\ldots\land dx^{k_{n-L}}\\
            &amp; \ast(dx^0) = \frac{\sqrt{-(-1)} }{(4-1)!}g^{0 0}\in_{0k_1k_2k_3}dx^{k_1}\land dx^{k_2}\land dx^{k_3}, i,k = 0,\ldots,3\\
            &amp;\ast(dx^0) = -\frac{1}{3!} 3! dx^1\land dx^2\land dx^3\\
            &amp;\ast(dt) = -dx\land dy\land dz\\
            &amp;\ast(dx^1) = \frac{\sqrt{-(-1)} }{(4-1)!} g^{1 1}\in_{1k_1k_2k_3}dx^{k_1}\land dx^{k_2}\land dx^{k_3}\\
            &amp;\ast(dx) = 3! \frac{1}{3!}dy\land dt\land dz\\
            &amp;\ast(dy) = dt\land dx\land dz\\
            &amp;\ast(dz) = dx\land dt\land dy\\
            &amp;\ast df = -\frac{\partial f}{\partial t} dx\land dy\land dz + \frac{\partial f}{\partial x} dy\land dt\land dz + \frac{\partial f}{\partial y} dt\land dx\land dz + \frac{\partial f}{\partial z} dx\land dt\land dy\\
            &amp;d\ast d f =\left( -\frac{\partial ^2f}{\partial t^2} + \frac{\partial ^2f}{\partial x^2} + \frac{\partial ^2f}{\partial y^2} + \frac{\partial ^2f}{\partial z^2}   \right)dt\land dx\land dy\land dz
        .\end{aligned}\]</span> Na koniec:<br />
Mamy dwuformę pola elektromagnetycznego: <span class="math display">\[\begin{aligned}
            &amp;F = -E_x dt\land dx + E_y dt\land dy - E_2 dt\land dz + B_x dy\land dz + B_y dz\land dy + B_z dy\land dx.\\
            &amp;dF = 0 \text{ to jest pierwsza część równań Maxwella}\\
            &amp;\begin{bmatrix} \rho\\ \rho v^x\\ \rho v^y\\ \rho v^z \end{bmatrix} = \begin{bmatrix} \rho\\ j^x\\ j^y\\ j^z \end{bmatrix}\\
            &amp;j=-gdt + j^xdx+j^ydy+j^zdz\\
            &amp;d(\ast F) = \ast j \text{ a to druga}
        .\end{aligned}\]</span></p>
</body>
</html>
