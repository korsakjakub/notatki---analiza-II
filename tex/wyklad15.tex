\documentclass[../main.tex]{subfiles}
\graphicspath{
    {"../img/"}
    {"img/"}
}

\begin{document}
Pytanie:\\
Czy kolumny $R(t,t_0)$ są liniowo niezależne $\underset{t,t_0\in [a,b]}{\forall} $?\\
Wiemy, że $R(t,t_0) = \mathbb{I} = \begin{bmatrix} 1&0&\ldots&0\\ \vdots&1&\ddots&\vdots \\ \vdots&\dots&\ddots&\vdots \\ 0&\dots&\dots&1 \end{bmatrix} $.\\
Chcielibyśmy, żeby $\underset{t,t_0\in[a,b] }{\forall} \det R(t,t_0) \neq 0$.

Przypomnienie z algebry:\\
Z macierzą $\begin{bmatrix} a_{11}&\ldots&a_{1n}\\ \vdots & & \vdots \\a_{n1} &\dots& a_{n} \end{bmatrix} $ możemy związać macierz $D = \begin{bmatrix} D_{11}&\ldots&D_{1n}\\ \vdots &&\vdots \\ D_{n1} & \ldots & D_{nn} \end{bmatrix} $.\\
Zatem $\det A$ uzyskamy mnożąc np. pierwszy wiersz $A$ z pierwszą kolumną $D^T$.\\
Pytanie: Co się stanie, jeśli przemnożymy pierwszy wiersz $A$ przez drugą kolumnę $D^T$?
\begin{przyklad}
    $A = \begin{bmatrix} 1&2\\3&4 \end{bmatrix},\quad D = \begin{bmatrix} 4&-3\\ -2&1 \end{bmatrix},\quad D^T = \begin{bmatrix} 4&-2\\-3&1 \end{bmatrix} $ i wtedy $\begin{bmatrix} 1&2\\3&4 \end{bmatrix} \begin{bmatrix} 4&-2\\-3&1 \end{bmatrix} = \begin{bmatrix} -2&0\\0&-2 \end{bmatrix} $, zatem $AD^T = \sum_{i=1}^n D_{ik}a_{si} = \delta_{ks}\det A$
\end{przyklad}
\begin{tw}
    (Liouville)\\
    Jeżeli $R(t,t_0)$ - rezolwenta dla problemu
    \begin{align*}
        &\frac{dx}{dt} = A(x)x(t)\\
        &x(t_0) = x_0
    .\end{align*}
    i $x\in \mathbb{R}^n$, to $w(t) = w(t_0)e^{\int_{t_0}^t tr(A(s))ds}$, gdzie $w(t) = \det R(t,t_0)$ i $w(t)$ nazywamy wrońskianem.\\
\end{tw}
    Uwaga:\\
        Zauważmy, że $w(t)$ nigdy nie będzie równa zero, bo $w(t_0) = \det(R(t_0,t_0)) = \det \begin{bmatrix} 1&\ldots&0\\0&\ldots&1 \end{bmatrix} = 1$ a $\left| \int_{t_0}^t tr A(s)ds \right| < +\infty$ (bo $A(t)\to$ lipszycowalna).\\
        Oznacza to, że kolumny $R(t,t_0)$ są $\underset{t,t_0\in [a,b]}{\forall} $ liniowo niezależne, więc możemy badać bazę rozwiązań złożoną z kolumn $R(t,t_0)$
        \begin{dowod}
            Rezolwenta jest postaci:
            \[
                R(t,t_0) = \begin{bmatrix} u_{11}(t) & u_{12}(t) & \ldots & u_{1n}(t)\\ \vdots \\ u_{n1}(t) & \ldots & \ldots & u_{nn}(t)\end{bmatrix}
            ,\]
            gdzie $u_{ij}(t_0) = \delta_{ij}$.\\
            Wiemy, że $\frac{d R(t,t_0)}{dt} = A(t) R(t,t_0)$.\\
            Obserwacja: policzmy $\det R(t,t_0)$ względem pierwszego wiersza:
            \[
                w(t) = (-1)^{1+1}u_{11}(t) \begin{bmatrix} u_{22} & \ldots & u_{2n} \\ \vdots \\ u_{n2}(t) & \ldots & u_{nn}(t) \end{bmatrix} + (\text{ brak  }u_{11})
            .\]
            Zatem $\frac{\partial w(t)}{\partial u_{11}} = D_{11}$ i ogólnie $\frac{\partial w(t)}{\partial u_{ij}} = D_{ij}$.\\
            Zatem $w(t)$ możemy potraktować jako funkcję od $n\times n$ zmiennych. $w(t) = w(u_{11}(t), u_{12}(t), \ldots, u_{nn}(t))$, zatem
            \[
                \frac{\partial w(t)}{\partial t} = \frac{\partial w}{\partial u_{11}} \frac{\partial u_{11}}{\partial t} + \frac{\partial w}{\partial u_{12}} \frac{\partial u_{12}}{\partial t} +\ldots+ \frac{\partial w}{\partial u_{nn}} \frac{\partial u_{nn}}{\partial t}
            .\]
            Skoro $\frac{d R(t,t_0)}{dt} = A(t) R(t,t_0)$ to znaczy, że
            \[
                \frac{\partial u_{ki}}{\partial t} = \sum_{s=1}^n a_{ks}u_{si}
            .\]
            Czyli
            \begin{align*}
                &\frac{d w}{dt} = \sum_{k,i}D_{ki} \sum_s a_{ks}u_{si} = \sum_{k=1}^n \sum_{i=1}^n \sum_{s=1}^n a_{ks} D_{ki}u_{si} =\\
                &= \sum_{k=1}^n\sum_{s=1}^n a_{ks}\delta_{ks}w(t) = \sum_{k=1}^n a_{kk}w(t)\\
            .\end{align*}
            Zatem $\frac{\partial w}{\partial t} = tr(A(t))\cdot  w(t)$. Jak przyłożymy obustronnie całkę to otrzymamy:
            \[
                \int_{t_0}^t \frac{dw}{w} = \int_{t_0}^t tr(A(s))ds \implies -\ln t_0 + \ln w = \int_{t_0}^t tr(A(s))ds \to w(t) = e^{\int_{t_0}^t tr(A(s))ds}e^{\ln \ln t_0}
            .\]
            Czyli
            \[
                w(t) = w(t_0) e^{\int_{t_0}^t tr(A(s))ds}\quad\Box
            \]
        \end{dowod}

        \subsection{Równania liniowe wyższych rzędów (na skróty)}
        Rozważmy równanie:
        \begin{equation}
            \label{eq:linwrz}
            \frac{d^n x}{dt^n} = a_0x(t) + a_1x'(t) + \ldots + a_{n-1}x^{n-1}(t)
        \end{equation}
        (gdzie $a_0,\ldots,a_{n-1}\in \mathbb{R}$ ).\\
        Chcemy znaleźć bazę rozwiązań.\\
        Możemy zapisać  (\ref{eq:linwrz}) jako
         \[
             \frac{d}{dt} \begin{bmatrix} x(t)\\ x'(t) \\ \vdots \\ x^{n-1}(t) \end{bmatrix} = \begin{bmatrix} 0&1&0&\ldots&0
         \\ 0&0&1&\ldots&0
         \\ \vdots &&&&1\\
     a_0 & a_1 & \ldots && a_{n-1}\end{bmatrix}
     \begin{bmatrix} x(t) \\ x'(t) \\ \vdots \\ x^{n-1}(t) \end{bmatrix}
        .\]
        Zatem
        \[
            \begin{bmatrix}  x(t) \\ x'(t) \\ \vdots \\ x^n(t) \end{bmatrix} = \sum_i e^{\lambda_i(t-t_0)} \sum_j \frac{t-t_0}{j}(a-\lambda_i \mathbb{I})^{\ln_i - 1}\underbrace{x_0^i}_{(*)}
        .\]
        Chcemy znaleźć pierwiastki $w(\lambda) = \det(A - \lambda \mathbb{I})$
        \begin{przyklad}
            \[
                \begin{bmatrix} 0&1&0&0\\ 0&0&1&0\\ 0&0&0&1\\ a_0&a_1&a_2&a_3 \end{bmatrix}
            .\]
            \begin{align*}
                &\det(A - \lambda \mathbb{I}) = \det \begin{bmatrix} -\lambda & 1 & 0 & 0\\ 0 &-\lambda&1&0\\ 0&0&-\lambda&1\\ a_0&a_1&a_2&a_3-\lambda \end{bmatrix} = a_0(-1)^{1+4} \begin{vmatrix} 1&0&0\\-\lambda&1&0\\0&-\lambda&1 \end{vmatrix} + (-1)^{2+4} a_1 \begin{vmatrix} -\lambda&0&0\\0&1&0\\0&-\lambda&1 \end{vmatrix} +\\
                &(-1)^{3+4}a_2\begin{vmatrix} -\lambda&1&0\\0&-\lambda&0\\0&0&1 \end{vmatrix} + (-1)^{4+4}(a_3-\lambda)\begin{vmatrix} -\lambda&1&0\\0&-\lambda&1\\0&0&-\lambda \end{vmatrix} = -a_0 \cdot 1 - a_1\lambda - a_2\lambda^2 - a_3\lambda^3 + \lambda^4\\
                &\frac{d^4x}{dt^4} = a_0x + a_1x' + a_2x'' + a_3x''',\quad \lambda^4 = a_0+a_1\lambda+a_2\lambda^2+a_3\lambda^3\\
                \\
                &\ddot{x} + \omega^2 x = te^{t}\\
                &\frac{d}{dt} \begin{bmatrix} x\\x'\\ \vdots \\ x^{n+1} \end{bmatrix} = \begin{bmatrix} . \\ . \\ . \\ . \end{bmatrix}\begin{bmatrix} x\\x^1\\ \vdots \\ x^{n+1} \end{bmatrix} + \begin{bmatrix} 0\\ \vdots \\ b(t) \end{bmatrix}\\
                \\
                &\frac{d^nx}{dt^n} = a_0x + a_1x' + \ldots + a_{n-1}x^{n-1}\\
                &\lambda^n = a_0+a_1\lambda+\ldots+a_{n-1}\lambda^{n-1}
            .\end{align*}
            Połóżmy $x = e^{\lambda t}\to$ skrót mnemotechniczny\\
            \[
            e^{\lambda t}\lambda^n = e^{\lambda t}a_0 + a_1 \lambda e^{\lambda t} + \ldots + a_{n-1}\lambda^{n-1}e^{\lambda t}
            .\]
        \end{przyklad}

        \subsection{Warunek początkowy}
        czy można znaleźć współczynniki $x_0^i$ we wzorze $(*)$ bez konieczności rozkładu warunku brzegowego w bazie wektorów własnych macierzy $A$?
        \begin{przyklad}
            Niech $\ddot{x} + \omega^2 x = 0$ i $x(0) = 0, x'(0) = 1$ i wiemy, że $\lambda^2 + \omega^2 = 0$.\\
            Oznacza to, że $x(t) = A e^{i\omega t} + B e^{-i\omega t}(*)$,
            \begin{align*}
                &\lambda_1 = i\omega, &&n_1 = 1\\
                &\lambda_2 = -i\omega, &&n_2 = 1
            .\end{align*}
        \end{przyklad}
\end{document}
